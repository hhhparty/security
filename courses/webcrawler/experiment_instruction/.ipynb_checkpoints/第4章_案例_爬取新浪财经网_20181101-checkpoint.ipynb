{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “网络数据采集” 课程\n",
    "\n",
    "# 第4章 Web内容解析\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例名称\n",
    "\n",
    "爬取新浪财经网http://finance.sina.com.cn/stock/，各股票公司每日公告（爬取股票分析所需语料）\n",
    "\n",
    "## 案例内容\n",
    "\n",
    "本案例用于爬取新浪财经股票公司公告。\n",
    "\n",
    "基本流程如下：\n",
    "\n",
    "1. 输入开始日期、结束日期，作为查询条件，之后程序后计算之间有多少天，之后生成每一天的日期，以每天的日期作为查询条件提交到 http://vip.stock.finance.sina.com.cn/corp/view/vCB_BulletinGather.php?gg_date=&ftype=0 进行查询。\n",
    "\n",
    "2. 程序采用了多线程技术加速爬取过程。每个线程都将首先执行spiderOneGroupDays；\n",
    "\n",
    "3. 调用 spiderOneDay\n",
    "\n",
    "4. 调用 spiderOnePage\n",
    "\n",
    "5. 调用 spiderOnePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coding:utf-8\n",
    "# 爬取新浪财经网股票公司每日公告\n",
    "# 提供日期即可  eg: 2017-02-21\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import threading\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "# 爬取一条公告并保存\n",
    "def spiderOnePiece(iurl,headers,datetime,base_dir,filename):\n",
    "    # 去除文件名中的非法字符\n",
    "    invaild=['*','\\\\','/',':','\\\"','<','>','|','?']\n",
    "    for c in invaild:\n",
    "        if c in filename:\n",
    "            filename=filename.replace(c,'')\n",
    "    print(\"    公告链接为：\",iurl)\n",
    "    try:\n",
    "        response=requests.get(iurl,headers=headers)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding\n",
    "        \n",
    "        page=etree.HTML(response.text)\n",
    "        content=page.xpath('//*[@id=\"content\"]/pre')\n",
    "        if len(content)==0:\n",
    "            return\n",
    "        content=content[0].text\n",
    "        print(content[:20])\n",
    "        filename = os.path.join(base_dir,datetime,filename)\n",
    "        with open(filename,'w+') as f:\n",
    "            f.write(content)\n",
    "    except Exception as e:\n",
    "        print(\"spiderOnePiece error:\",e)\n",
    "\n",
    "# 爬取一页\n",
    "def spiderOnePage(url,headers,datetime,base_dir):\n",
    "    website='http://vip.stock.finance.sina.com.cn'\n",
    "    try:\n",
    "        response=requests.get(url,headers=headers)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        page=etree.HTML(response.text)\n",
    "        trList=page.xpath(r'//*[@id=\"wrap\"]/div[@class=\"Container\"]/table/tbody/tr')\n",
    "\n",
    "        print(\"当前页面共有公告{}条\".format(len(trList) ))\n",
    "        if len(trList)==1:  # 爬取结束  该行（对不起没有相关记录）\n",
    "            return 0\n",
    "\n",
    "        #按日期建立文件目录\n",
    "        a_dirByDate = os.path.join(base_dir,datetime)\n",
    "        if not os.path.exists(a_dirByDate):  # 创建日期文件夹\n",
    "            os.mkdir(a_dirByDate)\n",
    "\n",
    "        for item in trList:\n",
    "            aUrl=item.xpath('th/a[1]')\n",
    "            title=aUrl[0].text    # 公告标题\n",
    "            href=aUrl[0].attrib['href']   # 公告uri\n",
    "            href=website+href    # 公告url\n",
    "\n",
    "            atype=item.xpath('td[1]')[0].text # 公告类型\n",
    "            print(\"准备爬取公告{}\".format(title))\n",
    "            a_filename = title+atype+'.txt'\n",
    "            print(\"    存储位置：\",a_filename)\n",
    "            spiderOnePiece(href,headers,datetime,base_dir,a_filename)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# 爬取一天\n",
    "def spiderOneDay(url,headers,datetime,base_dir='company_annoucement'):\n",
    "    url=url.replace('#datetime#',datetime)  # 填充日期\n",
    "    flag=1   # 爬取成功标志\n",
    "    index=1  # 起始页\n",
    "    \n",
    "    log_dir = os.path.join(base_dir,'log')\n",
    "    print(log_dir)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    log_file = os.path.join(log_dir,datetime+'.log')    \n",
    "    \n",
    "    with open(log_file,'w') as f:\n",
    "        while flag:\n",
    "            t_url=url+str(index)\n",
    "            try:\n",
    "                flag=spiderOnePage(t_url,headers,datetime,base_dir)\n",
    "            except Exception as e:\n",
    "                print('spiderOneDay err:',e)\n",
    "                flag=0\n",
    "            finally:\n",
    "                if flag:\n",
    "                    print('%s page_%d load success,continue.' %(datetime,index))\n",
    "                    f.write('%s_page_%d load success.\\n' %(datetime,index))\n",
    "                    f.flush()\n",
    "                else:\n",
    "                    print('%s page_%d load fail,end.' %(datetime,index))\n",
    "                    f.write('%s_page_%d load failed.\\n' %(datetime,index))\n",
    "                    f.flush()\n",
    "                index+=1\n",
    "    \n",
    "\n",
    "# 爬取一组天股票公司的数据\n",
    "def spiderOneGroupDays(url,headers,date_group,base_dir):\n",
    "    for idate in date_group:\n",
    "        try:\n",
    "\n",
    "            spiderOneDay(url,headers,idate,base_dir)\n",
    "            print('%s has load success.over.' %idate)\n",
    "        except Exception as e:\n",
    "            print('spiderOneGroupDays err:',e)\n",
    "            continue\n",
    "\n",
    "\n",
    "# 获取指定起始日期[包含]--结束日期[包含]之间的日期  \n",
    "def getBetweenDay(begin_date,end_date):\n",
    "    date_list=[]\n",
    "    begin_date=datetime.datetime.strptime(begin_date,'%Y-%m-%d')\n",
    "    # 现在的日期\n",
    "    now_date=datetime.datetime.strptime(time.strftime('%Y-%m-%d',time.localtime(time.time())),'%Y-%m-%d')\n",
    "    end_date=datetime.datetime.strptime(end_date,'%Y-%m-%d')\n",
    "    # 如果给出的结束日期大于现在的日期  则将今天的日期作为结束日期\n",
    "    if end_date > now_date:\n",
    "        end_date = now_date\n",
    "    while begin_date <= end_date:\n",
    "        date_str=begin_date.strftime('%Y-%m-%d')\n",
    "        date_list.append(date_str)\n",
    "        begin_date+=datetime.timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "# 将date_list 平均分成threadNum组  最后一组可能较少\n",
    "def split_date_list(date_list,threadNum):\n",
    "    # length=(len(date_list)/threadNum if len(date_list)%threadNum==0 else len(date_list)/threadNum+1)\n",
    "    length=int(math.ceil(len(date_list)*1.0/threadNum))\n",
    "    return [date_list[m:m+length] for m in range(0,len(date_list),length)]\n",
    "\n",
    "def main():\n",
    "    headers = {\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, sdch\", \n",
    "        \"Host\": \"vip.stock.finance.sina.com.cn\", \n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"Connection\": \"keep-alive\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    url='http://vip.stock.finance.sina.com.cn/corp/view/vCB_BulletinGather.php?gg_date=#datetime#&page='\n",
    "    \n",
    "    # 创建数据与日志的保存文件夹\n",
    "    base_dir = \"company_announcements\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    \n",
    "    # datetime='2017-02-19'\n",
    "    # spiderOneDay(url,headers,datetime,log_path)\n",
    "    #symbol = input(\"请输入A股股票的代码/名称/拼音\")\n",
    "    begin_date = input(\"请输入需查询的开始日期（例如：2017-01-01）：\")\n",
    "    end_date = input(\"请输入需查询的结束日期（例如：2017-01-31）：\")\n",
    "    # begin_date[包含]-->end_date[包含] 之间的所有date\n",
    "    date_list = getBetweenDay(begin_date,end_date)\n",
    "    print('%s 到 %s，共 %d 天。' % (begin_date,end_date,len(date_list)))\n",
    "    \n",
    "    # begin_date[包含]-->end_date[包含] 之间的所有date\n",
    "    date_list=getBetweenDay(begin_date,end_date)\n",
    "    print('%s-%s:%d days.' %(begin_date,end_date,len(date_list)))\n",
    "\n",
    "    cut_date_list=split_date_list(date_list,4)\n",
    "    print(cut_date_list)\n",
    "\n",
    "    threads=[]\n",
    "    for dgroup in cut_date_list:\n",
    "        t=threading.Thread(target=spiderOneGroupDays,args=(url,headers,dgroup,base_dir))\n",
    "        threads.append(t)\n",
    "\n",
    "    # 开始线程\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    # 等待所有线程结束  阻塞主线程\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    print('all load success...')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例结论\n",
    "\n",
    "这个案例较为综合的演示了：\n",
    "\n",
    "- 使用requests库爬取sina股票网站的公司公告\n",
    "\n",
    "- 使用lxml的xpath解析文档内容，提取公告文本\n",
    "\n",
    "- 使用多线程技术，加速爬取。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.063px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

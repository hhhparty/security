{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “网络数据采集” 课程\n",
    "\n",
    "# 第3章 Web页面爬取\n",
    "\n",
    "## 案例1\n",
    "\n",
    "### 名称：从百度贴吧下载多页话题内容\n",
    "\n",
    "### 内容：\n",
    "\n",
    "先了解以下百度贴吧http://tieba.baidu.com/f?\n",
    "我们定义几个函数：  \n",
    "\n",
    "- loadPage(url) 用于获取网页\n",
    "- writePage(html,filename) 用于将已获得的网页存储为本地文件\n",
    "- tiebaCrawler(url,beginpPage,endPage,keyword)用于调度，提供需要抓取的页面URLs\n",
    "- main：程序主控模块，完成基本命令行交互接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A case of crawler is used to fetch the content of baidu's tieba url, in according to user's input keywords.\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def loadPage(url):\n",
    "    \"\"\"\n",
    "        Function: Fetching url and accessing the webpage content.\n",
    "        url: the wanted webpage url.\n",
    "    \"\"\"\n",
    "    headers = {'Accept': 'text/html','User-Agent':'Mozilla/5.0',}\n",
    "    print('To send http request to %s' % url)\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "    \n",
    "    return  urllib.request.urlopen(request).read().decode('utf-8')\n",
    "\n",
    "def writePage(html,filename):\n",
    "    \"\"\"\n",
    "        Fuction: To write the content of html into a local file.\n",
    "        html: The response content.\n",
    "        filename: the local filename to be used stored the response.\n",
    "    \"\"\"\n",
    "    print('To write html into a local file %s ...' % filename)\n",
    "    with open(filename,'wb') as f:\n",
    "        f.write(html.encode('utf-8'))\n",
    "    print('Work done.')\n",
    "    \n",
    "    print('-'*10)\n",
    "    \n",
    "    print(\"for cosole debug:\")\n",
    "    print(html.encode('utf-8'))\n",
    "\n",
    "def tiebaCrawler(url,beginpPage,endPage,keyword):\n",
    "    \"\"\"\n",
    "        Function: The scheduler of tieba crawler, is used to access every wanted url in turns.\n",
    "        url: the url of baidu's tieba webpage\n",
    "        beginPage: initial page\n",
    "        endPage: end page\n",
    "        keyword: the wanted keyword \n",
    "    \"\"\"\n",
    "    filename = keyword + '_tieba.html'\n",
    "    for page in range(beginpPage,endPage+1):\n",
    "        pn = (page - 1) * 50\n",
    "        queryurl = url + '&pn=' + str(pn)\n",
    "        writePage(loadPage(queryurl),filename)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    kw = input('Pl input the wanted tieba\\'s name:' )\n",
    "    beginPage = int(input('The beginning page number:'))\n",
    "    endPage = int(input('The ending page number:'))\n",
    "    # 百度贴吧查询url例子：http://tieba.baidu.com/f?ie=utf-8&kw=%E5%8C%97%E8%88%AA&fr=search&red_tag=i2305631770\n",
    "    url = 'http://tieba.baidu.com/f?'\n",
    "    key = urllib.parse.urlencode({'kw':kw})\n",
    "    queryurl = url+ key\n",
    "    tiebaCrawler(queryurl,beginPage,endPage,kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例结论\n",
    "\n",
    "本案例演示了使用urllib访问百度贴吧，并根据用户兴趣，下载相关web页面的过程。\n",
    "\n",
    "本案例可以作为课后作业布置给学生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2\n",
    "\n",
    "### 名称\n",
    "\n",
    "爬取新浪财经网http://finance.sina.com.cn/stock/，各股票公司每日公告（爬取股票分析所需语料）\n",
    "\n",
    "###  内容\n",
    "\n",
    "本案例用于爬取新浪财经股票公司公告。\n",
    "\n",
    "基本流程如下：\n",
    "\n",
    "1. 输入开始日期、结束日期，作为查询条件，之后程序后计算之间有多少天，之后生成每一天的日期，以每天的日期作为查询条件提交到 http://vip.stock.finance.sina.com.cn/corp/view/vCB_BulletinGather.php?gg_date=&ftype=0 进行查询。\n",
    "\n",
    "2. 程序采用了多线程技术加速爬取过程。每个线程都将首先执行spiderOneGroupDays；\n",
    "\n",
    "3. 调用 spiderOneDay\n",
    "\n",
    "4. 调用 spiderOnePage\n",
    "\n",
    "5. 调用 spiderOnePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coding:utf-8\n",
    "# 爬取新浪财经网股票公司每日公告\n",
    "# 提供日期即可  eg: 2017-02-21\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import threading\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "# 爬取一条公告并保存\n",
    "def spiderOnePiece(iurl,headers,datetime,filename):\n",
    "    # 去除文件名中的非法字符\n",
    "    invaild=['*','\\\\','/',':','\\\"','<','>','|','?']\n",
    "    for c in invaild:\n",
    "        if c in filename:\n",
    "            filename=filename.replace(c,'')\n",
    "    print(\"    公告链接为：\",iurl)\n",
    "    response=requests.get(iurl,headers=headers).content\n",
    "    page=etree.HTML(response)\n",
    "    content=page.xpath('//*[@id=\"content\"]/pre')\n",
    "    if len(content)==0:\n",
    "        return\n",
    "    content=content[0].text\n",
    "    print(content[:20])\n",
    "    with open(datetime+os.sep+filename,'w') as f:\n",
    "        f.write(content.encode('utf-8'))\n",
    "\n",
    "# 爬取一页\n",
    "def spiderOnePage(url,headers,datetime):\n",
    "    website='http://vip.stock.finance.sina.com.cn'\n",
    "\n",
    "    response=requests.get(url,headers=headers).content\n",
    "    page=etree.HTML(response)\n",
    "    trList=page.xpath(r'//*[@id=\"wrap\"]/div[@class=\"Container\"]/table/tbody/tr')\n",
    "\n",
    "    print(\"当前页面共有公告{}条\".format(len(trList) ))\n",
    "    if len(trList)==1:  # 爬取结束  该行（对不起没有相关记录）\n",
    "        return 0\n",
    "\n",
    "    if not os.path.exists(datetime):  # 创建日期文件夹\n",
    "        os.mkdir(datetime)\n",
    "\n",
    "    for item in trList:\n",
    "        aUrl=item.xpath('th/a[1]')\n",
    "        title=aUrl[0].text    # 公告标题\n",
    "        href=aUrl[0].attrib['href']   # 公告uri\n",
    "        href=website+href    # 公告url\n",
    "\n",
    "        atype=item.xpath('td[1]')[0].text # 公告类型\n",
    "        print(\"准备爬取公告{}\".format(title))\n",
    "        spiderOnePiece(href,headers,datetime,title+'_'+atype+'.txt')\n",
    "    return 1\n",
    "\n",
    "# 爬取一天\n",
    "def spiderOneDay(url,headers,datetime,log_path='log'):\n",
    "    url=url.replace('#datetime#',datetime)  # 填充日期\n",
    "    flag=1   # 爬取成功标志\n",
    "    index=1  # 起始页\n",
    "    with open(log_path+os.sep+datetime+'.txt','a') as f:\n",
    "        while flag:\n",
    "            t_url=url+str(index)\n",
    "            try:\n",
    "                flag=spiderOnePage(t_url,headers,datetime)\n",
    "            except Exception as e:\n",
    "                print('err:',e)\n",
    "                flag=0\n",
    "            finally:\n",
    "                if flag:\n",
    "                    print('%s page_%d load success,continue.' %(datetime,index))\n",
    "                    f.write('%s_page_%d load success.\\n' %(datetime,index))\n",
    "                    f.flush()\n",
    "                else:\n",
    "                    print('%s page_%d load fail,end.' %(datetime,index))\n",
    "                    f.write('%s_page_%d load failed.\\n' %(datetime,index))\n",
    "                    f.flush()\n",
    "                index+=1\n",
    "    \n",
    "\n",
    "# 爬取一组天股票公司的数据\n",
    "def spiderOneGroupDays(url,headers,date_group,log_path):\n",
    "    for idate in date_group:\n",
    "        try:\n",
    "            spiderOneDay(url,headers,idate,log_path)\n",
    "            print('%s has load success.over.' %idate)\n",
    "        except Exception as e:\n",
    "            print('err:',e)\n",
    "            continue\n",
    "\n",
    "\n",
    "# 获取指定起始日期[包含]--结束日期[包含]之间的日期  \n",
    "def getBetweenDay(begin_date,end_date):\n",
    "    date_list=[]\n",
    "    begin_date=datetime.datetime.strptime(begin_date,'%Y-%m-%d')\n",
    "    # 现在的日期\n",
    "    now_date=datetime.datetime.strptime(time.strftime('%Y-%m-%d',time.localtime(time.time())),'%Y-%m-%d')\n",
    "    end_date=datetime.datetime.strptime(end_date,'%Y-%m-%d')\n",
    "    # 如果给出的结束日期大于现在的日期  则将今天的日期作为结束日期\n",
    "    if end_date>now_date:\n",
    "        end_date=now_date\n",
    "    while begin_date<=end_date:\n",
    "        date_str=begin_date.strftime('%Y-%m-%d')\n",
    "        date_list.append(date_str)\n",
    "        begin_date+=datetime.timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "# 将date_list 平均分成threadNum组  最后一组可能较少\n",
    "def split_date_list(date_list,threadNum):\n",
    "    # length=(len(date_list)/threadNum if len(date_list)%threadNum==0 else len(date_list)/threadNum+1)\n",
    "    length=int(math.ceil(len(date_list)*1.0/threadNum))\n",
    "    return [date_list[m:m+length] for m in range(0,len(date_list),length)]\n",
    "\n",
    "def main():\n",
    "    headers = {\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, sdch\", \n",
    "        \"Host\": \"vip.stock.finance.sina.com.cn\", \n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"Connection\": \"keep-alive\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    url='http://vip.stock.finance.sina.com.cn/corp/view/vCB_BulletinGather.php?gg_date=#datetime#&page='\n",
    "    \n",
    "    # 创建数据与日志的保存文件夹\n",
    "    base_dir = \"company_announcements\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    log_path = os.path.join(base_dir,'log')\n",
    "    if not os.path.exists(log_path):\n",
    "        os.mkdir(log_path)\n",
    "\n",
    "    # datetime='2017-02-19'\n",
    "    # spiderOneDay(url,headers,datetime,log_path)\n",
    "    #symbol = input(\"请输入A股股票的代码/名称/拼音\")\n",
    "    begin_date = input(\"请输入需查询的开始日期（例如：2017-01-01）：\")\n",
    "    end_date = input(\"请输入需查询的结束日期（例如：2017-01-31）：\")\n",
    "    # begin_date[包含]-->end_date[包含] 之间的所有date\n",
    "    date_list = getBetweenDay(begin_date,end_date)\n",
    "    print('%s 到 %s，共 %d 天。' % (begin_date,end_date,len(date_list)))\n",
    "    \n",
    "    # begin_date[包含]-->end_date[包含] 之间的所有date\n",
    "    date_list=getBetweenDay(begin_date,end_date)\n",
    "    print('%s-%s:%d days.' %(begin_date,end_date,len(date_list)))\n",
    "\n",
    "    cut_date_list=split_date_list(date_list,4)\n",
    "    print(cut_date_list)\n",
    "\n",
    "    threads=[]\n",
    "    for dgroup in cut_date_list:\n",
    "        t=threading.Thread(target=spiderOneGroupDays,args=(url,headers,dgroup,log_path,))\n",
    "        threads.append(t)\n",
    "\n",
    "    # 开始线程\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    # 等待所有线程结束  阻塞主线程\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    print('all load success...')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"company_announcements\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "log_path = os.path.join(base_dir,'log')\n",
    "if not os.path.exists(log_path):\n",
    "    os.mkdir(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.063px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

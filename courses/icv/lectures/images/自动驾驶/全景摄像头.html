<!DOCTYPE html>
<!-- saved from url=(0044)https://ieeexplore.ieee.org/document/9046805 -->
<html lang="en-US" class=" js postmessage history draganddrop borderimage borderradius boxshadow textshadow cssgradients csstransforms csstransforms3d csstransitions generatedcontent localstorage sessionstorage" style="--scroll-y:18149;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style type="text/css">@charset "UTF-8";[ng\:cloak],[ng-cloak],[data-ng-cloak],[x-ng-cloak],.ng-cloak,.x-ng-cloak,.ng-hide:not(.ng-hide-animate){display:none !important;}ng\:form{display:block;}.ng-animate-shim{visibility:hidden;}.ng-anchor{position:absolute;}</style><script src="./全景摄像头_files/utag.js" type="text/javascript" async=""></script><script async="" type="text/javascript" src="https://securepubads.g.doubleclick.net/tag/js/gpt.js"></script><script type="text/javascript">

var home = {	
			metadata:{
				searchCount: '5,550,174',
				logoRelPath: '/customer_logos',
				thirdParthAuth: false,
				currentPage:  'document',
				xploreVirtual:'https://ieeexplore.ieee.org',
				isWebAccount: false,
				isProvisioned: false,
				globalNotification:{},
				cart: {
						count: 0
				}
			}						
		};
		





		
</script>


	
	
					
						
				
					
			 
			
				
		<meta name="Description" id="meta-description" content="Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on ">
		
		<link rel="canonical" href="https://ieeexplore.ieee.org/document/9046805/"> 
		
		
		
		<!-- Disable "click" touch event 300ms delay for Chrome/Firefox on Android -->
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		

		<title>A Survey of Autonomous Driving: Common Practices and Emerging Technologies | IEEE Journals &amp; Magazine | IEEE Xplore</title>
		

	
		
			<meta property="twitter:title" content="A Survey of Autonomous Driving: &lt;i&gt;Common Practices and Emerging Technologies&lt;/i&gt;">
			<meta property="og:title" content="A Survey of Autonomous Driving: &lt;i&gt;Common Practices and Emerging Technologies&lt;/i&gt;">
		
		
			<meta property="twitter:description" content="Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.">
			<meta property="og:description" content="Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.">
		
		
		
			<meta name="twitter:card" content="summary_large_image">
    		<meta property="twitter:image" content="https://ieeexplore.ieee.org/ielx7/6287639/8948470/9046805/graphical_abstract/access-gagraphic-2983149.jpg">
			<meta property="og:image" content="https://ieeexplore.ieee.org/ielx7/6287639/8948470/9046805/graphical_abstract/access-gagraphic-2983149.jpg">
  		
  		
		
						 	
	
	

		


<script src="./全景摄像头_files/settings.js" async=""></script>
<link rel="stylesheet" type="text/css" href="./全景摄像头_files/cookieconsent.min.css">
<script src="./全景摄像头_files/cookieconsent.min.js" async=""></script>
<script>
window.addEventListener("load", function(){
 window.cookieconsent.initialise(json)
});
</script>
		





 


	
	
	
	
	
	
	
	
	
	
	
	
	

	

	

	
	
	

	

	

	
	
	
	
	
	

	

	
	
	
	
	
	
	
	
	
	
	
		
	

	

	

	
		
	
		
		
			
			
	



<meta http-equiv="X-UA-Compatible" content="IE=Edge">






<style>/* You can add global styles to this file, and also import other style files */
/*
 * From http://simbo.github.io/2014/03/less-media-query-mixins.html
 * `@screenwidth-*` variables found in `variables.less`
*/
/* =============================================================================
   Media queries for different screen sizes
   ========================================================================== */
/* ============================================================
   Clearfix Hack
   ------------------------------------------------------------
   This is a short-term hack to apply clearfix without 
   modifying with the markup during the css cleanup process.  
   All instances of this hack should be replaced with a
   'clearfix' class on the element.
   ============================================================ */
/*
 * Common UI Element styling
 */
.noUi-target,.noUi-target *{-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;-webkit-user-select:none;touch-action:none;-ms-user-select:none;-moz-user-select:none;user-select:none;box-sizing:border-box}
.noUi-target{position:relative}
.noUi-base,.noUi-connects{width:100%;height:100%;position:relative;z-index:1}
.noUi-connects{overflow:hidden;z-index:0}
.noUi-connect,.noUi-origin{will-change:transform;position:absolute;z-index:1;top:0;right:0;height:100%;width:100%;-ms-transform-origin:0 0;-webkit-transform-origin:0 0;-webkit-transform-style:preserve-3d;transform-origin:0 0;transform-style:flat}
.noUi-txt-dir-rtl.noUi-horizontal .noUi-origin{left:0;right:auto}
.noUi-vertical .noUi-origin{top:-100%;width:0}
.noUi-horizontal .noUi-origin{height:0}
.noUi-handle{-webkit-backface-visibility:hidden;backface-visibility:hidden;position:absolute}
.noUi-touch-area{height:100%;width:100%}
.noUi-state-tap .noUi-connect,.noUi-state-tap .noUi-origin{transition:transform .3s}
.noUi-state-drag *{cursor:inherit!important}
.noUi-horizontal{height:18px}
.noUi-horizontal .noUi-handle{width:34px;height:28px;right:-17px;top:-6px}
.noUi-vertical{width:18px}
.noUi-vertical .noUi-handle{width:28px;height:34px;right:-6px;bottom:-17px}
.noUi-txt-dir-rtl.noUi-horizontal .noUi-handle{left:-17px;right:auto}
.noUi-target{background:#FAFAFA;border-radius:4px;border:1px solid #D3D3D3;box-shadow:inset 0 1px 1px #F0F0F0,0 3px 6px -5px #BBB}
.noUi-connects{border-radius:3px}
.noUi-connect{background:#3FB8AF}
.noUi-draggable{cursor:ew-resize}
.noUi-vertical .noUi-draggable{cursor:ns-resize}
.noUi-handle{border:1px solid #D9D9D9;border-radius:3px;background:#FFF;cursor:default;box-shadow:inset 0 0 1px #FFF,inset 0 1px 7px #EBEBEB,0 3px 6px -3px #BBB}
.noUi-active{box-shadow:inset 0 0 1px #FFF,inset 0 1px 7px #DDD,0 3px 6px -3px #BBB}
.noUi-handle:after,.noUi-handle:before{content:"";display:block;position:absolute;height:14px;width:1px;background:#E8E7E6;left:14px;top:6px}
.noUi-handle:after{left:17px}
.noUi-vertical .noUi-handle:after,.noUi-vertical .noUi-handle:before{width:14px;height:1px;left:6px;top:14px}
.noUi-vertical .noUi-handle:after{top:17px}
[disabled] .noUi-connect{background:#B8B8B8}
[disabled] .noUi-handle,[disabled].noUi-handle,[disabled].noUi-target{cursor:not-allowed}
.noUi-pips,.noUi-pips *{box-sizing:border-box}
.noUi-pips{position:absolute;color:#999}
.noUi-value{position:absolute;white-space:nowrap;text-align:center}
.noUi-value-sub{color:#ccc;font-size:10px}
.noUi-marker{position:absolute;background:#CCC}
.noUi-marker-sub{background:#AAA}
.noUi-marker-large{background:#AAA}
.noUi-pips-horizontal{padding:10px 0;height:80px;top:100%;left:0;width:100%}
.noUi-value-horizontal{transform:translate(-50%,50%)}
.noUi-rtl .noUi-value-horizontal{transform:translate(50%,50%)}
.noUi-marker-horizontal.noUi-marker{margin-left:-1px;width:2px;height:5px}
.noUi-marker-horizontal.noUi-marker-sub{height:10px}
.noUi-marker-horizontal.noUi-marker-large{height:15px}
.noUi-pips-vertical{padding:0 10px;height:100%;top:0;left:100%}
.noUi-value-vertical{transform:translate(0,-50%);padding-left:25px}
.noUi-rtl .noUi-value-vertical{transform:translate(0,50%)}
.noUi-marker-vertical.noUi-marker{width:5px;height:2px;margin-top:-1px}
.noUi-marker-vertical.noUi-marker-sub{width:10px}
.noUi-marker-vertical.noUi-marker-large{width:15px}
.noUi-tooltip{display:block;position:absolute;border:1px solid #D9D9D9;border-radius:3px;background:#fff;color:#000;padding:5px;text-align:center;white-space:nowrap}
.noUi-horizontal .noUi-tooltip{transform:translate(-50%,0);left:50%;bottom:120%}
.noUi-vertical .noUi-tooltip{transform:translate(0,-50%);top:50%;right:120%}
.noUi-horizontal .noUi-origin>.noUi-tooltip{transform:translate(50%,0);left:auto;bottom:10px}
.noUi-vertical .noUi-origin>.noUi-tooltip{transform:translate(0,-18px);top:auto;right:28px}
.button-blue {
  color: #FFF;
  background: #069;
  border-radius: 3px;
  transition: all 0.3s ease-in-out 0s;
}
.button-blue:hover {
  background: #0081C1;
}
.button-blue:active {
  background: #17445A;
}
.button-blue:disabled,
.button-blue.disabled {
  color: #BBB;
  background: #f8f8f8;
  cursor: pointer;
  pointer-events: none;
  border: 1px solid #BBB;
}
.color-xplore-blue {
  color: #069;
}
.color-xplore-green {
  color: #006600;
}
.white {
  color: white;
}
.color-gray-dark {
  color: #B7B7B7;
}
@media only screen and (min-width: 768px) {
  .body-resp:not(.ea-only)  .icon-size-md {
    font-size: 25px;
  }
}
@media only screen and (min-width: 1025px) {
  .ea-only  .icon-size-md {
    font-size: 25px;
  }
}
@media only screen and (max-width: 767px) {
  .icon-size-md {
    font-size: 20px;
  }
}
@media only screen and (max-width: 767px) {
  .icon-size-md {
    font-size: 20px;
  }
}
.ut-text-dec-none {
  text-decoration: none;
}
.ut-text-dec-none:hover {
  text-decoration: none;
}
.global-text-header {
  color: #196600;
  font-family: Helvetica Neue, Helvetica, Arial, sans-serif;
  text-transform: capitalize;
  font-size: 32px;
  font-weight: 400;
  line-height: 36px;
  padding-top: 9px;
  padding-bottom: 9px;
  margin-top: 0;
  margin-left: 0;
  margin-bottom: 0;
}
@media only screen and (min-width: 768px) {
  .body-resp:not(.ea-only)  .text-sm-md {
    font-size: 15px !important;
  }
  .body-resp:not(.ea-only)  .text-sm-md-lh {
    font-size: 15px !important;
    line-height: 20px !important;
  }
  .body-resp:not(.ea-only)  .text-base-md {
    font-size: 18px !important;
  }
  .body-resp:not(.ea-only)  .text-base-md-lh {
    font-size: 18px !important;
    line-height: 30px !important;
  }
  .body-resp:not(.ea-only)  .text-md-md {
    font-size: 21px !important;
  }
  .body-resp:not(.ea-only)  .text-md-md-lh {
    font-size: 21px !important;
    line-height: 30px !important;
  }
  .body-resp:not(.ea-only)  .text-lg-md {
    font-size: 24px !important;
  }
  .body-resp:not(.ea-only)  .text-xl-md {
    font-size: 30px !important;
  }
  .body-resp:not(.ea-only)  .text-2xl-md {
    font-size: 32px !important;
  }
  .body-resp:not(.ea-only)  .text-2xl-md-lh {
    font-size: 32px !important;
    line-height: 38px !important;
  }
  .body-resp:not(.ea-only)  .text-3xl-md {
    font-size: 36px !important;
  }
}
@media only screen and (min-width: 1025px) {
  .ea-only  .text-sm-md {
    font-size: 15px !important;
  }
  .ea-only  .text-sm-md-lh {
    font-size: 15px !important;
    line-height: 20px !important;
  }
  .ea-only  .text-base-md {
    font-size: 18px !important;
  }
  .ea-only  .text-base-md-lh {
    font-size: 18px !important;
    line-height: 30px !important;
  }
  .ea-only  .text-md-md {
    font-size: 21px !important;
  }
  .ea-only  .text-md-md-lh {
    font-size: 21px !important;
    line-height: 30px !important;
  }
  .ea-only  .text-lg-md {
    font-size: 24px !important;
  }
  .ea-only  .text-xl-md {
    font-size: 30px !important;
  }
  .ea-only  .text-2xl-md {
    font-size: 32px !important;
  }
  .ea-only  .text-2xl-md-lh {
    font-size: 32px !important;
    line-height: 38px !important;
  }
  .ea-only  .text-3xl-md {
    font-size: 36px !important;
  }
}
span ~ a,
span ~ i,
span ~ button,
span ~ span {
  padding-left: 0.25rem;
}
a ~ a {
  padding-left: 0.25rem;
}
.global-margin-px {
  padding-right: 15px;
  padding-left: 15px;
}
.global-margin-pr {
  padding-right: 15px;
}
.global-margin-pl {
  padding-left: 15px;
}
.global-viewport-margin {
  margin: 0 auto;
  max-width: 1680px !important;
}
@media screen and (-ms-high-contrast: active), (-ms-high-contrast: none) {
  .global-viewport-margin {
    max-width: none !important;
  }
}
.global-content-width-w-lr {
  width: calc(100% - 300px);
  flex: 0 0 calc(100% - 300px);
  max-width: 100%;
}
.global-content-width-w-rr {
  width: calc(100% - 330px);
  flex: 0 0 calc(100% - 330px);
  max-width: 100%;
}
.global-content-width-w-rr-lr {
  width: calc(100% - 330px - 300px);
  flex: 0 0 calc(100% - 330px - 300px);
  max-width: 100%;
}
.global-content-width {
  width: 100%;
  flex: 0 0 100%;
  max-width: 100%;
}
.global-left-rail {
  box-sizing: border-box;
  max-width: 300px !important;
  flex: 0 0 300px;
}
.global-right-rail {
  box-sizing: border-box;
  max-width: 330px !important;
  flex: 0 0 330px;
}
.noscroll {
  overflow: hidden;
}
@media only screen and (max-width: 767px) {
  .subscription-details p .learn-more-link {
    float: none;
    flex-grow: 1;
    justify-content: center;
    display: flex;
    padding: 1.5rem;
    margin: 1.5rem;
    margin-top: 0;
    font-size: 18px;
  }
}
@media only screen and (max-width: 767px) {
  .subscription-details p .learn-more-link {
    float: none;
    flex-grow: 1;
    justify-content: center;
    display: flex;
    padding: 1.5rem;
    margin: 1.5rem;
    margin-top: 0;
    font-size: 18px;
  }
}
</style><style>.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: block;
  max-width: 276px;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  font-style: normal;
  font-weight: 400;
  line-height: 1.5;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  letter-spacing: normal;
  word-break: normal;
  word-spacing: normal;
  white-space: normal;
  line-break: auto;
  font-size: 0.875rem;
  word-wrap: break-word;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 0.3rem;
}
.popover .arrow {
  position: absolute;
  display: block;
  width: 1rem;
  height: 0.5rem;
  margin: 0 0.3rem;
}
.popover .arrow::before, .popover .arrow::after {
  position: absolute;
  display: block;
  content: "";
  border-color: transparent;
  border-style: solid;
}
.bs-popover-top, .bs-popover-auto[x-placement^=top] {
  margin-bottom: 0.5rem;
}
.bs-popover-top > .arrow, .bs-popover-auto[x-placement^=top] > .arrow {
  bottom: calc(-0.5rem - 1px);
}
.bs-popover-top > .arrow::before, .bs-popover-auto[x-placement^=top] > .arrow::before {
  bottom: 0;
  border-width: 0.5rem 0.5rem 0;
  border-top-color: rgba(0, 0, 0, 0.25);
}
.bs-popover-top > .arrow::after, .bs-popover-auto[x-placement^=top] > .arrow::after {
  bottom: 1px;
  border-width: 0.5rem 0.5rem 0;
  border-top-color: #fff;
}
.bs-popover-right, .bs-popover-auto[x-placement^=right] {
  margin-left: 0.5rem;
}
.bs-popover-right > .arrow, .bs-popover-auto[x-placement^=right] > .arrow {
  left: calc(-0.5rem - 1px);
  width: 0.5rem;
  height: 1rem;
  margin: 0.3rem 0;
}
.bs-popover-right > .arrow::before, .bs-popover-auto[x-placement^=right] > .arrow::before {
  left: 0;
  border-width: 0.5rem 0.5rem 0.5rem 0;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.bs-popover-right > .arrow::after, .bs-popover-auto[x-placement^=right] > .arrow::after {
  left: 1px;
  border-width: 0.5rem 0.5rem 0.5rem 0;
  border-right-color: #fff;
}
.bs-popover-bottom, .bs-popover-auto[x-placement^=bottom] {
  margin-top: 0.5rem;
}
.bs-popover-bottom > .arrow, .bs-popover-auto[x-placement^=bottom] > .arrow {
  top: calc(-0.5rem - 1px);
}
.bs-popover-bottom > .arrow::before, .bs-popover-auto[x-placement^=bottom] > .arrow::before {
  top: 0;
  border-width: 0 0.5rem 0.5rem 0.5rem;
  border-bottom-color: rgba(0, 0, 0, 0.25);
}
.bs-popover-bottom > .arrow::after, .bs-popover-auto[x-placement^=bottom] > .arrow::after {
  top: 1px;
  border-width: 0 0.5rem 0.5rem 0.5rem;
  border-bottom-color: #fff;
}
.bs-popover-bottom .popover-header::before, .bs-popover-auto[x-placement^=bottom] .popover-header::before {
  position: absolute;
  top: 0;
  left: 50%;
  display: block;
  width: 1rem;
  margin-left: -0.5rem;
  content: "";
  border-bottom: 1px solid #f7f7f7;
}
.bs-popover-left, .bs-popover-auto[x-placement^=left] {
  margin-right: 0.5rem;
}
.bs-popover-left > .arrow, .bs-popover-auto[x-placement^=left] > .arrow {
  right: calc(-0.5rem - 1px);
  width: 0.5rem;
  height: 1rem;
  margin: 0.3rem 0;
}
.bs-popover-left > .arrow::before, .bs-popover-auto[x-placement^=left] > .arrow::before {
  right: 0;
  border-width: 0.5rem 0 0.5rem 0.5rem;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.bs-popover-left > .arrow::after, .bs-popover-auto[x-placement^=left] > .arrow::after {
  right: 1px;
  border-width: 0.5rem 0 0.5rem 0.5rem;
  border-left-color: #fff;
}
.popover-header {
  padding: 0.5rem 0.75rem;
  margin-bottom: 0;
  font-size: 1rem;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-top-left-radius: calc(0.3rem - 1px);
  border-top-right-radius: calc(0.3rem - 1px);
}
.popover-header:empty {
  display: none;
}
.popover-body {
  padding: 0.5rem 0.75rem;
  color: #212529;
}</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><script type="text/javascript" async="" charset="utf-8" id="utag_ieeexplore.main_32" src="./全景摄像头_files/utag.32.js"></script><script type="text/javascript" async="" charset="utf-8" id="utag_ieeexplore.main_34" src="./全景摄像头_files/utag.34.js"></script><script type="text/javascript" async="" charset="utf-8" id="tiqapp" src="./全景摄像头_files/utag.v.js"></script><style>#gs-casa-r,#gs-casa-c,#gs-casa-b,#gs-casa-f,#gs-casa-h,#gs-casa-r::before,#gs-casa-c::before,#gs-casa-b::before,#gs-casa-f::before,#gs-casa-h::before,#gs-casa-r::after,#gs-casa-c::after,#gs-casa-b::after,#gs-casa-f::after,#gs-casa-h::after{background:transparent;border:0;box-sizing:content-box;content:normal;font-family:Arial,sans-serif;font-weight:normal;letter-spacing:normal;line-height:normal;margin:0;opacity:1;outline:none;overflow:visible;padding:0;pointer-events:auto;text-decoration:none;text-transform:none;transition:none;vertical-align:baseline;z-index:0}#gs-casa-r{height:0;position:absolute;right:0;top:0;width:0;z-index:2147483647}#gs-casa-c{bottom:100px;height:110px;overflow:hidden;pointer-events:none;position:fixed;right:0;width:116px}#gs-casa-c.gs-casa-top{bottom:75%;position:fixed}#gs-casa-c.gs-casa-mid{position:absolute;bottom:0}#gs-casa-b{animation:gs-casa-a-l-ent .225s cubic-bezier(.0,.0,.2,1) forwards;perspective:1px;position:absolute;right:0;text-align:center;top:20px;width:96px}#gs-casa-b::before,#gs-casa-b::after{border-radius:8px 0 0 8px;content:"";height:100%;left:0;perspective:1px;position:absolute;top:0;transform:translate3d(0,0,0);transition:opacity .2s;width:100%;z-index:-1}#gs-casa-b::before{box-shadow:0 0px 2px 0 rgba(0,0,0,.14),0 2px 2px 0 rgba(0,0,0,.12),0 4px 15px 0 rgba(0,0,0,.2);opacity:1}#gs-casa-b::after{box-shadow:0 8px 10px 1px rgba(0,0,0,.14),0 3px 14px 3px rgba(0,0,0,.12),0 4px 15px 0 rgba(0,0,0,.2);opacity:0}#gs-casa-b:active::before{opacity:0}#gs-casa-b:active::after{opacity:1}#gs-casa-f,#gs-casa-h{display:block;overflow:hidden;padding-left:4px}#gs-casa-f{background-color:#424242;border-radius:8px 0 0 0;color:#fff;font-size:18px;height:54px;line-height:54px;word-spacing:-2px}#gs-casa-h{background-color:#777;border-radius:0 0 0 8px;color:#e0e0e0;font-size:11px;height:16px;line-height:16px}@keyframes gs-casa-a-l-ent{0%{transform:translate3d(96px,0,0)}100%{transform:translate3d(0,0,0)}}.gs-casa-touch #gs-casa-c{bottom:30vh;position:fixed;transform:translate(0,48px)}@media (max-width:599px),(max-height:599px){#gs-casa-c,#gs-casa-c.gs-casa-top,#gs-casa-c.gs-casa-mid{border-radius:6px 0 0 6px;bottom:30vh;height:96px;position:fixed;transform:translate(0,48px);width:92px}#gs-casa-b{width:72px}#gs-casa-f,#gs-casa-h{padding-left:3px}#gs-casa-f{border-radius:6px 0 0 0;font-size:14px;height:44px;line-height:44px}#gs-casa-h{border-radius:0 0 0 6px;font-size:9px;height:12px;line-height:12px}@keyframes gs-casa-a-l-ent{0%{transform:translate3d(72px,0,0)}100%{transform:translate3d(0,0,0)}}}</style><style>.refinement-section[_ngcontent-eae-c458]   input[type=password][_ngcontent-eae-c458], .refinement-section[_ngcontent-eae-c458]   input[type=text][_ngcontent-eae-c458]{border:1px solid #b7b7b7;border-radius:0;box-sizing:border-box;width:100%}.refinement-section[_ngcontent-eae-c458]   .refinement-content[_ngcontent-eae-c458]   label[_ngcontent-eae-c458]{display:inline}.layout-btn-blue[_ngcontent-eae-c458]{padding:.25em 2.5em}.button-blue[_ngcontent-eae-c458]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c458]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c458]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c458], .button-blue[_ngcontent-eae-c458]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.global-ng-wrapper[_ngcontent-eae-c458]{max-width:1680px;margin:0 auto;min-height:80vh;box-sizing:border-box}@media only screen and (min-width:768px){.global-ng-wrapper[_ngcontent-eae-c458]{padding-right:15px;padding-left:15px}.global-ng-wrapper.disable-global-padding[_ngcontent-eae-c458]{padding-right:0;padding-left:0}}.remove-global-margin[_ngcontent-eae-c458]{max-width:none;margin:none}@media only screen and (max-width:767px){.overlay-on[_ngcontent-eae-c458]{display:none}}</style><style>.button-blue[_ngcontent-eae-c67]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c67]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c67]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c67], .button-blue[_ngcontent-eae-c67]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.gl-notification[_ngcontent-eae-c67]{left:0}.Notification--global[_ngcontent-eae-c67]{display:flex;align-items:center;margin-top:10px;padding:10px 20px;border-radius:5px;background-color:#f5f4be}.Notification-text1[_ngcontent-eae-c67]{color:#333}.stats-global-notification[_ngcontent-eae-c67]{display:flex;justify-content:center;width:100%;position:fixed;left:0;top:0;z-index:80001}.icon-container[_ngcontent-eae-c67]{margin-right:10px}.check-mark[_ngcontent-eae-c67]{font-size:1rem;color:#060}.notification-close[_ngcontent-eae-c67]{display:flex;align-items:center;font-size:1.25rem;margin-left:10px;color:#333}</style><style>.button-blue{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue:hover{background:#0081c1}.button-blue:active{background:#17445a}.button-blue.disabled,.button-blue:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.ng2-xplore-meta-nav #global-header-cart-count{padding-right:.5rem;border-right:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav #global-header-cart-count{padding-right:0}}.ng2-xplore-meta-nav .meta-nav-container{display:flex;justify-content:center}.ng2-xplore-meta-nav .global-maintenance-message-meta-nav{padding:8px;background-color:#e4a42c;border-radius:0;position:relative;text-align:center;z-index:11001;color:#000}.ng2-xplore-meta-nav .global-maintenance-message-meta-nav div p{margin:0}.ng2-xplore-meta-nav .global-maintenance-message-meta-nav .maintenance-msg{padding-right:2rem;padding-left:2rem;word-break:break-word}.ng2-xplore-meta-nav .global-maintenance-message-meta-nav .maintenance-close-button{font-size:1.25rem;padding:.25rem;position:absolute;top:0;right:.4em;color:#000;z-index:12000}.ng2-xplore-meta-nav .xplore-meta-nav{display:flex;width:100%;background-color:#17445a;box-sizing:border-box;padding:.35rem 15px;max-width:1680px}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links{width:auto;max-width:none}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-menu{-webkit-padding-start:0;padding-inline-start:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{width:auto;max-width:none;margin-left:auto}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .nav-right{width:100%;padding-right:1rem;padding-left:1rem}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .cart-container{margin-left:auto;display:flex}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel{padding-right:0;padding-left:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel .subscribe-link{text-align:center}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel .user-title{flex-basis:0;flex-grow:1;max-width:100%;text-align:center}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel .user-title>a{padding-left:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel .cart-count{display:inline}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{flex:none;max-width:none;margin-left:0;width:100%;box-sizing:border-box}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{list-style:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .personal-sign-in{z-index:100000}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{list-style:none;font-size:14px;padding-right:.75rem}@media only screen and (max-width:991px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:.5rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:1px solid #fff}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child).remove-border{border-right:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:none}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:.75rem}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{text-decoration:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{padding-left:.75rem}}.ng2-xplore-meta-nav .xplore-meta-nav .Menu--dividers.Menu--horizontal .Menu-item:not(:first-child):before{content:none}.ng2-xplore-meta-nav .xplore-meta-nav .subscribe-link{color:#51c4fd;font-weight:600}.ng2-xplore-meta-nav .xplore-meta-nav a{color:#fff}.ng2-xplore-meta-nav .xplore-meta-nav .gdpr-signin-container .checkbox-disclaimer a{color:#069}.ng2-xplore-meta-nav .xplore-meta-nav .ieee-xplore{color:#e4a42c}.ng2-xplore-meta-nav .xplore-meta-nav a.gdpr-hidden-personal-signin{color:#069}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-container{position:relative;z-index:1049}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-modal-open{background-color:#fff;padding:0 .75rem;border:1px solid #3d7ea8;border-bottom:none}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-modal-open a{color:#069;font-weight:800;letter-spacing:-.47px}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-modal-open{background-color:inherit}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-modal-open a{color:#fff;font-weight:400;letter-spacing:0}}</style><style>.button-blue[_ngcontent-eae-c452]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c452]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c452]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c452], .button-blue[_ngcontent-eae-c452]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.bg-hero-img[_ngcontent-eae-c452]{background-image:url(/Xplorehelp/cfg/heroImgs/bgimg_hero.jpg);background-size:cover;background-repeat:no-repeat;background-position:top;width:100%;height:410px;margin-right:auto;margin-left:auto;display:flex;flex-direction:column}@media only screen and (max-width:767px){.bg-hero-img[_ngcontent-eae-c452]{background-image:url(/Xplorehelp/cfg/heroImgs/bgimg_hero_tablet.jpg);height:auto}}@media only screen and (max-width:575px){.bg-hero-img[_ngcontent-eae-c452]{background-image:url(/Xplorehelp/cfg/heroImgs/bgimg_hero_mobile.jpg)}}.main-header[_ngcontent-eae-c452]{position:relative;display:flex;flex-direction:column;background-color:#14303e}.header-overlay[_ngcontent-eae-c452]{position:absolute;width:100%;background-color:rgba(23,68,90,.75);height:calc(18px*2 + 35px)}.header-text[_ngcontent-eae-c452]{width:100%;text-align:center;display:flex;flex-direction:column;align-items:center}.header-text[_ngcontent-eae-c452]   .header-small-text[_ngcontent-eae-c452]{text-shadow:2px 2px 4px #000;color:#fff;font-size:.88rem;font-family:Lato-Bold,Arial,sans-serif}@media only screen and (max-width:575px){.header-text[_ngcontent-eae-c452]   .header-small-text[_ngcontent-eae-c452]{font-size:.75rem}}.header-text[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]   span[_ngcontent-eae-c452]{padding:0 .25rem}.header-text[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]   a[_ngcontent-eae-c452]{padding:0 .35rem}.header-home[_ngcontent-eae-c452]{margin:8.6rem 0 1.4rem;font-family:"IBM Plex Serif",Arial,sans-serif}@media only screen and (max-width:767px){.header-home[_ngcontent-eae-c452]{margin:12.5rem 0 .85rem}}.header-home[_ngcontent-eae-c452]   h1[_ngcontent-eae-c452]{margin:0;font-size:34px;color:#ffdd7c;text-shadow:2px 2px 4px #000}@media only screen and (max-width:767px){.header-home[_ngcontent-eae-c452]   h1[_ngcontent-eae-c452]{font-size:2rem}}@media only screen and (max-width:575px){.header-home[_ngcontent-eae-c452]   h1[_ngcontent-eae-c452]{font-size:1.5rem}}@media only screen and (max-width:405px){.header-home[_ngcontent-eae-c452]   h1[_ngcontent-eae-c452]{font-size:1rem}}.header-home[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]{font-size:34px}@media only screen and (max-width:575px){.header-home[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]{font-size:1.5rem}}.header-home[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]   a[_ngcontent-eae-c452]{color:#0fe41d;font-size:2rem}.header-dashboard[_ngcontent-eae-c452]   .header-small-text[_ngcontent-eae-c452]{font-family:Lato-Bold,Arial,sans-serif;font-size:.875rem}.header-dashboard[_ngcontent-eae-c452]   .header-small-text[_ngcontent-eae-c452]   .article-numbers[_ngcontent-eae-c452]   a[_ngcontent-eae-c452]{font-size:1.5rem;color:#51c4fd}.search-bar-container[_ngcontent-eae-c452]{z-index:20;width:100%}.fill-background[_ngcontent-eae-c452]{display:flex;justify-content:center;align-items:center;position:relative;background-color:#14303e;top:auto;left:auto;transform:translate(0);width:100%;min-height:115px}</style><style>.button-blue[_ngcontent-eae-c462]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c462]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c462]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c462], .button-blue[_ngcontent-eae-c462]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.menu-link[_ngcontent-eae-c462]{padding:.25rem .5rem;white-space:nowrap}.active-hover[_ngcontent-eae-c462]{background:#069;text-decoration:none;color:#fff}.homepage[_ngcontent-eae-c462]   .menu-link[_ngcontent-eae-c462]{color:#fff;white-space:nowrap}.homepage[_ngcontent-eae-c462]   .file-cabinet-link[_ngcontent-eae-c462], .homepage[_ngcontent-eae-c462]   .hamburger-menu[_ngcontent-eae-c462]   a[_ngcontent-eae-c462]{color:#fff}@media only screen and (max-width:767px){.homepage.navbar-container[_ngcontent-eae-c462]{background-color:rgba(23,68,90,.75);padding-bottom:1rem}.homepage.navbar-container.inst-logged-in[_ngcontent-eae-c462]{padding:0}}.homepage.navbar-container.inst-logged-in[_ngcontent-eae-c462]{padding-top:18px}@media only screen and (max-width:767px){.homepage.navbar-container.inst-logged-in[_ngcontent-eae-c462]{padding-top:0}}.primary-menu[_ngcontent-eae-c462]{flex-grow:1;min-width:301px;max-width:400px;margin-top:5px}.primary-menu[_ngcontent-eae-c462]   ul[_ngcontent-eae-c462]{display:flex;list-style-type:none;flex-grow:1;justify-content:space-around;margin:0;padding:0 1.5rem 0 .75rem}.primary-menu[_ngcontent-eae-c462]   a[_ngcontent-eae-c462]{color:#fff}.hamburger-menu[_ngcontent-eae-c462]{width:33%}.hamburger-menu[_ngcontent-eae-c462]   a[_ngcontent-eae-c462]{font-size:2rem;display:flex;color:#fff}.inst-sign-in[_ngcontent-eae-c462]{color:#fff;padding:.5rem 2rem;background-color:#069;font-size:1.15rem;border-radius:2px}.institution-container[_ngcontent-eae-c462]{flex-shrink:0;margin-top:5px}.institution-container.inst-logged-in[_ngcontent-eae-c462]{margin-top:0}.right-side-container[_ngcontent-eae-c462]{flex-basis:50%;flex-shrink:1;display:flex;flex-direction:row-reverse}.right-side-container.inst-logged-in[_ngcontent-eae-c462]{width:auto;padding-left:1rem;margin-left:auto}@media only screen and (max-width:767px){.right-side-container[_ngcontent-eae-c462]{width:33%}}.ieee-logo-container[_ngcontent-eae-c462]{height:30px;margin-top:1px}.ieee-logo[_ngcontent-eae-c462]{display:flex;width:100px}@media only screen and (max-width:575px){.ieee-logo[_ngcontent-eae-c462]{display:none}}.inst-logged-in[_ngcontent-eae-c462]   .left-side-container[_ngcontent-eae-c462]{min-width:465px}.left-side-container[_ngcontent-eae-c462]{flex-basis:50%;flex-shrink:1}@media only screen and (max-width:767px){.left-side-container[_ngcontent-eae-c462]{width:33%}}.left-side-content[_ngcontent-eae-c462]{display:flex;justify-content:flex-start}@media only screen and (max-width:767px){.left-side-content[_ngcontent-eae-c462]{justify-content:center}}.left-side-content[_ngcontent-eae-c462]   .xplore-logo-wrapper[_ngcontent-eae-c462]{margin-top:-5px}@media only screen and (min-width:768px){.file-cabinet-enabled[_ngcontent-eae-c462]{padding-left:.5rem}.file-cabinet-enabled.inst-logged-in[_ngcontent-eae-c462]{padding-top:0}}.file-cabinet-container[_ngcontent-eae-c462]{display:flex;align-items:center;padding-right:.75rem}.file-cabinet-container[_ngcontent-eae-c462]   i[_ngcontent-eae-c462]{color:#fff;font-size:1rem;cursor:pointer}.file-cabinet-container[_ngcontent-eae-c462]   .file-cabinet-link[_ngcontent-eae-c462]{padding-right:.5rem;color:#fff}@media only screen and (max-width:575px){.file-cabinet-container[_ngcontent-eae-c462]   .file-cabinet-link[_ngcontent-eae-c462]{padding:0}}.navbar-container[_ngcontent-eae-c462]{box-sizing:border-box;position:absolute;display:flex;flex-direction:column;flex-grow:1;z-index:1004;width:100%;max-width:1680px;padding:18px 15px}@media only screen and (max-width:767px){.navbar-container[_ngcontent-eae-c462]{padding-top:0}}.inst-details-container[_ngcontent-eae-c462]{flex-grow:1}.top-navbar[_ngcontent-eae-c462]{display:flex}@media only screen and (max-width:767px){.top-navbar[_ngcontent-eae-c462]{padding:1rem 1rem .5rem}}.bottom-navbar[_ngcontent-eae-c462]{display:flex;justify-content:center;padding-top:calc(18px - 5px)}.navbar-container.not-homepage[_ngcontent-eae-c462]{position:relative;margin:0 auto;padding-bottom:1.5rem}.navbar-container.homepage[_ngcontent-eae-c462]{right:50%;transform:translate(50%)}.xplore-logo[_ngcontent-eae-c462]{width:160px;height:40px;margin-top:-5px}.logo-ieee[_ngcontent-eae-c462]{margin-left:auto;margin-right:2rem}@media screen and (-ms-high-contrast:none){.navbar-container[_ngcontent-eae-c462]{max-width:none;width:1460px}}</style><style>.button-blue[_ngcontent-eae-c465]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c465]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c465]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c465], .button-blue[_ngcontent-eae-c465]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}[_nghost-eae-c465]{width:100%}.not-homepage[_nghost-eae-c465]   .search-bar-wrapper[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar-wrapper[_ngcontent-eae-c465]{margin-bottom:0}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465]{display:flex;align-items:center;color:#fff;cursor:pointer}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465]   label[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465]   label[_ngcontent-eae-c465]{padding-left:.5rem;cursor:pointer;font-size:12px}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465]   input[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .check-input[_ngcontent-eae-c465]   input[_ngcontent-eae-c465]{cursor:pointer}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]{justify-content:unset;margin-left:auto;margin-right:0}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465]{background-color:transparent;padding:0;margin:0;min-width:0}@media only screen and (max-width:767px){.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465]{padding-right:.5rem}}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465]   a[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .advanced-search-div[_ngcontent-eae-c465]   a[_ngcontent-eae-c465]{font-size:12px}.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .adv-search-arrow[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]   .adv-search-arrow[_ngcontent-eae-c465]{display:none}@media only screen and (max-width:767px){.not-homepage[_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465], .not-homepage   [_nghost-eae-c465]   .search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]{padding-left:.5rem}}.search-bar[_ngcontent-eae-c465]{max-width:960px;margin:0 auto}.search-bar[_ngcontent-eae-c465]   .Notification-text[_ngcontent-eae-c465]{color:#ce1d00;background-color:#edc9c8;border-color:#edc9c8;padding:1%;max-width:500px;border-radius:.2rem;margin:0 auto;text-align:center}.search-bar[_ngcontent-eae-c465]   .below-search-bar[_ngcontent-eae-c465]{display:flex;justify-content:center;max-width:780px;flex-grow:1;margin:0 auto}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]{display:flex;justify-content:center;margin:0 32%}@media only screen and (max-width:767px){.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465]{flex-direction:column;margin:.5rem auto 0;max-width:230px}}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{min-width:200px;text-align:center;max-width:200px;background-color:rgba(23,68,90,.75);padding:.5em;margin:0 4%;border-radius:.2rem}@media only screen and (max-width:767px){.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div.top-search-btn[_ngcontent-eae-c465]{margin-bottom:3rem}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{margin:1rem 4% 1%}}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]{color:#fff;font-family:Lato-Bold,Arial,sans-serif;font-weight:700}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]   .fa-caret-right[_ngcontent-eae-c465], .search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]   .fa-minus[_ngcontent-eae-c465], .search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]   .fa-plus[_ngcontent-eae-c465]{color:#e4a42c;vertical-align:bottom;font-size:20px;padding-left:1.5%}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]   .fa-minus[_ngcontent-eae-c465], .search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]   .fa-plus[_ngcontent-eae-c465]{font-size:15px;vertical-align:middle}.search-bar[_ngcontent-eae-c465]   .advanced-search-wrapper[_ngcontent-eae-c465] > div[_ngcontent-eae-c465] > a[_ngcontent-eae-c465]:hover{text-decoration:none}.search-bar-wrapper[_ngcontent-eae-c465]{display:flex;max-width:780px;background-color:rgba(0,0,0,.1);padding:.5em;margin:0 auto 1%;justify-content:space-between}.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465]{flex-grow:0.08}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465]{flex-grow:1;max-width:4rem}}.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465] > label[_ngcontent-eae-c465]{position:relative}.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465] > label[_ngcontent-eae-c465]:after{content:"\f0d7";font-family:Font Awesome\ 5 Pro;font-size:22px;font-weight:900;color:#e4a42c;right:4px;top:-6px;padding:0 2%;position:absolute;pointer-events:none;height:19px;width:15px}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465] > label[_ngcontent-eae-c465]:after{font-size:20px;right:5px;top:-4px;text-align:center}}.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465] > label[_ngcontent-eae-c465]:before{content:"";right:4px;top:0;background:#fff;position:absolute;pointer-events:none;display:block}.search-bar-wrapper[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465] > label[_ngcontent-eae-c465] > select[_ngcontent-eae-c465]{padding:.35em .35em .35em .5rem;width:100%;height:100%;margin:0;color:#000;border:none;font-weight:700;background-color:#ddd;border-radius:0;-webkit-appearance:none;-moz-appearance:none}@media only screen and (max-width:767px){.search-bar-wrapper.wrap[_ngcontent-eae-c465]{flex-wrap:wrap}.search-bar-wrapper.wrap[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465]{flex-wrap:wrap;width:100%}.search-bar-wrapper.wrap[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{padding-top:2%}}.search-bar-wrapper.wrap[_ngcontent-eae-c465] > .drop-down[_ngcontent-eae-c465]{max-width:100%}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465]{display:flex;justify-content:space-evenly;flex-grow:1}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{flex-grow:1;padding-right:5%}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]:last-child{padding-right:0}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{padding-right:0}}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465]   .global-search-bar[_ngcontent-eae-c465]{flex-grow:1}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465]   .search-field.authors[_ngcontent-eae-c465] > div[_ngcontent-eae-c465]{width:100%}}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465]   input[_ngcontent-eae-c465]{width:100%;height:100%;box-sizing:border-box;padding:.6em;font-size:14px;font-family:sans-serif;text-transform:none;border:none}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465]{display:flex}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field[_ngcontent-eae-c465]:not(.all).search-bar-wrapper   .search-field-icon-container[_ngcontent-eae-c465]{flex-direction:column}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465]   .search-field.citations[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465]{width:48%}}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field.authors[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465], .search-bar-wrapper[_ngcontent-eae-c465]   .search-field.citations[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465]{padding-right:0}.search-bar-wrapper[_ngcontent-eae-c465]   .search-field.authors[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465]:not(:first-child)   input[_ngcontent-eae-c465], .search-bar-wrapper[_ngcontent-eae-c465]   .search-field.citations[_ngcontent-eae-c465]   .search-field-icon-container[_ngcontent-eae-c465]:not(:first-child)   input[_ngcontent-eae-c465]{border-left:1px solid #879da8}.search-bar-wrapper[_ngcontent-eae-c465]   .search-icon[_ngcontent-eae-c465]{background-color:#e4a42c;width:3.5625rem;display:flex;align-items:center;justify-content:center;cursor:pointer}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-eae-c465]   .search-icon[_ngcontent-eae-c465]{order:1}}.search-bar-wrapper[_ngcontent-eae-c465]   .search-icon[_ngcontent-eae-c465] > .fa-search[_ngcontent-eae-c465]{width:29px;height:25px;color:#000;text-align:center}.global-search-bar[_ngcontent-eae-c465]{position:relative}</style><style>.xplore-logo-container[_ngcontent-eae-c448]{display:flex;flex-direction:column}.xplore-logo-container[_ngcontent-eae-c448]   a[_ngcontent-eae-c448]{align-self:center}.xplore-logo-container[_ngcontent-eae-c448]   img.xplore-logo[_ngcontent-eae-c448]{width:160px;height:40px}</style><style>.button-blue[_ngcontent-eae-c447]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c447]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c447]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c447], .button-blue[_ngcontent-eae-c447]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.ieee-logo-container[_ngcontent-eae-c447]{display:flex;flex-direction:column}.ieee-logo-container[_ngcontent-eae-c447]   .ieee-logo[_ngcontent-eae-c447], .ieee-logo-container[_ngcontent-eae-c447]   .logo-ieee[_ngcontent-eae-c447]{width:100px;align-self:flex-end}</style><style>.button-blue[_ngcontent-eae-c457]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c457]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c457]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c457], .button-blue[_ngcontent-eae-c457]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.footer-new[_ngcontent-eae-c457]{display:flex;background-color:#17445a;padding-top:3rem;color:#fff}@media only screen and (max-width:767px){.footer-new[_ngcontent-eae-c457]{padding-top:.1rem}}.footer-new[_ngcontent-eae-c457] > div[_ngcontent-eae-c457]{padding-bottom:2rem}.footer-new[_ngcontent-eae-c457]   h3[_ngcontent-eae-c457]{font-weight:800;font-family:"IBM Plex Serif",Arial,sans-serif}.footer-new[_ngcontent-eae-c457]   ul[_ngcontent-eae-c457]{list-style-type:none;padding-left:0}.footer-new[_ngcontent-eae-c457]   li[_ngcontent-eae-c457]{padding:.35rem 0;text-transform:uppercase}.footer-new[_ngcontent-eae-c457]   a[_ngcontent-eae-c457]{width:100%;text-decoration:none;color:#fff}.footer-new[_ngcontent-eae-c457]   a[_ngcontent-eae-c457]:hover{color:#e4a42c}.footer-new[_ngcontent-eae-c457]   .follow[_ngcontent-eae-c457]   ul[_ngcontent-eae-c457]{display:flex}.footer-new[_ngcontent-eae-c457]   .follow[_ngcontent-eae-c457]   ul[_ngcontent-eae-c457]   li[_ngcontent-eae-c457]{padding:0 .5rem}.footer-new[_ngcontent-eae-c457]   .follow[_ngcontent-eae-c457]   ul[_ngcontent-eae-c457]   li[_ngcontent-eae-c457]:first-child{padding-left:0}.footer-new[_ngcontent-eae-c457]   .footer-wrapper[_ngcontent-eae-c457]{flex-grow:1;max-width:1680px;margin:0 auto}.footer-new[_ngcontent-eae-c457]   .flexible-row-col[_ngcontent-eae-c457]{display:flex;padding-bottom:3rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-eae-c457]   .flexible-row-col[_ngcontent-eae-c457]{padding-bottom:.1rem;flex-direction:column}}.footer-new[_ngcontent-eae-c457]   .footer-col[_ngcontent-eae-c457]{flex-grow:1;padding-left:2rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-eae-c457]   .footer-col[_ngcontent-eae-c457]{padding-top:1rem}.footer-new[_ngcontent-eae-c457]   .footer-col[_ngcontent-eae-c457]:first-child{padding-top:2rem}.footer-new[_ngcontent-eae-c457]   .footer-col[_ngcontent-eae-c457]:last-child{padding-bottom:.25rem}}.footer-new[_ngcontent-eae-c457]   .footer-bottom-section[_ngcontent-eae-c457]   p[_ngcontent-eae-c457]{margin:0;padding-left:2rem;padding-right:2rem}.footer-new[_ngcontent-eae-c457]   .footer-bottom-section[_ngcontent-eae-c457]   p[_ngcontent-eae-c457]:last-child{padding-top:1rem}.footer-new[_ngcontent-eae-c457]   .footer-bottom-section[_ngcontent-eae-c457]   p[_ngcontent-eae-c457]   span[_ngcontent-eae-c457]{padding-left:0}.footer-new[_ngcontent-eae-c457]   .footer-bottom-section[_ngcontent-eae-c457]   p[_ngcontent-eae-c457]   .ethics-reporting-link[_ngcontent-eae-c457]   i[_ngcontent-eae-c457]{padding-left:.25rem}.footer-new[_ngcontent-eae-c457]   .nowrap[_ngcontent-eae-c457]{white-space:nowrap}</style><style>.modal-close-container[_ngcontent-eae-c132]{top:1rem;right:1rem}.mobile-modal-close-container-courses[_ngcontent-eae-c132]{background-color:#17445a;color:#fff;padding:.5em;z-index:2;width:100%;display:flex}.mobile-modal-close-icon-courses[_ngcontent-eae-c132]{font-size:1.6rem;left:10px}.mobile-modal-title[_ngcontent-eae-c132]{margin-left:auto;margin-right:auto}.close-modal-btn[_ngcontent-eae-c132]{color:#fff;font-size:1.5rem}.close-course-modal-btn[_ngcontent-eae-c132]{color:#069;font-size:1.5rem}.grey[_ngcontent-eae-c132]{color:#919191}</style><style>.global-search-bar[_nghost-eae-c53]{box-sizing:border-box}.Typeahead-input[_ngcontent-eae-c53]{border-radius:0}</style><style>.button-blue[_ngcontent-eae-c189]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c189]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c189]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c189], .button-blue[_ngcontent-eae-c189]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}@media only screen and (max-width:767px){.document-main.sidebar-opened[_ngcontent-eae-c189]{position:absolute;top:0;padding-top:128px;height:100%;overflow:hidden;opacity:.6}}.tab-inner[_ngcontent-eae-c189]{padding-left:0;padding-right:0;padding-bottom:10px}.tab-inner[_ngcontent-eae-c189]   .tab-content[_ngcontent-eae-c189]{text-align:left;margin:20px;white-space:normal}.tab-inner[_ngcontent-eae-c189]   .tab-content[_ngcontent-eae-c189]   .action[_ngcontent-eae-c189]{text-align:center;margin-top:10px}.tab-inner[_ngcontent-eae-c189]   .tab-content[_ngcontent-eae-c189]   .export-form[_ngcontent-eae-c189]{width:100%}.tab-inner[_ngcontent-eae-c189]   .tab-content[_ngcontent-eae-c189]   .header[_ngcontent-eae-c189]   a[_ngcontent-eae-c189]{margin-top:-4px}.tab-inner[_ngcontent-eae-c189]   ul.nav.nav-tabs[_ngcontent-eae-c189]{font-size:small;border-bottom:1px solid #999;display:-moz-flex;display:-ms-flex;display:flex;box-sizing:border-box;width:100%;padding-left:10px;padding-right:10px;margin-bottom:10px}.tab-inner[_ngcontent-eae-c189]   ul.nav.nav-tabs[_ngcontent-eae-c189]   .nav-item[_ngcontent-eae-c189]   .nav-link[_ngcontent-eae-c189]{padding:.4em .3em;background-color:#b7b7b7;border:1px solid #999;margin-bottom:-1px;margin-right:.1em;box-sizing:border-box;color:#333;text-decoration:none}.tab-inner[_ngcontent-eae-c189]   ul.nav.nav-tabs[_ngcontent-eae-c189]   .nav-item[_ngcontent-eae-c189]   .nav-link.active[_ngcontent-eae-c189]{color:#069;font-weight:700;background-color:#f5f5f5;border-bottom:0}.overflow-ellipsis[_ngcontent-eae-c189]{text-overflow:ellipsis}.fullWidth[_ngcontent-eae-c189]{width:100%}.disqus-container[_ngcontent-eae-c189]{padding:0 1em}@media only screen and (max-width:767px){.document-sidebar[_ngcontent-eae-c189]{position:absolute;height:calc(100% - 129px);z-index:11000;right:0;padding:0;max-width:40vw;transform:translateX(40vw);transition:transform .25s ease-in-out 0s}}@media only screen and (max-width:575px){.document-sidebar[_ngcontent-eae-c189]{max-width:80vw;transform:translateX(80vw)}}@media only screen and (max-width:767px){.document-sidebar.opened[_ngcontent-eae-c189]{position:relative;height:100%;margin-left:calc(100% - 40vw);transform:translateX(0);flex:0 0 auto}}@media only screen and (max-width:575px){.document-sidebar.opened[_ngcontent-eae-c189]{margin-left:calc(100% - 80vw)}}@media only screen and (max-width:767px){.document-sidebar-content[_ngcontent-eae-c189]{max-height:calc(100% - 129px);width:100%;overflow:scroll;position:absolute}.document-sidebar-content.opened[_ngcontent-eae-c189]{position:relative;height:100%;max-height:100%}}.document-sidebar.top-spacing[_ngcontent-eae-c189]{margin-top:35px}@media only screen and (max-width:767px){.document-sidebar.top-spacing[_ngcontent-eae-c189]{margin-top:8.5rem}}@media only screen and (min-width:768px){.document-sidebar-rel-art[_ngcontent-eae-c189]{padding:0 15px 35px}}.header-rel-art-toggle-mobile[_ngcontent-eae-c189]{top:-2px}.related-content-link[_ngcontent-eae-c189]{margin-top:2rem;border-top:3px solid #069;border-bottom:5px solid #069}@media only screen and (max-width:767px){.related-content-link[_ngcontent-eae-c189]{display:none}}.document[_ngcontent-eae-c189]   .tab-nav[_ngcontent-eae-c189]   div.browse-pub-tab.related-content-link[_ngcontent-eae-c189] > a.document-tab-link[_ngcontent-eae-c189]{border-top:2px solid #333;border-bottom:2px solid #333;background:#f2f2f2;padding-bottom:.1rem;padding-top:.1rem}@media only screen and (max-width:767px){.document[_ngcontent-eae-c189]   .tab-nav[_ngcontent-eae-c189]   div.browse-pub-tab.related-content-link[_ngcontent-eae-c189] > a.document-tab-link[_ngcontent-eae-c189]{border:inherit;background:inherit}}.document[_ngcontent-eae-c189]   .tab-nav[_ngcontent-eae-c189]   div.browse-pub-tab.related-content-link.active[_ngcontent-eae-c189]{border-top:2px solid #333}</style><style>.button-blue[_ngcontent-eae-c141]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c141]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c141]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c141], .button-blue[_ngcontent-eae-c141]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.document-title[_ngcontent-eae-c141], .document[_ngcontent-eae-c141]   .document-header[_ngcontent-eae-c141]   .document-title-container[_ngcontent-eae-c141], .pure-g[_ngcontent-eae-c141]{letter-spacing:normal}.document-title[_ngcontent-eae-c141]{margin:0}.document-title-fix[_ngcontent-eae-c141]{flex-grow:1;width:100%}.pdf-btn-container[_ngcontent-eae-c141]{margin-left:28px}.pdf-btn-container[_ngcontent-eae-c141]   .doc-actions-link[_ngcontent-eae-c141]{height:36px;background-color:#ff3500;padding:0 25px;display:flex;align-items:center;justify-content:center;border-radius:2px;color:#fff}.pdf-btn-container[_ngcontent-eae-c141]   .doc-actions-link[_ngcontent-eae-c141]   .icon[_ngcontent-eae-c141]{font-size:1.15rem;color:#fff;margin-right:.45rem}.pdf-btn-container[_ngcontent-eae-c141]   .doc-actions-link[_ngcontent-eae-c141] > span[_ngcontent-eae-c141]{font-weight:700}.document-disqus-anchor-icon[_ngcontent-eae-c141]{display:inline-block}.document-header-inner-container[_ngcontent-eae-c141]{max-width:100%;width:100%}.document-header-breadcrumbs-container[_ngcontent-eae-c141]{padding:.4rem 1rem .8rem;margin:0;font-size:.8em}.document-header-breadcrumbs-container[_ngcontent-eae-c141]   #help[_ngcontent-eae-c141]{font-size:14px}.document-header-metrics-banner[_ngcontent-eae-c141]{padding:.4rem 1rem .8rem;width:100%}.document-header-metrics-banner.ccby-document[_ngcontent-eae-c141]{padding-bottom:0}.document-header-title-container[_ngcontent-eae-c141]{padding:.4rem 1rem .8rem;display:flex}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-eae-c141]{flex-direction:column}}.document-header-title-container[_ngcontent-eae-c141]   .right-container[_ngcontent-eae-c141]{margin-left:auto;display:flex;flex-direction:column}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-eae-c141]   .right-container[_ngcontent-eae-c141]{margin-left:0;margin-right:auto}}.document-header-title-container[_ngcontent-eae-c141]   .right-container[_ngcontent-eae-c141]   .sort-code[_ngcontent-eae-c141]{order:-1;margin-bottom:.5rem}.document-header-title-container[_ngcontent-eae-c141]   .right-container[_ngcontent-eae-c141]   div.reprod-badge-container[_ngcontent-eae-c141]:first-child{margin-bottom:.5rem}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]{display:flex;align-items:center;margin-left:auto;background:no-repeat url(/assets/dist/ng-new/styles/svgs/bg_badge_border.svg);width:192px;height:74px;cursor:pointer;background-size:cover}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]{margin-top:1rem}}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .fa-chevron-circle-down[_ngcontent-eae-c141]{padding-right:.25rem;padding-bottom:.25rem;align-self:flex-end;margin-left:auto;color:#069}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .badge[_ngcontent-eae-c141]{height:2.813rem;width:2.813rem;margin-left:1.5rem}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .badge-text[_ngcontent-eae-c141]{margin-left:.25rem;font-size:1rem;margin-bottom:.5rem;font-family:Helvetica-Nue-Bold,Arial,sans-serif;line-height:normal;color:#069;max-width:6rem;display:flex;flex-direction:column}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .badge-text[_ngcontent-eae-c141]   span[_ngcontent-eae-c141]{padding-left:0}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .badge-text-algorithm[_ngcontent-eae-c141]{margin-left:.5rem}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Available[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-available-default.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reviewed[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reviewed-default.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reproducible[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reproducible-default.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Replicated[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-replicated-default.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Available[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Replicated[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reproducible[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reviewed[_ngcontent-eae-c141]{background-size:contain;background-position:50%;background-repeat:no-repeat;margin-bottom:.25rem}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Available[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Replicated[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reproducible[_ngcontent-eae-c141], .document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]   .reproducability-badge-Reviewed[_ngcontent-eae-c141]{margin-bottom:.35rem}}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:hover   .badge-text[_ngcontent-eae-c141]{color:#333}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:hover   .reproducability-badge-Available[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-available-hover.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:hover   .reproducability-badge-Reviewed[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reviewed-hover.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:hover   .reproducability-badge-Reproducible[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reproducible-hover.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:hover   .reproducability-badge-Replicated[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-replicated-hover.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:active   .badge-text[_ngcontent-eae-c141]{color:#17445a}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:active   .reproducability-badge-Available[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-available-click.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:active   .reproducability-badge-Reviewed[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reviewed-click.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:active   .reproducability-badge-Reproducible[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-reproducible-click.svg)}.document-header-title-container[_ngcontent-eae-c141]   .badge-container[_ngcontent-eae-c141]:active   .reproducability-badge-Replicated[_ngcontent-eae-c141]{background-image:url(/assets/img/reproducability-badge-replicated-click.svg)}.document-header-title-container[_ngcontent-eae-c141]   .header-help-icon[_ngcontent-eae-c141]{font-size:18px;padding-left:.5rem}.document-header-cover-image[_ngcontent-eae-c141]{padding:.4rem 0 .8rem 1rem}.document-header-cover-image[_ngcontent-eae-c141]   img[_ngcontent-eae-c141]{border:1px solid #000;max-width:98%}.document-banner-access[_ngcontent-eae-c141]{width:100%;display:flex}.mobile-serp-nav[_ngcontent-eae-c141]{padding:.4rem 1rem .8rem}.breadcrumbs-separator[_ngcontent-eae-c141]{padding:.4rem}.book-chapter-mark-container[_ngcontent-eae-c141]{padding-top:.5rem}.book-chapter-icon-container[_ngcontent-eae-c141]{padding:.8rem 0;display:flex}.book-chapter-icon-link[_ngcontent-eae-c141]{font-size:.87rem}.book-chapter-icon-link.truncated[_ngcontent-eae-c141]{overflow:hidden;white-space:nowrap}.book-chapter-mark-container[_ngcontent-eae-c141]{max-width:80vw}.icon-open-blue-book[_ngcontent-eae-c141]{top:2px;position:relative;margin-right:.5em}.ellipsis--horizontal[_ngcontent-eae-c141]{display:flex;max-height:1.2rem;border-radius:3px;padding:0 .4rem;color:#fff;background-color:#0081c1}.ellipsis--horizontal[_ngcontent-eae-c141]:hover{opacity:.9}.ellipsis--horizontal.active[_ngcontent-eae-c141]{background-color:#17445a}.ellipsis--horizontal[_ngcontent-eae-c141]   i[_ngcontent-eae-c141]{align-self:center}.publisher-info-label[_ngcontent-eae-c141]{color:#060;font-size:1rem;font-weight:700}.btn-container[_ngcontent-eae-c141]{display:flex}.btn-container[_ngcontent-eae-c141]   .cite-this-related-btn-wrapper[_ngcontent-eae-c141]   .cite-this-btn[_ngcontent-eae-c141]{padding:.5rem;border:2px solid #069;font-weight:700}@media only screen and (max-width:767px){.btn-container.u-flex-column[_ngcontent-eae-c141]{flex-direction:column}.btn-container.u-flex-column[_ngcontent-eae-c141]   .cite-this-related-btn-wrapper[_ngcontent-eae-c141]{display:flex;margin-top:1rem}}.publisher-title-tooltip[_ngcontent-eae-c141]{margin-top:.3em;padding-right:30px}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-eae-c141]{position:relative}}.reading-room-get-program-container[_ngcontent-eae-c141]{display:flex;background-color:#fae8a9;align-items:center;color:#196600;margin-left:2.9rem;padding:.5rem;line-height:1.3}@media only screen and (max-width:767px){.reading-room-get-program-container[_ngcontent-eae-c141]{margin-left:1rem}}.reading-room-get-program-container[_ngcontent-eae-c141]   i[_ngcontent-eae-c141]{color:#196600;font-size:1.5rem;margin-right:1rem}.reading-room-get-program-container[_ngcontent-eae-c141]   .get-prog-label[_ngcontent-eae-c141]{color:#069}</style><style>.button-blue[_ngcontent-eae-c143]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c143]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c143]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c143], .button-blue[_ngcontent-eae-c143]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.action-item-container[_ngcontent-eae-c143]{width:135px}.download-pdf[_ngcontent-eae-c143]{padding-left:0!important}.copyright-icon[_ngcontent-eae-c143]{font-size:1.25rem}.red-pdf[_ngcontent-eae-c143]{font-size:1.3rem;margin-right:.15rem;font-style:normal}.red-pdf[_ngcontent-eae-c143], .red-pdf[_ngcontent-eae-c143]:after{color:#fc0d1b}.save-to-container[_ngcontent-eae-c143]{display:flex;flex-direction:column;text-align:left;padding:.5rem}.save-to-container[_ngcontent-eae-c143]   span[_ngcontent-eae-c143]{color:#000}.save-to-container[_ngcontent-eae-c143]   .myprojects-link[_ngcontent-eae-c143]{color:#069;padding-left:0;margin:.5rem 0 .25rem}.expand-my-projects[_ngcontent-eae-c143]{min-width:100vw;padding:0;position:absolute;top:0;left:0;z-index:1;font-size:.85em;background-color:#f5f5f5;border-bottom:1px solid #e5e5e5;border-right:1px solid #e5e5e5;min-height:100%;overflow-y:auto}</style><style>.button-blue[_ngcontent-eae-c137]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c137]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c137]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c137], .button-blue[_ngcontent-eae-c137]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.social-media-tool[_ngcontent-eae-c137]{display:flex;flex-direction:column}.social-media-tool[_ngcontent-eae-c137]   a[_ngcontent-eae-c137], .social-media-tool[_ngcontent-eae-c137]   i[_ngcontent-eae-c137]{color:#069;font-size:.875rem}.social-media-tool[_ngcontent-eae-c137]   a[_ngcontent-eae-c137], .social-media-tool[_ngcontent-eae-c137]   span[_ngcontent-eae-c137]:not(:first-child){padding-top:.9375rem;padding-left:0}.social-media-tool[_ngcontent-eae-c137]   .share-text[_ngcontent-eae-c137]{color:#000;font-weight:700}.social-media-tool[_ngcontent-eae-c137]   i[_ngcontent-eae-c137]{padding-right:.65rem}.doc-share-tool[_ngcontent-eae-c137]   i[_ngcontent-eae-c137], .social-media-tool[_ngcontent-eae-c137]   i[_ngcontent-eae-c137]{font-size:1.2rem}.doc-share-tool[_ngcontent-eae-c137]   i.fa-share-alt[_ngcontent-eae-c137]{color:#069}</style><style>[_nghost-eae-c156]{width:100%}.ft-toc[_ngcontent-eae-c156]{height:52px;border-top:1px solid #e5e5e5;border-bottom:1px solid #e5e5e5}.ft-toc[_ngcontent-eae-c156] > div[_ngcontent-eae-c156]{margin-top:13px}.ft-toc[_ngcontent-eae-c156]   a.toc-link[_ngcontent-eae-c156]{font-size:1.2em;font-weight:700}.ft-toc[_ngcontent-eae-c156]   a.toc-link[_ngcontent-eae-c156]:hover{text-decoration:none}.ft-toc[_ngcontent-eae-c156]   a.toc-link[_ngcontent-eae-c156]   img[_ngcontent-eae-c156]{position:relative;top:-2px;margin-right:.3em}.full-text-toc-wrapper[_ngcontent-eae-c156]   .previous-next-nav-ctrl[_ngcontent-eae-c156]{overflow:visible}.stats-document-container-fullTextSection[_ngcontent-eae-c156]{padding:0 15px}.stats-document-container-rh[_ngcontent-eae-c156]{padding-right:1em;padding-left:1em}.previous-next-nav-ctrl[_ngcontent-eae-c156]{overflow:visible}.toc-container[_ngcontent-eae-c156]{margin-bottom:.7em;font-weight:700}.toc-container[_ngcontent-eae-c156]   a[_ngcontent-eae-c156]:hover{text-decoration:none}.hide-full-text[_ngcontent-eae-c156]{max-height:0;overflow:hidden}.show-full-text[_ngcontent-eae-c156]{max-height:inherit}</style><style></style><style>.accordion-header[_ngcontent-eae-c161]{color:#069;display:flex;align-items:center}.accordion-header[_ngcontent-eae-c161]   .accordion-chevron[_ngcontent-eae-c161]   .fa[_ngcontent-eae-c161]{font-size:1.5rem}.accordion-header[_ngcontent-eae-c161]:hover{color:#0081c1}.container-active[_ngcontent-eae-c161]   .accordion-header[_ngcontent-eae-c161]{color:#17445a}</style><style>.button-blue[_ngcontent-eae-c188]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c188]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c188]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c188], .button-blue[_ngcontent-eae-c188]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.document-all-references[_ngcontent-eae-c188]{position:fixed;top:0;right:0;width:30vw;min-width:450px;padding:1rem 1rem 3rem;box-shadow:3px 10px 10px #000;overflow:auto;height:100vh;box-sizing:border-box;z-index:99999;transform:translateX(100%);background-color:#fff;transition:transform .25s ease-in-out 0s}.document-all-references.panel-opened[_ngcontent-eae-c188]{transform:translateX(0)}.reference-container[_ngcontent-eae-c188]{padding:1rem 0}.header[_ngcontent-eae-c188]{display:flex;align-items:center;padding:.5rem 0}.header[_ngcontent-eae-c188]   h1[_ngcontent-eae-c188]{margin:0;font-weight:400;color:#333}.header[_ngcontent-eae-c188]   a[_ngcontent-eae-c188]{margin-left:auto;font-size:1.25rem;color:#333}</style><style>.button-blue[_ngcontent-eae-c58]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c58]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c58]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c58], .button-blue[_ngcontent-eae-c58]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}i.help-link[_ngcontent-eae-c58]{color:#069}i.help-link[_ngcontent-eae-c58]:hover{color:#0081c1}.help-link-icon[_ngcontent-eae-c58]{font-size:1.2rem}.breadcrumb-help-link-icon[_ngcontent-eae-c58], .small-help-link-icon[_ngcontent-eae-c58]{font-size:1rem}</style><style>.button-blue[_ngcontent-eae-c122]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c122]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c122]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c122], .button-blue[_ngcontent-eae-c122]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.cite-this-container[_ngcontent-eae-c122]{border-radius:3px;padding:1.5em;border:2px solid #333;max-width:1200px;color:#333}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]{padding:0 .3rem 1rem}}.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122]{display:flex;justify-content:flex-end;font-size:.85rem;margin-top:1rem}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122]{flex-direction:column}}.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122]   .layout-btn-blue[_ngcontent-eae-c122]{padding:.75em 1.25em}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122]   .layout-btn-blue[_ngcontent-eae-c122]{padding:.75rem .25rem}}.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122] > a[_ngcontent-eae-c122]{align-self:center;padding-right:1rem}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122] > a[_ngcontent-eae-c122]{order:1;padding:1rem 0 0}}.cite-this-container[_ngcontent-eae-c122]   .btn-wrapper[_ngcontent-eae-c122] > a[_ngcontent-eae-c122]:hover{text-decoration:none}.cite-this-container[_ngcontent-eae-c122]   .tab-nav[_ngcontent-eae-c122]{margin-bottom:1rem}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .tab-nav[_ngcontent-eae-c122] > nav[_ngcontent-eae-c122]{flex-wrap:nowrap}}.cite-this-container[_ngcontent-eae-c122]   .tab-nav[_ngcontent-eae-c122]   .browse-pub-tab[_ngcontent-eae-c122]{padding:.75rem;border-right:1px solid #ddd;flex-grow:1;text-align:center}.cite-this-container[_ngcontent-eae-c122]   .tab-nav[_ngcontent-eae-c122]   .active[_ngcontent-eae-c122]{border-bottom:5px solid #17445a;font-weight:700}.cite-this-container[_ngcontent-eae-c122]   .tab-nav[_ngcontent-eae-c122]   .active[_ngcontent-eae-c122] > a[_ngcontent-eae-c122]{color:#17445a}.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]{display:flex;justify-content:space-between;margin-bottom:.5rem}.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]   .btn-container[_ngcontent-eae-c122]   a[_ngcontent-eae-c122]{text-decoration:none;padding:0}.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]   .btn-container[_ngcontent-eae-c122]   .separator[_ngcontent-eae-c122]{padding:0 .5rem}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]   .btn-container[_ngcontent-eae-c122]   .separator[_ngcontent-eae-c122]{padding:0 .3rem}}.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]   .enable-abstract[_ngcontent-eae-c122] > span[_ngcontent-eae-c122]{vertical-align:middle}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .user-selection-wrapper[_ngcontent-eae-c122]   .enable-abstract[_ngcontent-eae-c122]{margin-left:.25rem}}.cite-this-container[_ngcontent-eae-c122]   .title[_ngcontent-eae-c122]{text-align:center;padding-bottom:.75rem;font-weight:700}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .title[_ngcontent-eae-c122]{text-align:left;padding:1rem .5rem}}.cite-this-container[_ngcontent-eae-c122]   .copy-msg-container[_ngcontent-eae-c122]{padding:.5rem;border:1px solid #f7e6a4;background-color:#f9efc6;margin:0 auto;font-size:.8rem;max-width:11rem;text-align:center;position:absolute;top:8%;left:39%;z-index:9}.cite-this-container[_ngcontent-eae-c122]   .copy-msg-container[_ngcontent-eae-c122] > i[_ngcontent-eae-c122]{padding-left:1rem}.cite-this-container[_ngcontent-eae-c122]   .modal-close-container[_ngcontent-eae-c122]{top:1rem;right:1.5rem}@media only screen and (max-width:767px){.cite-this-container[_ngcontent-eae-c122]   .modal-close-container[_ngcontent-eae-c122]{top:1rem;right:.5rem}}.cite-this-container[_ngcontent-eae-c122]   .text[_ngcontent-eae-c122]{padding:1rem;font-size:.85rem;border:1px solid #ddd}.cite-this-container[_ngcontent-eae-c122]   .bgColor[_ngcontent-eae-c122]{background-color:#ccdce6}.cite-this-container[_ngcontent-eae-c122]   .ris-text[_ngcontent-eae-c122]{font-family:sans-serif;overflow:auto;max-height:20rem}</style><style>.pdf-btn-link[_ngcontent-eae-c110]{height:36px;background-color:#ff3500;padding:0 25px;display:flex;align-items:center;justify-content:center;border-radius:2px;color:#fff}.pdf-btn-link[_ngcontent-eae-c110]   .icon[_ngcontent-eae-c110]{font-size:1.15rem;color:#fff;margin-right:.45rem}.add-to-fc-link[_ngcontent-eae-c110], .pdf-btn-link[_ngcontent-eae-c110] > span[_ngcontent-eae-c110]{font-weight:700}.red-pdf[_ngcontent-eae-c110]{font-size:1.3rem;margin-right:.15rem;font-style:normal}.red-pdf[_ngcontent-eae-c110], .red-pdf[_ngcontent-eae-c110]:after{color:#fc0d1b}</style><style>.copyright-icon[_ngcontent-eae-c140]{font-size:1.25rem}.download-collabratec[_ngcontent-eae-c140]{width:100%}.save-to[_ngcontent-eae-c140]   span[_ngcontent-eae-c140]{font-weight:600;color:#000;width:100%;font-size:1rem;margin-bottom:.5rem;display:block}.save-to-container[_ngcontent-eae-c140]   div[_ngcontent-eae-c140]{padding-bottom:.5rem}</style><style>.manage-citation-alerts-container[_ngcontent-eae-c138]{width:151px}.manage-citation-alerts-container[_ngcontent-eae-c138]   .manage-alerts-link[_ngcontent-eae-c138]{margin:10px 0}</style><style>.whitespace-normal[_ngcontent-eae-c134]{white-space:normal!important}.document-authors-banner[_ngcontent-eae-c134]   .authors-container[_ngcontent-eae-c134]{padding:0 0 0 1rem}.stats-document-authors-banner[_ngcontent-eae-c134]{padding:.25rem 1rem .25rem 0}.stats-document-authors-banner[_ngcontent-eae-c134]   .authors-view-all-link-container[_ngcontent-eae-c134]{padding:0}.authors-minimized[_ngcontent-eae-c134]{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}</style><style>.toc-container[_ngcontent-eae-c142]{padding:.6em .5em;border-bottom:1px solid #ddd}.toc-heading[_ngcontent-eae-c142]{font-size:1em;padding-bottom:.5em}.toc-list[_ngcontent-eae-c142]{list-style:none;padding:0}.toc-list-item[_ngcontent-eae-c142]{padding:.5em 0}.toc-list-item[_ngcontent-eae-c142]   a.disabled[_ngcontent-eae-c142]{color:#333}.toc-list-link[_ngcontent-eae-c142]{display:flex;font-size:.9em}.toc-list-icon[_ngcontent-eae-c142]{margin-right:5px}.toc-show-more-btn[_ngcontent-eae-c142]{font-size:.85em;font-weight:700}</style><style>.abstract-first-page-preview[_ngcontent-eae-c184]{border:1px solid #000;width:100%}.abstract-graphic-asset[_ngcontent-eae-c184]{width:450px!important}.isbn-value[_ngcontent-eae-c184]{white-space:nowrap}.document-cadmore-player-container[_ngcontent-eae-c184]{padding-bottom:1rem}.funding-agency-info[_ngcontent-eae-c184]{padding-top:1rem;display:flex;flex-direction:column;padding-left:1.6875rem}.funding-agency-info[_ngcontent-eae-c184]   span[_ngcontent-eae-c184]{padding-left:0}.funding-agency-info[_ngcontent-eae-c184]:first-child{padding-top:.6875rem}</style><style>.document-resizer[_ngcontent-eae-c155]{text-align:center;display:flex;padding-right:8px}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]{line-height:normal;padding-bottom:10px}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   .small-font[_ngcontent-eae-c155]{font-size:10px}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   input[type=range][_ngcontent-eae-c155]{vertical-align:middle;width:60%;border-radius:3px;height:5px;background-color:#fff}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   input[type=range][_ngcontent-eae-c155]::-webkit-slider-runnable-track{background:#fff;border:none}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   input[type=range][_ngcontent-eae-c155]::-webkit-slider-thumb{background-color:#fff;border:1px solid #fff}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   .large-font[_ngcontent-eae-c155]{font-size:20px}.document-resizer[_ngcontent-eae-c155]   .document-resizer-slider[_ngcontent-eae-c155]   span[_ngcontent-eae-c155]{font-weight:700;margin:0 5px;vertical-align:middle;color:#fff}.search-error-msg[_ngcontent-eae-c155]{position:absolute;top:-1.25rem;color:red}.pure-button-primary[_ngcontent-eae-c155]{border:1px solid #fff}</style><style>ul.back-to-top[_ngcontent-eae-c81]{display:none;list-style:none;padding:0;width:100%}.back-to-top-btn[_ngcontent-eae-c81]{display:flex;justify-content:center;align-items:center;width:45px;height:45px;background-color:#069;opacity:.7}</style><script type="text/javascript" async="" src="./全景摄像头_files/embed.js"></script><style>.button-blue[_ngcontent-eae-c187]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c187]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c187]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c187], .button-blue[_ngcontent-eae-c187]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.header-rel-art-pub[_ngcontent-eae-c187]{margin:.25rem 0}.header-rel-art-pub[_ngcontent-eae-c187]:last-child{margin:0 0 .5rem}.header-rel-art[_ngcontent-eae-c187]{font-size:.9375rem;color:#333;border:1px solid #e5e5e5;border-top:5px solid #0081c1;background-color:#f8f8f8}@media only screen and (max-width:767px){.header-rel-art[_ngcontent-eae-c187]{border-bottom:none}}.header-rel-art-action[_ngcontent-eae-c187]   a[_ngcontent-eae-c187], .header-rel-art-title[_ngcontent-eae-c187]{font-family:Helvetica-Nue-Bold,Arial,sans-serif}.header-rel-art-item[_ngcontent-eae-c187] > span[_ngcontent-eae-c187], .header-rel-art-list[_ngcontent-eae-c187]{font-family:Helvetica Regular,Arial,sans-serif}</style><style>.ref-pop-up[_ngcontent-eae-c146]{display:block;position:absolute;width:calc(100% - 2em);z-index:1000;font-size:.8em;box-shadow:0 0 5px 1px #000;margin-top:1.05rem}.ref-pop-up[_ngcontent-eae-c146]   .close-btn-container[_ngcontent-eae-c146]{display:block;position:absolute;right:15px;top:15px;font-size:1.15rem}.ref-pop-up[_ngcontent-eae-c146]   .header[_ngcontent-eae-c146]{background-color:#17445a;height:.75rem}.ref-pop-up[_ngcontent-eae-c146]   .body[_ngcontent-eae-c146]{background-color:#fff;padding:1.25rem 45px 5px 35px}.ref-pop-up[_ngcontent-eae-c146]   .body[_ngcontent-eae-c146]   p[_ngcontent-eae-c146]{margin:0}.ref-pop-up[_ngcontent-eae-c146]   .footer[_ngcontent-eae-c146]{background-color:#fff;padding:1rem 35px 1.25rem}</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><link rel="prefetch" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.7ab903feba7624935283ca4c7d8c7203.css"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.f612a596225060ba8aa2aa903e502caf.js"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.f0599c2118b43a53a7ddae9c471ea77c.js"><link rel="prefetch" as="script" href="https://disqus.com/next/config.js"><script async="" id="dsq_recs_scr" src="./全景摄像头_files/recommendations.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.4') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.4') format('woff'), url('https://ieeexplore.ieee.org/xploreAssets/MathJax-274/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.4') format('opentype')}
</style><link rel="prefetch" as="style" href="https://c.disquscdn.com/next/recommendations/styles/recommendations.10022a97346f1c6e3798931bbd8e4bb5.css"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/recommendations/common.bundle.a3659a8e961f4dff2575f07c23268b7f.js"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/recommendations/recommendations.bundle.926bc472e4859a48daa346b4ba2ab4f4.js"><link rel="prefetch" as="script" href="https://disqus.com/next/config.js"><style></style><style type="text/css" nonce="" media="print">.usabilla_live_button_container { display: none; }</style><style type="text/css" nonce="">iframe.usabilla-live-button#usabilla_live_button_container_iframe661210976{width:125px;height:50px;margin: 0;padding: 0;border: 0;overflow: hidden;z-index: 9998;position: absolute;left: 0;top: 0;box-shadow: 0 0 0;background-color: transparent;}</style><style>ngb-tooltip-window{pointer-events:none}ngb-tooltip-window .tooltip-inner{pointer-events:auto}ngb-tooltip-window.bs-tooltip-bottom .arrow,ngb-tooltip-window.bs-tooltip-top .arrow{left:calc(50% - .4rem)}ngb-tooltip-window.bs-tooltip-bottom-left .arrow,ngb-tooltip-window.bs-tooltip-top-left .arrow{left:1em}ngb-tooltip-window.bs-tooltip-bottom-right .arrow,ngb-tooltip-window.bs-tooltip-top-right .arrow{left:auto;right:.8rem}ngb-tooltip-window.bs-tooltip-left .arrow,ngb-tooltip-window.bs-tooltip-right .arrow{top:calc(50% - .4rem)}ngb-tooltip-window.bs-tooltip-left-top .arrow,ngb-tooltip-window.bs-tooltip-right-top .arrow{top:.4rem}ngb-tooltip-window.bs-tooltip-left-bottom .arrow,ngb-tooltip-window.bs-tooltip-right-bottom .arrow{bottom:.4rem;top:auto}</style><script src="./全景摄像头_files/alfie_v4.63f1ab6d6b9d5807dc0c94ef3fe0b851.js" async="" charset="UTF-8"></script><style>.button-blue[_ngcontent-eae-c150]{color:#fff;background:#069;border-radius:3px;transition:all .3s ease-in-out 0s}.button-blue[_ngcontent-eae-c150]:hover{background:#0081c1}.button-blue[_ngcontent-eae-c150]:active{background:#17445a}.button-blue.disabled[_ngcontent-eae-c150], .button-blue[_ngcontent-eae-c150]:disabled{color:#bbb;background:#f8f8f8;cursor:pointer;pointer-events:none;border:1px solid #bbb}.close-container[_ngcontent-eae-c150]{display:flex;justify-content:flex-end;font-size:1.5rem}.close-container[_ngcontent-eae-c150]   i[_ngcontent-eae-c150]{font-size:1.5rem;padding:.5rem;cursor:pointer}@media only screen and (max-width:767px){.close-container[_ngcontent-eae-c150]   i[_ngcontent-eae-c150]{font-size:.9375rem}}.doc-images-container[_ngcontent-eae-c150]{height:700px;overflow:scroll;background:#fff;display:flex;flex-direction:column}@media only screen and (max-width:767px){.doc-images-container[_ngcontent-eae-c150]{height:100%}}.doc-images-container[_ngcontent-eae-c150]   .image-container[_ngcontent-eae-c150]{display:flex;justify-content:center;padding:1rem;margin:auto 0}.doc-images-container[_ngcontent-eae-c150]   .image-container[_ngcontent-eae-c150]   img[_ngcontent-eae-c150]{max-width:100%}.doc-images-container[_ngcontent-eae-c150]   .video-container[_ngcontent-eae-c150]{margin:auto 0;min-height:33rem}@media only screen and (max-width:767px){.doc-images-container[_ngcontent-eae-c150]   .video-container[_ngcontent-eae-c150]{min-height:10rem}}.button-container[_ngcontent-eae-c150]{display:flex;justify-content:space-between;background-color:#0081c1;font-size:1.25rem;padding:1rem}@media only screen and (max-width:767px){.button-container[_ngcontent-eae-c150]{font-size:.6rem;padding:.5rem}}.button-container[_ngcontent-eae-c150]   .disabled[_ngcontent-eae-c150], .button-container[_ngcontent-eae-c150]   .disabled[_ngcontent-eae-c150]   i[_ngcontent-eae-c150]{color:#333}.button-container[_ngcontent-eae-c150]   .separator[_ngcontent-eae-c150]{padding:0 .25rem}.button-container[_ngcontent-eae-c150]   a[_ngcontent-eae-c150], .button-container[_ngcontent-eae-c150]   i[_ngcontent-eae-c150], .button-container[_ngcontent-eae-c150]   span[_ngcontent-eae-c150]{color:#fff}.button-container[_ngcontent-eae-c150]   a[_ngcontent-eae-c150], .button-container[_ngcontent-eae-c150]   i[_ngcontent-eae-c150]{cursor:pointer;text-decoration:none}.button-container[_ngcontent-eae-c150]   .show-all[_ngcontent-eae-c150]{text-align:center}.button-container[_ngcontent-eae-c150]   .compress-icon-container[_ngcontent-eae-c150]{text-align:end;display:flex;justify-content:flex-end;padding-top:.3rem}.button-container[_ngcontent-eae-c150]   .compress-icon-container[_ngcontent-eae-c150]   i.fa-compress[_ngcontent-eae-c150]{margin-left:.5rem}</style><style>ngb-modal-window .component-host-scrollable{display:flex;flex-direction:column;overflow:hidden}</style></head><body class="body-resp cmpl_embed_complete" style=""><div role="dialog" aria-live="polite" aria-label="cookieconsent" aria-describedby="cookieconsent:desc" class="cc-window cc-banner cc-type-info cc-theme-block cc-bottom cc-color-override-170793312 " style=""><!--googleoff: all--><span id="cookieconsent:desc" class="cc-message">IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our <a aria-label="learn more about cookies" role="button" tabindex="0" class="cc-link" href="https://www.ieee.org/about/help/security_privacy.html" target="_blank">Privacy Policy.</a></span><div class="cc-compliance"><a aria-label="dismiss cookie message" role="button" tabindex="0" class="cc-btn cc-dismiss">Accept &amp; Close</a></div><!--googleon: all--></div><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div><div id="lightningjs-usabilla_live" style="display: none;"><div><iframe frameborder="0" id="lightningjs-frame-usabilla_live" src="./全景摄像头_files/saved_resource.html"></iframe></div></div><g:compress>
	<link rel="stylesheet" href="./全景摄像头_files/simplePassMeter.min.css">
	<link rel="stylesheet" type="text/css" media="screen, print" href="./全景摄像头_files/styles.css">
	<link rel="stylesheet" href="./全景摄像头_files/idangerous.swiper.css">
	<link rel="stylesheet" type="text/css" media="screen, print" href="./全景摄像头_files/jquery-ui-1.8.19.custom.css">
</g:compress>




		

	<script type="text/javascript">
		var googletag = googletag || {};
		googletag.cmd = googletag.cmd || [];
		(function() {
		var gads = document.createElement('script');
		gads.async = true;
		gads.type = 'text/javascript';
		var useSSL = 'https:' == document.location.protocol;
		gads.src = (useSSL ? 'https:' : 'http:') + 
		'//securepubads.g.doubleclick.net/tag/js/gpt.js';
		var node = document.getElementsByTagName('script')[0];
		node.parentNode.insertBefore(gads, node);
		})();
	</script>

		<script type="text/javascript" src="./全景摄像头_files/jquery-3.5.1.min.js" charset="utf-8"></script>
		
	
	
		<p class="JumpLink" id="PageTop"><a href="https://ieeexplore.ieee.org/document/#" title="Click here to Skip to main content" accesskey="s">Skip to Main Content</a></p>

		<div id="global-notification" class="row stats-global-notification">
			<div class="hide u-hide-important col Notification Notification--global Notification--fixed">
				<a href="https://ieeexplore.ieee.org/document/" class="Notification-close js-close" aria-label="close message button"><i class="fa fa-close"></i></a>
				<div class="Notification-header"></div>
				<div class="Notification-text"></div>
			</div>
		</div>

		<div id="LayoutWrapper">
			<div class="container-fluid">
				<div class="row">
					<div class="col">
						








	
		<script type="text/javascript" src="./全景摄像头_files/6Mc4deRE.js"></script>
	

	


<div class="Header" id="xplore-header" data-service="true" data-inst="false" data-web="false" style="display:none"></div>



						<div id="global-alert-message"></div>
						








<meta name="cToken" content="eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKGZibEREFsaAeUTS2AChmaWRrUAAAAA__8.m0fahXq0P48lKEPDjGKi2UGdS1mPaV7-Z9dgAb4WHHopgFpmknlDCqfJ2DTX3o2SflL0haVJX8vI_KYD2Oxj7w">

<script type="text/javascript">
	
	
	
	document.write('<base href="/document/" />');
	
	
</script><!--<base href="/document/">--><base href=".">

<!-- XPL-21560-Added as part of Universal CASA-Dev -->
<script type="text/javascript" src="./全景摄像头_files/f.txt" async=""></script>

<script type="text/javascript" src="./全景摄像头_files/embed(1).js"></script>
<!--- This is a picture popup embed for mobilew view.  -->
<script type="text/javascript" src="./全景摄像头_files/PicturePop.js"></script>







<script type="text/javascript">
var body_rightsLink ="", body_publisher = "";
var recordId = "";


var AUTHOR_PROFILE = 'ON';
if (AUTHOR_PROFILE.toUpperCase() == "OFF"){
	var AUTHOR_PROFILE_ENABLED = false;
}else{
	var AUTHOR_PROFILE_ENABLED = true;
}

var xplGlobal = {
	document: {
		disqus:{
			remote_auth_s3 : '',
			public_api_key:'1lxKgMbpNIbQvfk2tqLcWeSVloE8rgIY2CV1tCu3Vp641oL4eEITYBbkViJJGNYY',
			short_name:'ieeexplore',
			client_url:'https://ieeexplore.ieee.org',
			sso_enabled:'{$sessionProfile.provisioned}'
		},

		fullTextAccess: false,
		isAccessFromInstitution: false
		
	}
};
	
	xplGlobal.document.metadata={"userInfo":{"institute":false,"member":false,"individual":false,"guest":false,"subscribedContent":false,"fileCabinetContent":false,"fileCabinetUser":false,"institutionalFileCabinetUser":false,"showPatentCitations":true,"showGet802Link":false,"showOpenUrlLink":false,"tracked":false,"desktop":false,"delegatedAdmin":false,"isInstitutionDashboardEnabled":false,"isInstitutionProfileEnabled":false,"isRoamingEnabled":false,"isDelegatedAdmin":false,"isMdl":false,"isCwg":false},"authors":[{"name":"Ekim Yurtsever","affiliation":["Graduate School of Informatics, Nagoya University, Nagoya, Japan"],"bio":{"graphic":"/mediastore_new/IEEE/content/freeimages/6287639/8948470/9046805/yurts-2983149-small.gif","p":["Ekim Yurtsever (Member, IEEE) received the B.S. and M.S. degrees from Istanbul Technical University, in 2012 and 2014, respectively, and the Ph.D. degree in information science from Nagoya University, Japan, in 2019.","Since 2019, he has been a Postdoctoral Researcher with the Department of Electrical and Computer Engineering, Ohio State University. His research interests include artificial intelligence, machine learning, and computer vision. He is currently working on machine learning and computer vision tasks in the intelligent vehicle domain."]},"firstName":"Ekim","lastName":"Yurtsever","orcid":"0000-0002-3103-6052","id":"37085640372"},{"name":"Jacob Lambert","affiliation":["Graduate School of Informatics, Nagoya University, Nagoya, Japan"],"bio":{"graphic":"/mediastore_new/IEEE/content/freeimages/6287639/8948470/9046805/lambe-2983149-small.gif","p":["Jacob Lambert (Student Member, IEEE) received the B.S. degree (Hons.) in physics from McGill University, Montreal, Canada, in 2014, and the M.A.Sc. degree from the University of Toronto, Canada, in 2017. He is currently pursuing the Ph.D. degree with Nagoya University, Japan.","His current research focuses on 3D perception through lidar sensors for autonomous robotics."]},"firstName":"Jacob","lastName":"Lambert","id":"37087105608"},{"name":"Alexander Carballo","affiliation":["Graduate School of Informatics, Nagoya University, Nagoya, Japan"],"bio":{"graphic":"/mediastore_new/IEEE/content/freeimages/6287639/8948470/9046805/carba-2983149-small.gif","p":["Alexander Carballo (Member, IEEE) received the Dr.Eng. degree from the Intelligent Robot Laboratory, University of Tsukuba, Japan.","From 1996 to 2006, he worked as a Lecturer at the School of Computer Engineering, Costa Rica Institute of Technology. From 2011 to 2017, he worked at the Research and Development, Hokuyo Automatic Co., Ltd. Since 2017, he has been a Designated Assistant Professor with the Institutes of Innovation for Future Society, Nagoya University, Japan. His main research interests are lidar sensors, robotic perception, and autonomous driving."]},"firstName":"Alexander","lastName":"Carballo","id":"37604736800"},{"name":"Kazuya Takeda","affiliation":["Graduate School of Informatics, Nagoya University, Nagoya, Japan","Tier IV Inc., Nagoya, Japan"],"bio":{"graphic":"/mediastore_new/IEEE/content/freeimages/6287639/8948470/9046805/taked-2983149-small.gif","p":["Kazuya Takeda (Senior Member, IEEE) received the B.E.E., M.E.E., and Ph.D. degrees from Nagoya University, Japan.","Since 1985, he has been working with the Advanced Telecommunication Research Laboratories and KDD R&D Laboratories, Japan. In 1995, he started a research group for signal processing applications at Nagoya University. He is currently a Professor with the Institutes of Innovation for Future Society, Nagoya University, and with Tier IV Inc. He is also serving as a member of the Board of Governors of the IEEE ITS society. His main focus is investigating driving behavior using data centric approaches, utilizing signal corpora of real driving behavior."]},"firstName":"Kazuya","lastName":"Takeda","id":"37269965700"}],"issn":[{"format":"Electronic ISSN","value":"2169-3536"}],"articleNumber":"9046805","dbTime":"2 ms","metrics":{"citationCountPaper":196,"citationCountPatent":0,"totalDownloads":16402},"sponsors":[{"packageNumber":0,"name":"IEEE","url":"http://www.ieee.org/index.html"}],"purchaseOptions":{"showOtherFormatPricingTab":false,"showPdfFormatPricingTab":true,"pdfPricingInfoAvailable":false,"otherPricingInfoAvailable":false,"mandatoryBundle":false,"optionalBundle":false,"displayTextWhenPdfPricingNotAvailable":"The purchase and pricing options for this item are unavailable. Select items are only available as part of a subscription package. You may try again later or <a href='https://ieeexplore.ieee.org/xpl/contact' target='_blank'>contact us</a> for more information.","displayTextWhenOtherFormatPricingNotAvailable":"The purchase and pricing options for this item are unavailable. Select items are only available as part of a subscription package. You may try again later or <a href='https://ieeexplore.ieee.org/xpl/contact' target='_blank'>contact us</a> for more information."},"getProgramTermsAccepted":false,"sections":{"abstract":"true","authors":"true","figures":"true","multimedia":"false","references":"true","citedby":"true","keywords":"true","definitions":"false","algorithm":"false","dataset":"false","cadmore":"false","footnotes":"true","disclaimer":"false","relatedContent":"false","metrics":"true"},"graphicalAbstract":{"summary":"This figure illustrates outputs of state-of-the-art perception algorithms in an urban scene near Nagoya University, with camera and lidar data collected by our experimental vehicle. (a) A front facing camera's view, with overlaid bounding box results from YOLOv3. (b) Instance segmentation results from MaskRCNN. (c) Semantic segmentation masks produced by DeepLabv3. (d) The 3D Lidar data with object detection results from SECOND. Amongst the four, only the 3D perception algorithm outputs range to detected objects.","type":"graphic","fileSize":"45KB","file":"/ielx7/6287639/8948470/9046805/graphical_abstract/access-gagraphic-2983149.jpg","coverImage":"/ielx7/6287639/8948470/9046805/graphical_abstract/access-gagraphic-2983149.jpg"},"formulaStrippedArticleTitle":"A Survey of Autonomous Driving: <i>Common Practices and Emerging Technologies</i>","pdfUrl":"/stamp/stamp.jsp?tp=&arnumber=9046805","keywords":[{"type":"IEEE Keywords","kwd":["Automation","Task analysis","Systems architecture","Accidents","Planning","Vehicle dynamics","Robot sensing systems"]},{"type":"INSPEC: Controlled Indexing","kwd":["mobile robots","road vehicles","traffic engineering computing"]},{"type":"INSPEC: Non-Controlled Indexing","kwd":["real-world driving setting","human machine interfaces","high-level system architectures","safe driving experience","ADS development","automated driving systems","autonomous driving"]},{"type":"Author Keywords ","kwd":["Autonomous vehicles","control","robotics","automation","intelligent vehicles","intelligent transportation systems"]}],"allowComments":true,"abstract":"Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.","doi":"10.1109/ACCESS.2020.2983149","publicationTitle":"IEEE Access","displayPublicationTitle":"IEEE Access","pdfPath":"/iel7/6287639/8948470/09046805.pdf","startPage":"58443","endPage":"58469","issueLink":"/xpl/tocresult.jsp?isnumber=8948470","doiLink":"https://doi.org/10.1109/ACCESS.2020.2983149","pubLink":"/xpl/RecentIssue.jsp?punumber=6287639","isReadingRoomArticle":false,"isGetArticle":false,"isGetAddressInfoCaptured":false,"isMarketingOptIn":false,"pubTopics":[{"name":"Aerospace"},{"name":"Bioengineering"},{"name":"Communication, Networking and Broadcast Technologies"},{"name":"Components, Circuits, Devices and Systems"},{"name":"Computing and Processing"},{"name":"Engineered Materials, Dielectrics and Plasmas"},{"name":"Engineering Profession"},{"name":"Fields, Waves and Electromagnetics"},{"name":"General Topics for Engineers"},{"name":"Geoscience"},{"name":"Nuclear Engineering"},{"name":"Photonics and Electrooptics"},{"name":"Power, Energy and Industry Applications"},{"name":"Robotics and Control Systems"},{"name":"Signal Processing and Analysis"},{"name":"Transportation"}],"publisher":"IEEE","xploreDocumentType":"Journals & Magazine","isPromo":false,"chronOrPublicationDate":"2020","journalDisplayDateOfPublication":"25 March 2020","isSMPTE":false,"isOUP":false,"isSAE":false,"isNow":false,"isCustomDenial":false,"isNotDynamicOrStatic":false,"displayDocTitle":"A Survey of Autonomous Driving: <i>Common Practices and Emerging Technologies</i>","isStandard":false,"volume":"8","isProduct":false,"isMorganClaypool":false,"isOpenAccess":true,"isEphemera":false,"isChapter":false,"isStaticHtml":false,"isBookWithoutChapters":false,"htmlLink":"/document/9046805/","isConference":false,"dateOfInsertion":"06 April 2020","persistentLink":"https://ieeexplore.ieee.org/servlet/opac?punumber=6287639","publicationDate":"2020","accessionNumber":"19499294","isEarlyAccess":false,"isJournal":true,"isBook":false,"htmlAbstractLink":"/document/9046805/","isDynamicHtml":true,"isFreeDocument":true,"startPage":"58443","openAccessFlag":"T","ephemeraFlag":"false","title":"A Survey of Autonomous Driving: <italic>Common Practices and Emerging Technologies</italic>","accessionNumber":"19499294","html_flag":"false","ml_html_flag":"true","sourcePdf":"access-yurtsever-2983149-x.pdf","content_type":"Journals & Magazines","mlTime":"PT0.382854S","chronDate":"2020","xplore-pub-id":"6287639","pdfPath":"/iel7/6287639/8948470/09046805.pdf","isNumber":"8948470","dateOfInsertion":"06 April 2020","contentType":"periodicals","publicationDate":"2020","publicationNumber":"6287639","citationCount":"196","xplore-issue":"8948470","articleId":"9046805","publicationTitle":"IEEE Access","sections":{"abstract":"true","authors":"true","figures":"true","multimedia":"false","references":"true","citedby":"true","keywords":"true","definitions":"false","algorithm":"false","dataset":"false","cadmore":"false","footnotes":"true","disclaimer":"false","relatedContent":"false","metrics":"true"},"volume":"8","onlineDate":"25 March 2020","contentTypeDisplay":"Journals","publicationYear":"2020","subType":"IEEE Journal","_value":"CCBY","lastupdate":"2022-01-26","mediaPath":"/mediastore_new/IEEE/content/media/6287639/8948470/9046805","endPage":"58469","displayPublicationTitle":"IEEE Access","doi":"10.1109/ACCESS.2020.2983149"};








</script>



<div class="ng2-app">
	
	
		<!-- <xpl-searchbar show-count="false"></xpl-searchbar> -->
	
	<div class="global-content-wrapper">
		<xpl-root _nghost-eae-c458="" ng-version="11.2.14"><xpl-meta-nav _ngcontent-eae-c458=""><div class="ng2-xplore-meta-nav"><!----><!----><div class="metanav-container u-flex-display-flex u-flex-justify-center"><div class="stats-metanav xplore-meta-nav"><div class="meta-nav-ieee-links hide-mobile text-sm-md-lh"><ul class="meta-nav-menu u-flex-display-flex u-m-0"><li class="meta-nav-item stats-extLink stats-Unav_exit_aaa"><a href="http://www.ieee.org/" id="u-home" class="ieeeorg">IEEE.org</a></li><li class="meta-nav-item stats-extLink ieee-xplore">IEEE <em>Xplore</em></li><li class="meta-nav-item stats-extLink"><a href="http://standards.ieee.org/" id="u-standards" class="exitstandardsorg">IEEE-SA</a></li><li class="meta-nav-item stats-extLink"><a href="http://spectrum.ieee.org/" id="u-spectrum" class="exitspectrum">IEEE Spectrum</a></li><li class="meta-nav-item stats-extLink"><a href="http://www.ieee.org/sitemap.html" id="u-more" class="exitmoreieeesites">More Sites</a></li></ul></div><div class="meta-nav-user-links u-flex-display-flex text-sm-md-lh"><span class="hide-mobile"><a href="https://innovate.ieee.org/Xplore/Subscribebutton" target="_blank" class="stats-Xplnav_exit_subscribe subscribe-link">SUBSCRIBE</a></span><!----><ul class="u-flex-display-flex u-relative u-m-0 nav-right icons-panel"><div class="col-4 hide-desktop"></div><!----><span class="col-3 hide-desktop subscribe-link" style="font-size: .75rem;"><a href="https://innovate.ieee.org/Xplore/Subscribebutton" target="_blank" class="stats-Xplnav_exit_subscribe subscribe-link">SUBSCRIBE</a></span><!----><div class="cart-container"><li id="global-header-cart-count" class="meta-nav-item stats-mnEvLinks"><a title="View Cart" tabindex="0" class="cart stats-Unav_exit_Cart" style="white-space: nowrap;" href="https://www.ieee.org/cart/public/myCart/page.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore"><span id="cartCount">Cart&nbsp;</span></a><div id="mc_ieee-mini-cart-include_wrapper" class="content-r cart-summary product-cart" style="display: none;"><a onclick="closeCart(event);" tabindex="0" class="cart-close"></a><span id="mc_ieee-mini-cart-include" style="display: none;"></span><div id="mc_minicart-container" class="mc-minicart-content"></div></div></li><li class="meta-nav-item stats-mnEvLinks hide-desktop"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-desktop" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore.ieee.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore.ieee.org/document/9046805"><i class="fas fa-user-plus"></i></a></li><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container hide-desktop"><a title="Sign In" class="stats-Unav_P_SignIn hide-desktop"><i aria-hidden="true" class="fas fa-sign-in-alt"></i></a></li><!----><!----></div><!----><!----><div class="u-flex-display-flex text-sm-md-lh"><li class="meta-nav-item stats-mnEvLinks"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-mobile" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore.ieee.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore.ieee.org/document/9046805">Create Account</a></li><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container"><a href="javascript:void()" title="Sign In" class="stats-Unav_P_SignIn hide-mobile u-pr-05">Personal Sign In</a></li></div><!----><!----></ul></div></div></div></div></xpl-meta-nav><!----><xpl-global-notification _ngcontent-eae-c458="" _nghost-eae-c67=""><!----></xpl-global-notification><!----><!----><xpl-header _ngcontent-eae-c458="" _nghost-eae-c452=""><div _ngcontent-eae-c452="" class="main-header"><!----><xpl-navbar _ngcontent-eae-c452="" _nghost-eae-c462=""><div _ngcontent-eae-c462="" class="navbar-container not-homepage"><div _ngcontent-eae-c462="" class="top-navbar"><div _ngcontent-eae-c462="" class="hamburger-menu hide-desktop"><a _ngcontent-eae-c462=""><i _ngcontent-eae-c462="" aria-hidden="true" class="fa fa-bars"></i></a></div><div _ngcontent-eae-c462="" class="left-side-container"><div _ngcontent-eae-c462="" class="left-side-content"><div _ngcontent-eae-c462="" class="xplore-logo-wrapper"><xpl-xplore-logo _ngcontent-eae-c462="" _nghost-eae-c448=""><div _ngcontent-eae-c448="" class="xplore-logo-container"><a _ngcontent-eae-c448="" accesskey="1" title="Delivering full text access to the world&#39;s highest quality technical literature in engineering and technology" alt="IEEE Advancing Technology for Humanity" href="https://ieeexplore.ieee.org/Xplore/home.jsp"><img _ngcontent-eae-c448="" alt="IEEE Xplore logo - Link to home" class="xplore-logo" src="./全景摄像头_files/xplore_logo_white.png"></a></div></xpl-xplore-logo></div><div _ngcontent-eae-c462="" class="primary-menu hide-mobile text-base-md-lh"><ul _ngcontent-eae-c462=""><li _ngcontent-eae-c462=""><div _ngcontent-eae-c462=""><a _ngcontent-eae-c462="" tabindex="0" class="menu-link stats-browse-book"> Browse <i _ngcontent-eae-c462="" aria-hidden="true" class="fas fa-chevron-down"></i></a><!----></div></li><li _ngcontent-eae-c462=""><div _ngcontent-eae-c462=""><a _ngcontent-eae-c462="" tabindex="0" class="menu-link stats-my-settings"> My Settings <i _ngcontent-eae-c462="" aria-hidden="true" class="fas fa-chevron-down"></i></a><!----></div></li><li _ngcontent-eae-c462=""><div _ngcontent-eae-c462=""><a _ngcontent-eae-c462="" tabindex="0" class="menu-link stats-get-help"> Help <i _ngcontent-eae-c462="" aria-hidden="true" class="fas fa-chevron-down"></i></a><!----></div></li><!----></ul></div></div></div><div _ngcontent-eae-c462="" class="institution-container hide-mobile"><xpl-login-modal-trigger _ngcontent-eae-c462="" modalsource="header" _nghost-eae-c132=""><!----><a _ngcontent-eae-c462="" href="javascript:void()" class="inst-sign-in">Institutional Sign In</a></xpl-login-modal-trigger><!----><!----></div><div _ngcontent-eae-c462="" class="right-side-container"><div _ngcontent-eae-c462="" class="row"><!----><xpl-ieee-logo _ngcontent-eae-c462="" _nghost-eae-c447=""><div _ngcontent-eae-c447="" class="ieee-logo-container hide-mobile"><img _ngcontent-eae-c447="" alt="IEEE logo - Link to IEEE main site homepage" class="ieee-logo" src="./全景摄像头_files/ieee_logo_white.png"><!----><!----></div></xpl-ieee-logo></div></div></div><div _ngcontent-eae-c462="" class="bottom-navbar hide-desktop"><xpl-login-modal-trigger _ngcontent-eae-c462="" modalsource="header" _nghost-eae-c132=""><!----><a _ngcontent-eae-c462="" tabindex="0" class="inst-sign-in">Institutional Sign In</a></xpl-login-modal-trigger><!----><!----></div></div></xpl-navbar><div _ngcontent-eae-c452="" role="img"><!----><div _ngcontent-eae-c452="" class="search-bar-container fill-background not-homepage"><xpl-search-bar-migr _ngcontent-eae-c452="" _nghost-eae-c465=""><div _ngcontent-eae-c465="" class="search-bar"><form _ngcontent-eae-c465="" novalidate="" class="search-bar-wrapper ng-untouched ng-pristine ng-valid"><div _ngcontent-eae-c465="" class="drop-down"><label _ngcontent-eae-c465=""><select _ngcontent-eae-c465="" aria-label="content type dropdown"><option _ngcontent-eae-c465="">All</option><option _ngcontent-eae-c465="">Books</option><option _ngcontent-eae-c465="">Conferences</option><option _ngcontent-eae-c465="">Courses</option><option _ngcontent-eae-c465="">Journals &amp; Magazines</option><option _ngcontent-eae-c465="">Standards</option><option _ngcontent-eae-c465="">Authors</option><option _ngcontent-eae-c465="">Citations</option><!----></select></label></div><div _ngcontent-eae-c465="" class="search-field all"><div _ngcontent-eae-c465="" class="search-field-icon-container"><div _ngcontent-eae-c465="" class="global-search-bar"><xpl-typeahead-migr _ngcontent-eae-c465="" placeholder="" name="search-term" ulclass="search-within-results ui-autocomplete ui-front ui-menu ui-widget ui-widget-content ui-corner-all" minchars="3" _nghost-eae-c53=""><div _ngcontent-eae-c53="" class="Typeahead text-sm-md-lh"><input _ngcontent-eae-c53="" type="text" autocomplete="off" aria-label="Enter search text" class="Typeahead-input ng-untouched ng-pristine ng-valid" placeholder=""><!----></div></xpl-typeahead-migr></div><!----><!----><div _ngcontent-eae-c465="" class="search-icon"><button _ngcontent-eae-c465="" type="submit" aria-label="Search" class="fa fa-search"></button></div><!----></div><!----></div></form><!----><div _ngcontent-eae-c465="" class="below-search-bar"><!----><div _ngcontent-eae-c465="" class="advanced-search-wrapper text-sm-md-lh"><div _ngcontent-eae-c465="" class="advanced-search-div"><a _ngcontent-eae-c465="" href="https://ieeexplore.ieee.org/search/advanced" target="_self"><span _ngcontent-eae-c465="">ADVANCED SEARCH </span><i _ngcontent-eae-c465="" aria-hidden="true" class="fas fa-caret-right adv-search-arrow"></i></a></div><!----><!----></div></div></div></xpl-search-bar-migr></div></div><!----></div></xpl-header><!----><!----><!----><div _ngcontent-eae-c458="" class="global-ng-wrapper"><router-outlet _ngcontent-eae-c458=""></router-outlet><xpl-document-details _nghost-eae-c189=""><div _ngcontent-eae-c189="" class="row document ng-document stats-document"><div _ngcontent-eae-c189="" class="document-main global-content-width-w-rr"><section _ngcontent-eae-c189="" class="document-main-leaderboard-ad col-12"><xpl-leaderboard-ad _ngcontent-eae-c189="" class="hide-desktop" _nghost-eae-c121=""><div _ngcontent-eae-c121="" class="Ads-leaderboard ad-panel"><div _ngcontent-eae-c121="" class="row u-flex-wrap-nowrap"><div _ngcontent-eae-c121="" class="ads-close-container"><i _ngcontent-eae-c121="" aria-hidden="true" class="ads-close-button"></i></div><!----></div><div _ngcontent-eae-c121="" class="ad-leaderboard-ad-container"><!----><div _ngcontent-eae-c121="" xplgoogleadmigr="" class="Ads-leaderBoardTablet"><div id="div-gpt-ad-1606861783216-0" style="width:576px;
			height:71px; display:block; margin: 0 auto; padding-bottom: 0.5em;"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861783216-0"); });</script></div><!----><div _ngcontent-eae-c121="" xplgoogleadmigr="" class="Ads-leaderBoardMobile"><div id="div-gpt-ad-1606861783316-0" style="width:320px;
			height:50px; display:block; margin: 0 auto; padding-bottom: 0.5em;"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861783316-0"); });</script></div><!----></div></div><!----></xpl-leaderboard-ad><!----></section><!----><section _ngcontent-eae-c189="" class="document-main-header row"><div _ngcontent-eae-c189="" class="col-12"><xpl-document-header _ngcontent-eae-c189="" _nghost-eae-c141=""><section _ngcontent-eae-c141="" class="document-header row"><div _ngcontent-eae-c141="" class="document-header-breadcrumbs-container col-12"><div _ngcontent-eae-c141="" class="breadcrumbs col text-sm-md-lh"><span _ngcontent-eae-c141=""><a _ngcontent-eae-c141="" href="https://ieeexplore.ieee.org/browse/periodicals/title/">Journals &amp; Magazines</a><!----><!----><span _ngcontent-eae-c141="" class="breadcrumbs-separator"> &gt;</span></span><span _ngcontent-eae-c141=""><a _ngcontent-eae-c141="" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><!----><!----><span _ngcontent-eae-c141="" class="breadcrumbs-separator"> &gt;</span></span><span _ngcontent-eae-c141=""><a _ngcontent-eae-c141="" href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8948470">Volume: 8</a><!----><!----><span _ngcontent-eae-c141="" class="breadcrumbs-separator"></span></span><!----><xpl-help-link _ngcontent-eae-c141="" id="help" tooltiptype="breadcrumb" _nghost-eae-c58=""><a _ngcontent-eae-c58="" target="_blank" tooltipclass="helplink-tooltip" triggers="hover" class="icon-size-md u-flex-display-inline" href="https://ieeexplore.ieee.org/Xplorehelp/ieee-xplore-training/working-with-documents#interactive-html"><i _ngcontent-eae-c58="" class="fa fa-question-circle help-link breadcrumb-help-link-icon"></i></a><!----><!----></xpl-help-link></div></div><!----><div _ngcontent-eae-c141="" class="document-header-inner-container row"><!----><div _ngcontent-eae-c141="" class="col-12"><div _ngcontent-eae-c141="" class="row stats-document-header"><div _ngcontent-eae-c141="" class="row document-title-fix"><!----><div _ngcontent-eae-c141="" class="document-header-title-container col"><div _ngcontent-eae-c141="" class="left-container"><h1 _ngcontent-eae-c141="" class="document-title text-2xl-md-lh"><span _ngcontent-eae-c141="">A Survey of Autonomous Driving: <i>Common Practices and Emerging Technologies</i></span><!----></h1><div _ngcontent-eae-c141="" class="u-mb-1 u-mt-05 btn-container"><div _ngcontent-eae-c141="" class="publisher-title-tooltip"><xpl-publisher _ngcontent-eae-c141="" tooltipplacement="right" _nghost-eae-c104=""><span _ngcontent-eae-c104="" class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-eae-c104="" xplhighlight=""><span _ngcontent-eae-c104=""><span _ngcontent-eae-c104="" class="title">Publisher: </span><!----><span _ngcontent-eae-c104="">IEEE</span></span></span><!----><!----></span><!----></xpl-publisher></div><!----><div _ngcontent-eae-c141="" class="cite-this-related-btn-wrapper"><xpl-cite-this-modal _ngcontent-eae-c141="" _nghost-eae-c122=""><!----><div _ngcontent-eae-c141=""><button _ngcontent-eae-c141="" placement="bottom" class="layout-btn-white cite-this-btn">Cite This</button></div></xpl-cite-this-modal><!----></div><div _ngcontent-eae-c141="" class="black-tooltip tool-tip-pdf-button"><div _ngcontent-eae-c141="" placement="bottom" class="pdf-btn-container hide-mobile"><xpl-view-pdf _ngcontent-eae-c141="" placement="document-page-desktop" _nghost-eae-c110=""><div _ngcontent-eae-c110=""><!----><!----><div _ngcontent-eae-c110=""><a _ngcontent-eae-c110="" class="pdf-btn-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9046805"><i _ngcontent-eae-c110="" class="icon-size-md icon red-pdf fas fa-file-pdf"></i><span _ngcontent-eae-c110="">PDF</span></a><!----><!----><!----></div><!----></div><!----><!----></xpl-view-pdf><!----><xpl-login-modal-trigger _ngcontent-eae-c141="" _nghost-eae-c132=""><!----><!----></xpl-login-modal-trigger><!----></div><!----></div></div><!----><!----><!----><!----><!----></div><div _ngcontent-eae-c141="" class="right-container"><!----></div></div></div><!----><!----><div _ngcontent-eae-c141="" class="document-main-subheader"><div _ngcontent-eae-c141="" class="document-main-author-banner"><div _ngcontent-eae-c141="" class="document-authors-banner stats-document-authors-banner"><div _ngcontent-eae-c141="" class="row authors-banner-row u-flex-align-items-center u-flex-wrap-nowrap"><xpl-author-banner _ngcontent-eae-c141="" class="authors-banner-row-middle" _nghost-eae-c134=""><div _ngcontent-eae-c134="" class="document-authors-banner stats-document-authors-banner"><div _ngcontent-eae-c134="" class="row authors-banner-row u-flex-wrap-nowrap"><div _ngcontent-eae-c134="" class="authors-banner-row-middle"><div _ngcontent-eae-c134="" class="authors-container stats-document-authors-banner-authorsContainer"><div _ngcontent-eae-c134="" class="authors-info-container overflow-ellipsis text-base-md-lh authors-minimized" id="indexTerms-container-1643261710470-0"><!----><span _ngcontent-eae-c134="" class="authors-info"><span _ngcontent-eae-c134="" class="blue-tooltip"><a _ngcontent-eae-c134="" placement="bottom" triggers="hover" href="https://ieeexplore.ieee.org/author/37085640372"><span _ngcontent-eae-c134="">Ekim Yurtsever</span><!----></a><!----><!----><!----><!----></span><!----><!----><span _ngcontent-eae-c134="" class="u-px-02"><a _ngcontent-eae-c134="" target="_blank" href="https://orcid.org/0000-0002-3103-6052"><i _ngcontent-eae-c134="" class="icon icon-orcid"></i></a></span><!----><span _ngcontent-eae-c134="">; </span><!----></span><span _ngcontent-eae-c134="" class="authors-info"><span _ngcontent-eae-c134="" class="blue-tooltip"><a _ngcontent-eae-c134="" placement="bottom" triggers="hover" href="https://ieeexplore.ieee.org/author/37087105608"><span _ngcontent-eae-c134="">Jacob Lambert</span><!----></a><!----><!----><!----><!----></span><!----><!----><!----><span _ngcontent-eae-c134="">; </span><!----></span><span _ngcontent-eae-c134="" class="authors-info"><span _ngcontent-eae-c134="" class="blue-tooltip"><a _ngcontent-eae-c134="" placement="bottom" triggers="hover" href="https://ieeexplore.ieee.org/author/37604736800"><span _ngcontent-eae-c134="">Alexander Carballo</span><!----></a><!----><!----><!----><!----></span><!----><!----><!----><span _ngcontent-eae-c134="">; </span><!----></span><span _ngcontent-eae-c134="" class="authors-info"><span _ngcontent-eae-c134="" class="blue-tooltip"><a _ngcontent-eae-c134="" placement="bottom" triggers="hover" href="https://ieeexplore.ieee.org/author/37269965700"><span _ngcontent-eae-c134="">Kazuya Takeda</span><!----></a><!----><!----><!----><!----></span><!----><!----><!----><span _ngcontent-eae-c134=""></span><!----></span><!----></div></div></div><!----></div></div><!----></xpl-author-banner><div _ngcontent-eae-c141="" class="u-flex-display-flex u-flex-align-items-center nowrap text-base-md-lh"><div _ngcontent-eae-c141="" class="authors-view-all-link-container hide-mobile"><a _ngcontent-eae-c141="" href="javascript:void()" class="text-base-md-lh">All Authors</a></div><div _ngcontent-eae-c141="" class="authors-mobile-view-all-container blue-tooltip hide-desktop"><a _ngcontent-eae-c141="" placement="bottom-right" triggers="click:click" class="authors-viewall-link"><i _ngcontent-eae-c141="" class="authors-viewall-icon"></i></a><!----></div><!----></div></div><!----><!----></div><!----></div><div _ngcontent-eae-c141="" class="document-header-metrics-banner row ccby-document"><div _ngcontent-eae-c141="" class="document-banner col stats-document-banner"><xpl-login-modal-trigger _ngcontent-eae-c141="" _nghost-eae-c132=""><!----><!----></xpl-login-modal-trigger><!----><!----><button _ngcontent-eae-c141="" class="sip-modal-button stats-document-banner-viewDocument"><div _ngcontent-eae-c141="" class="main-txt"> View Document </div></button><!----><div _ngcontent-eae-c141="" class="document-banner-metric-container row"><button _ngcontent-eae-c141="" class="document-banner-metric text-base-md-lh col"><div _ngcontent-eae-c141="" class="document-banner-metric-count">196</div><div _ngcontent-eae-c141="">Paper</div><div _ngcontent-eae-c141="">Citations</div></button><!----><!----><button _ngcontent-eae-c141="" class="document-banner-metric text-base-md-lh col"><div _ngcontent-eae-c141="" class="document-banner-metric-count">16402</div><!----><div _ngcontent-eae-c141=""><div _ngcontent-eae-c141="">Full</div><div _ngcontent-eae-c141="">Text Views</div></div><!----></button><!----></div><!----><div _ngcontent-eae-c141="" class="document-banner-access"><div _ngcontent-eae-c141="" class="document-access-container hide-mobile"><div _ngcontent-eae-c141="" class="document-access-icon"><i _ngcontent-eae-c141="" class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i><span _ngcontent-eae-c141="">Open Access</span></div><!----><!----></div><!----><div _ngcontent-eae-c141="" class="document-disqus-anchor-container hide-mobile-imp"><a _ngcontent-eae-c141="" tabindex="0"><i _ngcontent-eae-c141="" class="icon-size-md color-xplore-blue fas fa-comment u-mr-05"></i><span _ngcontent-eae-c141="">Comment(s)</span></a></div><!----></div></div><div _ngcontent-eae-c141="" class="document-mobile-access-container"><div _ngcontent-eae-c141="" class="document-access-icon"><i _ngcontent-eae-c141="" class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i></div><!----><!----></div><!----><div _ngcontent-eae-c141="" class="document-disqus-anchor-container hide-desktop"><a _ngcontent-eae-c141="" tabindex="0"><i _ngcontent-eae-c141="" class="icon-size-md color-xplore-blue fas fa-comment"></i></a></div><!----><div _ngcontent-eae-c141="" class="col-7-24 black-tooltip hide-mobile"><!----><!----><xpl-document-toolbar _ngcontent-eae-c141="" _nghost-eae-c140=""><div _ngcontent-eae-c140="" class="col-actions stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff"><div _ngcontent-eae-c140="" class="action-item-container"><ul _ngcontent-eae-c140="" class="icon-size-md doc-actions doc-toolbar stats-document-lh-actions black-tooltip"><li _ngcontent-eae-c140="" placement="bottom" class="doc-actions-item"><a _ngcontent-eae-c140="" target="blank" class="doc-actions-link stats_ReferencesView_Doc_Details_9046805" href="https://ieeexplore.ieee.org/xpl/dwnldReferences?arnumber=9046805"><i _ngcontent-eae-c140="" class="icon-size-md color-xplore-blue fas fa-registered"></i></a></li><!----><li _ngcontent-eae-c140="" placement="bottom" class="doc-actions-item white-blue-border-tooltip"><xpl-document-social-media _ngcontent-eae-c140="" _nghost-eae-c137=""><button _ngcontent-eae-c137="" triggers="click" class="doc-share-tool"><i _ngcontent-eae-c137="" aria-hidden="true" class="fa fa-share-alt"></i><!----></button><!----><!----></xpl-document-social-media></li><!----><li _ngcontent-eae-c140="" placement="bottom" class="stats-permission doc-actions-item disabled-look enable-hover"><!----><a _ngcontent-eae-c140="" href="javascript:void()" class="doc-actions-link stats_Doc_Details_Copyright_9046805"><i _ngcontent-eae-c140="" class="color-xplore-blue copyright-icon far fa-copyright"></i></a><!----></li><!----><li _ngcontent-eae-c140="" placement="bottom" class="doc-actions-item white-blue-border-tooltip save-to disabled-look enable-hover"><a _ngcontent-eae-c140="" placement="bottom-right" triggers="click" href="javascript:void()" class="doc-save-tool"><i _ngcontent-eae-c140="" class="icon-size-md color-xplore-blue fas fa-folder-open"></i></a><!----><!----></li><!----><li _ngcontent-eae-c140="" placement="bottom" class="doc-actions-item"><xpl-manage-alerts _ngcontent-eae-c140="" class="white-blue-border-tooltip alerts-popover" _nghost-eae-c138=""><!----><a _ngcontent-eae-c138="" href="javascript:void()" triggers="click:click" class="doc-actions-link stats-document-lh-action-alerts hide-mobile"><i _ngcontent-eae-c138="" class="icon-size-md color-xplore-blue fas fa-bell"></i><span _ngcontent-eae-c138="" class="doc-actions-text">Alerts</span></a><!----><div _ngcontent-eae-c138="" class="manage-alerts-popover-content hide-desktop"><h1 _ngcontent-eae-c138="" class="header">Alerts</h1><div _ngcontent-eae-c138="" class="manage-alerts-link"><a _ngcontent-eae-c138="" href="https://ieeexplore.ieee.org/alerts/citation"> Manage Content Alerts <i _ngcontent-eae-c138="" class="icon icon-courses-chevron-blue"></i></a></div><div _ngcontent-eae-c138="" class="manage-alerts-link"><a _ngcontent-eae-c138="" href="javascript:void()"> Add to Citation Alerts <i _ngcontent-eae-c138="" class="icon icon-courses-chevron-blue"></i></a></div></div></xpl-manage-alerts></li><!----></ul></div></div></xpl-document-toolbar><!----></div><!----></div><div _ngcontent-eae-c141="" class="ccby-indicator u-pl-2"> Under a <a _ngcontent-eae-c141="" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons License</a></div><!----></div></div><!----></div></div><hr _ngcontent-eae-c141=""></section><!----><!----></xpl-document-header></div><!----></section><div _ngcontent-eae-c189="" class="row document-main-body"><div _ngcontent-eae-c189="" class="document-main-left-trail col-5-24"><div _ngcontent-eae-c189="" class="col-24-24"><div _ngcontent-eae-c189="" class="row"><nav _ngcontent-eae-c189="" class="col-24-24 bg-ltgry tab-nav text-base-md-lh"><div _ngcontent-eae-c189="" id="document-tabs" class="doc-tabs-list stats-document-tabs"><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab active"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805">Abstract</a></div><!----><xpl-full-text-toc _ngcontent-eae-c189="" class="hide-mobile" _nghost-eae-c142=""><div _ngcontent-eae-c142="" class="toc-container"><div _ngcontent-eae-c142="" class="toc-heading">Document Sections</div><ul _ngcontent-eae-c142="" class="toc-list"><li _ngcontent-eae-c142="" class="toc-list-item"><a _ngcontent-eae-c142="" href="javascript:void()" class="toc-list-link" tabindex="0"><div _ngcontent-eae-c142="" class="toc-list-icon">I.</div><!----><div _ngcontent-eae-c142="" class="toc-list-icon"><!----></div><div _ngcontent-eae-c142="">Introduction</div></a><!----><!----></li><li _ngcontent-eae-c142="" class="toc-list-item"><a _ngcontent-eae-c142="" href="javascript:void()" class="toc-list-link" tabindex="0"><div _ngcontent-eae-c142="" class="toc-list-icon">II.</div><!----><div _ngcontent-eae-c142="" class="toc-list-icon"><!----></div><div _ngcontent-eae-c142="">Prospects and Challenges</div></a><!----><!----></li><li _ngcontent-eae-c142="" class="toc-list-item"><a _ngcontent-eae-c142="" href="javascript:void()" class="toc-list-link" tabindex="0"><div _ngcontent-eae-c142="" class="toc-list-icon">III.</div><!----><div _ngcontent-eae-c142="" class="toc-list-icon"><!----></div><div _ngcontent-eae-c142="">System Components and Architecture</div></a><!----><!----></li><li _ngcontent-eae-c142="" class="toc-list-item"><a _ngcontent-eae-c142="" href="javascript:void()" class="toc-list-link" tabindex="0"><div _ngcontent-eae-c142="" class="toc-list-icon">IV.</div><!----><div _ngcontent-eae-c142="" class="toc-list-icon"><!----></div><div _ngcontent-eae-c142="">Localization and Mapping</div></a><!----><!----></li><li _ngcontent-eae-c142="" class="toc-list-item"><a _ngcontent-eae-c142="" href="javascript:void()" class="toc-list-link" tabindex="0"><div _ngcontent-eae-c142="" class="toc-list-icon">V.</div><!----><div _ngcontent-eae-c142="" class="toc-list-icon"><!----></div><div _ngcontent-eae-c142="">Perception</div></a><!----><!----></li><!----></ul><button _ngcontent-eae-c142="" class="toc-show-more-btn"><span _ngcontent-eae-c142="">Show Full Outline</span><i _ngcontent-eae-c142="" class="fa fa-caret-down"></i></button><!----></div><!----></xpl-full-text-toc><!----><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/authors">Authors</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/figures">Figures</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/references">References</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/citations">Citations</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/keywords">Keywords</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/metrics">Metrics</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab similar"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/similar">More Like This</a></div><div _ngcontent-eae-c189="" routerlinkactive="active" class="browse-pub-tab"><a _ngcontent-eae-c189="" class="document-tab-link" href="https://ieeexplore.ieee.org/document/9046805/footnotes">Footnotes</a></div><!----></div></nav></div></div><!----></div><div _ngcontent-eae-c189="" class="document-main-content-container"><xpl-left-side-bar _ngcontent-eae-c189="" _nghost-eae-c143=""><div _ngcontent-eae-c143="" xplscrollsnapmigr="" scrollreset="true" offsetfrom="100" fromelementid="mobile-tab-pane" tillelementid="full-text-footer" offsetto="-800" cssclasstostick="document-mobile-leftrail-stick" class="col-2 col-actions ng-col-actions hide-desktop stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff col-actions-mobile-closed ng-col-actions-mobile-closed document-mobile-leftrail-stick"><div _ngcontent-eae-c143="" id="left-rail-container"><div _ngcontent-eae-c143="" class="doc-actions-mobile-expand-button"></div><!----><ul _ngcontent-eae-c143="" class="doc-actions stats-document-lh-actions"><li _ngcontent-eae-c143="" class="doc-actions-item"><xpl-view-pdf _ngcontent-eae-c143="" placement="document-page-mobile" _nghost-eae-c110=""><div _ngcontent-eae-c110=""><!----><!----><div _ngcontent-eae-c110=""><!----><a _ngcontent-eae-c110="" target="_blank" class="doc-actions-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9046805"><i _ngcontent-eae-c110="" class="icon-size-md icon red-pdf fas fa-file-pdf"></i> Download PDF </a><!----><!----></div><!----></div><!----><!----></xpl-view-pdf><!----><xpl-login-modal-trigger _ngcontent-eae-c143="" _nghost-eae-c132=""><!----><!----></xpl-login-modal-trigger></li><li _ngcontent-eae-c143="" class="doc-actions-item"><a _ngcontent-eae-c143="" target="blank" class="doc-actions-link stats_ReferencesView_Doc_Details_9046805" href="https://ieeexplore.ieee.org/xpl/dwnldReferences?arnumber=9046805"><i _ngcontent-eae-c143="" class="icon-size-md color-xplore-blue fas fa-registered"></i> View References </a><!----><!----></li><li _ngcontent-eae-c143="" class="doc-actions-item white-blue-border-tooltip"><a _ngcontent-eae-c143="" class="doc-actions-link"><xpl-document-social-media _ngcontent-eae-c143="" tooltipplacement="right" placement="document-page-mobile" _nghost-eae-c137=""><button _ngcontent-eae-c137="" triggers="click" class="doc-share-tool"><i _ngcontent-eae-c137="" aria-hidden="true" class="fa fa-share-alt"></i><!----></button><!----><!----></xpl-document-social-media></a><!----></li><li _ngcontent-eae-c143="" class="stats-permission doc-actions-item disabled-look black-tooltip"><!----><a _ngcontent-eae-c143="" placement="right" triggers="click" class="doc-actions-link stats_Doc_Details_Copyright_9046805"><i _ngcontent-eae-c143="" class="copyright-icon far fa-copyright"></i> Request Permissions </a><!----><!----></li><li _ngcontent-eae-c143="" class="doc-actions-item disabled-look black-tooltip"><!----><a _ngcontent-eae-c143="" placement="right" triggers="click" autoclose="outside" class="doc-actions-link stats-document-lh-action-downloadPdf_3"><i _ngcontent-eae-c143="" class="icon-size-md color-xplore-blue fas fa-folder-open"></i> Save to </a><!----><!----><!----></li><li _ngcontent-eae-c143="" class="doc-actions-item"><a _ngcontent-eae-c143="" class="doc-actions-link stats-document-lh-action-alerts"><i _ngcontent-eae-c143="" class="icon-size-md color-xplore-blue fas fa-bell"></i> Alerts </a><!----><!----><!----></li><!----><!----></ul></div></div><!----></xpl-left-side-bar><!----><section _ngcontent-eae-c189="" class="tab-pane col-24-24 u-printing-display-inline-ie u-printing-display-inline-ff"><div _ngcontent-eae-c189="" id="mobile-tab-pane"></div><!----><div _ngcontent-eae-c189="" class="document-main-left-trail-content"><!----><div _ngcontent-eae-c189="" id=""><router-outlet _ngcontent-eae-c189=""></router-outlet><xpl-document-abstract _nghost-eae-c184=""><section _ngcontent-eae-c184="" class="document-abstract document-tab"><div _ngcontent-eae-c184="" class="col-12 hide-desktop mobile-graphical-abstract"><div _ngcontent-eae-c184="" class="abstract-graphic"><!----><!----><div _ngcontent-eae-c184="" class="mobile-graphic"><img _ngcontent-eae-c184="" class="abstract-graphic-img" src="./全景摄像头_files/access-gagraphic-2983149.jpg" alt="This figure illustrates outputs of state-of-the-art perception algorithms in an urban scene near Nagoya University, with camera and lidar data collected by our experimental vehicle. (a) A front facing camera&#39;s view, with overlaid bounding box results from YOLOv3. (b) Instance segmentation results from MaskRCNN. (c) Semantic segmentation masks produced by DeepLabv3. (d) The 3D Lidar data with object detection results from SECOND. Amongst the four, only the 3D perception algorithm outputs range to detected objects."></div><!----><div _ngcontent-eae-c184="" class="abstract-graphic-caption"><span _ngcontent-eae-c184="" xplmathjax="">This figure illustrates outputs of state-of-the-art perception algorithms in an urban scene near Nagoya University, with camera and lidar data collected by our experiment...</span><span _ngcontent-eae-c184=""><a _ngcontent-eae-c184="">View more</a></span><!----></div></div><!----><!----></div><!----><div _ngcontent-eae-c184="" class="abstract-mobile-div hide-desktop"><div _ngcontent-eae-c184="" class="row"><div _ngcontent-eae-c184="" class="mobile-col-12"><!----><!----><div _ngcontent-eae-c184="" class="u-pb-1"><strong _ngcontent-eae-c184=""> Abstract:</strong><span _ngcontent-eae-c184="" xplmathjax="">Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. ...</span><span _ngcontent-eae-c184=""><a _ngcontent-eae-c184="" class="mobile-toggle-btn">View more</a></span><!----></div><!----><!----><!----></div><!----></div><!----><!----><!----><div _ngcontent-eae-c184="" class="metadata-toggle-btn mobile-content"><strong _ngcontent-eae-c184=""><i _ngcontent-eae-c184="" class="icon-caret-abstract"></i><span _ngcontent-eae-c184="">Metadata</span></strong></div><!----><!----></div><div _ngcontent-eae-c184="" class="abstract-desktop-div hide-mobile text-base-md-lh"><div _ngcontent-eae-c184="" class="abstract-text row"><!----><div _ngcontent-eae-c184="" class="col-12"><!----><!----><div _ngcontent-eae-c184="" class="u-mb-1"><strong _ngcontent-eae-c184=""> Abstract:</strong><div _ngcontent-eae-c184="" xplmathjax="">Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.</div></div><!----><!----><!----></div><!----></div><!----><!----><!----><div _ngcontent-eae-c184="" data-tealium_data="{&quot;docType&quot;: &quot;Journal&quot;}" class="u-pb-1 stats-document-abstract-publishedIn"><strong _ngcontent-eae-c184="">Published in: </strong><a _ngcontent-eae-c184="" class="stats-document-abstract-publishedIn" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><!----><!----><span _ngcontent-eae-c184=""> ( <span _ngcontent-eae-c184="">Volume: 8</span><!----><!---->) </span><!----><!----></div><!----><!----><!----><div _ngcontent-eae-c184="" class="row u-pt-1"><div _ngcontent-eae-c184="" class="col-6"><!----><div _ngcontent-eae-c184="" class="u-pb-1"><strong _ngcontent-eae-c184="">Page(s): </strong> 58443 <span _ngcontent-eae-c184="">- 58469</span><!----></div><!----><!----><div _ngcontent-eae-c184="" class="u-pb-1 doc-abstract-pubdate"><strong _ngcontent-eae-c184="">Date of Publication:</strong> 25 March 2020 <xpl-help-link _ngcontent-eae-c184="" arialabel="Get help with using Publication Dates" helplinktext="Help with using Publication Dates" helplink="http://ieeexplore.ieee.org/Xplorehelp/Help_Pubdates.html" _nghost-eae-c58=""><a _ngcontent-eae-c58="" target="_blank" tooltipclass="helplink-tooltip" triggers="hover" class="icon-size-md u-flex-display-inline" href="http://ieeexplore.ieee.org/Xplorehelp/Help_Pubdates.html" aria-label="Get help with using Publication Dates"><i _ngcontent-eae-c58="" class="fa fa-question-circle help-link help-link-icon"></i></a><!----><!----></xpl-help-link></div><!----><!----><!----><!----><!----><!----><!----><div _ngcontent-eae-c184="" class="u-pb-1"><!----><div _ngcontent-eae-c184=""><div _ngcontent-eae-c184=""><strong _ngcontent-eae-c184="">Electronic ISSN:</strong> 2169-3536 </div><!----></div><!----></div><!----><!----></div><div _ngcontent-eae-c184="" class="col-6"><div _ngcontent-eae-c184="" class="u-pb-1"><strong _ngcontent-eae-c184="">INSPEC Accession Number: </strong> 19499294 </div><!----><div _ngcontent-eae-c184="" class="u-pb-1 stats-document-abstract-doi"><strong _ngcontent-eae-c184="">DOI: </strong><a _ngcontent-eae-c184="" append-to-href="?src=document" target="_blank" href="https://doi.org/10.1109/ACCESS.2020.2983149">10.1109/ACCESS.2020.2983149</a><!----><!----></div><!----><!----><!----><!----><div _ngcontent-eae-c184="" class="u-pb-1 doc-abstract-publisher"><xpl-publisher _ngcontent-eae-c184="" _nghost-eae-c104=""><span _ngcontent-eae-c104="" class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-eae-c104="" xplhighlight=""><span _ngcontent-eae-c104=""><span _ngcontent-eae-c104="" class="title">Publisher: </span><!----><span _ngcontent-eae-c104="">IEEE</span></span></span><!----><!----></span><!----></xpl-publisher></div><!----><!----><!----><!----></div><!----></div><div _ngcontent-eae-c184="" class="row"><div _ngcontent-eae-c184="" class="col-12"><div _ngcontent-eae-c184="" class="abstract-graphic"><!----><!----><div _ngcontent-eae-c184="" class="abstract-graphic-asset"><img _ngcontent-eae-c184="" class="abstract-graphic-img" src="./全景摄像头_files/access-gagraphic-2983149.jpg" alt="This figure illustrates outputs of state-of-the-art perception algorithms in an urban scene near Nagoya University, with camera and lidar data collected by our experimental vehicle. (a) A front facing camera&#39;s view, with overlaid bounding box results from YOLOv3. (b) Instance segmentation results from MaskRCNN. (c) Semantic segmentation masks produced by DeepLabv3. (d) The 3D Lidar data with object detection results from SECOND. Amongst the four, only the 3D perception algorithm outputs range to detected objects."></div><!----><div _ngcontent-eae-c184="" class="abstract-graphic-caption"><span _ngcontent-eae-c184="" xplmathjax="">This figure illustrates outputs of state-of-the-art perception algorithms in an urban scene near Nagoya University, with camera and lidar data collected by our experiment...</span><span _ngcontent-eae-c184=""><a _ngcontent-eae-c184="">View more</a></span><!----></div></div></div><!----><div _ngcontent-eae-c184="" class="show-full-abstract col-12"><a _ngcontent-eae-c184="" class="document-abstract-toggle-btn"> Hide Full Abstract <i _ngcontent-eae-c184="" class="fa fa-angle-up"></i></a></div></div><!----><!----><!----></div></section></xpl-document-abstract><!----></div><xpl-leaderboard-middle-ad _ngcontent-eae-c189="" class="hide-desktop" _nghost-eae-c185=""><div _ngcontent-eae-c185="" class="Ads-leaderboard ad-panel"><div _ngcontent-eae-c185="" class="row u-flex-wrap-nowrap"><div _ngcontent-eae-c185="" class="ads-close-container"><i _ngcontent-eae-c185="" aria-hidden="true" class="ads-close-button"></i></div><!----></div><div _ngcontent-eae-c185="" class="ad-leaderboard-ad-container"><!----><div _ngcontent-eae-c185="" xplgoogleadmigr="" class="Ads-leaderBoardMiddleTablet"><div id="div-gpt-ad-1606861708257-0" style="width:576px;
			height:71px; display:block; margin: 0 auto; padding-bottom: 0.5em;"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861708257-0"); });</script></div><!----><div _ngcontent-eae-c185="" xplgoogleadmigr="" class="Ads-leaderBoardMiddleMobile"><div id="div-gpt-ad-1606861708357-0" style="width:320px;
			height:50px; display:block; margin: 0 auto; padding-bottom: 0.5em;"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861708357-0"); });</script></div><!----></div></div><!----></xpl-leaderboard-middle-ad><!----><xpl-document-full-text _ngcontent-eae-c189="" _nghost-eae-c156=""><section _ngcontent-eae-c156=""><!----><div _ngcontent-eae-c156="" id="toc-wrapper" class="row full-text-toc-wrapper"><div _ngcontent-eae-c156="" xplscrollsnapmigr="" cssclasstostick="document-toc-stick" fromelementid="toc-wrapper" tillelementid="full-text-footer" offsetfrom="150" offsetto="-800" scrollreset="true" class="col-12 u-align-center ft-toc previous-next-nav-ctrl hide-desktop document-toc-stick"><div _ngcontent-eae-c156="" class="toc-container hide-desktop"><!----><a _ngcontent-eae-c156="" ngclass="{&#39;disabled&#39;: !toc}" class="toc-link {&#39;disabled&#39;: !toc}"><img _ngcontent-eae-c156="" src="./全景摄像头_files/toc-icon.png"> Contents </a></div><!----></div></div><!----><hr _ngcontent-eae-c156=""><div _ngcontent-eae-c156="" class="row document-full-text-content"><div _ngcontent-eae-c156="" id="full-text-section" class="col col-text stats-document-container-fullTextSection u-printing-display-inline-ie u-printing-display-inline-ff" style="font-size: 15px;"><span _ngcontent-eae-c156="" id="full-text-header"></span><!----><div _ngcontent-eae-c156=""><!----><!----><!----><!----><div _ngcontent-eae-c156="" xplmathjax="" xplfulltextdomhandler="" xpllazyloadfigures="" class="document-text hide-full-text ng-non-bindable stats-document-dynamicFullTextOrSnippet-container show-full-text"><!--?xml version="1.0" encoding="UTF-8"?-->
<response><accesstype>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via <a class="vglnk" href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow"><span>https</span><span>://</span><span>creativecommons</span><span>.</span><span>org</span><span>/</span><span>licenses</span><span>/</span><span>by</span><span>/</span><span>4</span><span>.</span><span>0</span><span>/</span></a> to obtain full-text articles and stipulations in the API documentation.</accesstype><div id="BodyWrapper" class="ArticlePage" xmlns:ieee="http://www.ieeexplore.ieee.org"><div id="article">
<div class="section" id="sec1"><div class="header article-hdr"><div class="kicker">
		                        SECTION I.</div><h2>Introduction</h2></div><p>According to a recent technical report by the National Highway Traffic Safety Administration (NHTSA), 94% of road accidents are caused by human errors <a ref-type="bibr" anchor="ref1" id="context_ref_1_1">[1]</a>. Against this backdrop, Automated Driving Systems (ADSs) are being developed with the promise of preventing accidents, reducing emissions, transporting the mobility-impaired and reducing driving related stress <a ref-type="bibr" anchor="ref2" id="context_ref_2_1">[2]</a>. If widespread deployment can be realized, annual social benefits of ADSs are projected to reach nearly $800 billion by 2050 through congestion mitigation, road casualty reduction, decreased energy consumption and increased productivity caused by the reallocation of driving time <a ref-type="bibr" anchor="ref3" id="context_ref_3_1">[3]</a>.</p><p>The accumulated knowledge in vehicle dynamics, breakthroughs in computer vision caused by the advent of deep learning <a ref-type="bibr" anchor="ref4" id="context_ref_4_1">[4]</a> and availability of new sensor modalities, such as lidar <a ref-type="bibr" anchor="ref5" id="context_ref_5_1">[5]</a>, catalyzed ADS research and industrial implementation. Furthermore, an increase in public interest and market potential precipitated the emergence of ADSs with varying degrees of automation. However, robust automated driving in urban environments has not been achieved yet <a ref-type="bibr" anchor="ref6" id="context_ref_6_1">[6]</a>. Accidents caused by immature systems <a ref-type="bibr" anchor="ref7" id="context_ref_7_1">[7]</a>–<a ref-type="bibr" anchor="ref8" id="context_ref_8_1">[8]</a><a ref-type="bibr" anchor="ref9" id="context_ref_9_1">[9]</a><a ref-type="bibr" anchor="ref10" id="context_ref_10_1">[10]</a> undermine trust, and furthermore, cost lives. As such, a thorough investigation of unsolved challenges and the state-of-the-art is deemed necessary here.</p><p>Eureka Project PROMETHEUS <a ref-type="bibr" anchor="ref11" id="context_ref_11_1">[11]</a> was carried out in Europe between 1987–1995, and it was one of the earliest major automated driving studies. The project led to the development of VITA II by Daimler-Benz, which succeeded in automatically driving on highways <a ref-type="bibr" anchor="ref12" id="context_ref_12_1">[12]</a>. DARPA Grand Challenge, organized by the US Department of Defense in 2004, was the first major automated driving competition where all of the attendees failed to finish the 150-mile off-road parkour. The difficulty of the challenge was in the rule that no human intervention at any level was allowed during the finals. Another similar DARPA Grand Challenge was held in 2005. This time five teams managed to complete the off-road track without any human interference <a ref-type="bibr" anchor="ref13" id="context_ref_13_1">[13]</a>.</p><p>Fully automated driving in urban scenes was seen as the biggest challenge of the field since the earliest attempts. During DARPA Urban Challenge <a ref-type="bibr" anchor="ref26" id="context_ref_26_1">[26]</a>, held in 2007, many different research groups around the globe tried their ADSs in a test environment that was modeled after a typical urban scene. Six teams managed to complete the event. Even though this competition was the biggest and most significant event up to that time, the test environment lacked certain aspects of a real-world urban driving scene such as pedestrians and cyclists. Nevertheless, the fact that six teams managed to complete the challenge attracted significant attention. After DARPA Urban Challenge, several more automated driving competitions such as <a ref-type="bibr" anchor="ref27" id="context_ref_27_1">[27]</a>–<a ref-type="bibr" anchor="ref28" id="context_ref_28_1">[28]</a><a ref-type="bibr" anchor="ref29" id="context_ref_29_1">[29]</a><a ref-type="bibr" anchor="ref30" id="context_ref_30_1">[30]</a> were held in different countries.</p><p>Common practices in system architecture have been established over the years. Most of the ADSs divide the massive task of automated driving into subcategories and employ an array of sensors and algorithms on various modules. More recently, end-to-end driving started to emerge as an alternative to modular approaches. Deep learning models have become dominant in many of these tasks <a ref-type="bibr" anchor="ref31" id="context_ref_31_1">[31]</a>.</p><p>The Society of Automotive Engineers (SAE) refers to hardware-software systems that can execute dynamic driving tasks (DDT) on a sustainable basis as ADS <a ref-type="bibr" anchor="ref32" id="context_ref_32_1">[32]</a>. There are also vernacular alternative terms such as “autonomous driving” and “self-driving car” in use. Nonetheless, despite being commonly used, SAE advices not to use them as these terms are unclear and misleading. In this paper we follow SAE’s convention.</p><p>The present paper attempts to provide a structured and comprehensive overview of state-of-the-art automated driving related hardware-software practices. Moreover, emerging trends such as end-to-end driving and connected systems are discussed in detail. There are overview papers on the subject, which covered several core functions <a ref-type="bibr" anchor="ref15" id="context_ref_15_1">[15]</a>, <a ref-type="bibr" anchor="ref16" id="context_ref_16_1">[16]</a>, and which concentrated only on the motion planning aspect <a ref-type="bibr" anchor="ref18" id="context_ref_18_1">[18]</a>, <a ref-type="bibr" anchor="ref19" id="context_ref_19_1">[19]</a>. However, a survey that covers: present challenges, available and emerging high-level system architectures, individual core functions such as localization, mapping, perception, planning, vehicle control, and human-machine interface altogether does not exist. The aim of this paper is to fill this gap in the literature with a thorough survey. In addition, a detailed summary of available datasets, software stacks, and simulation tools is presented here. Another contribution of this paper is the detailed comparison and analysis of alternative approaches through implementation. We implemented some state-of-the-art algorithms in our platform using open-source software. Comparison of existing overview papers and our work is shown in <a ref-type="table" anchor="table1" class="fulltext-link">Table 1</a>.</p><div class="figure figure-full table" id="table1"><div class="figcaption"><b class="title">TABLE 1 </b>
Comparison of ADS Related Survey Papers</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t1-2983149-large.gif"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts.t1-2983149-small.gif" alt="Table 1- 
Comparison of ADS Related Survey Papers" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t1-2983149-small.gif" data-alt="Table 1- 
Comparison of ADS Related Survey Papers"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><p>The remainder of this paper is written in eight sections. <a ref-type="sec" anchor="sec2" class="fulltext-link">Section II</a> is an overview of present challenges. Details of automated driving system components and architectures are given in <a ref-type="sec" anchor="sec3" class="fulltext-link">Section III</a>. <a ref-type="sec" anchor="sec4" class="fulltext-link">Section IV</a> presents a summary of state-of-the-art localization techniques followed by <a ref-type="sec" anchor="sec5" class="fulltext-link">Section V</a>, an in-depth review of perception models. Assessment of the driving situation and planning are discussed in <a ref-type="sec" anchor="sec6" class="fulltext-link">Section VI</a> and <a ref-type="sec" anchor="sec7" class="fulltext-link">VII</a> respectively. In <a ref-type="sec" anchor="sec8" class="fulltext-link">Section VIII</a>, current trends and shortcomings of human machine interface are introduced. Datasets and available tools for developing automated driving systems are given in <a ref-type="sec" anchor="sec9" class="fulltext-link">Section IX</a>.</p></div>
<div class="section" id="sec2"><div class="header article-hdr"><div class="kicker">
		                        SECTION II.</div><h2>Prospects and Challenges</h2></div><div class="section_2" id="sec2a"><h3>A. Social Impact</h3><p>Widespread usage of ADSs is not imminent. Yet it is still possible to foresee its potential impact and benefits to a certain degree:
</p><ol><li><p><i>Problems that can be solved</i>: preventing traffic accidents, mitigating traffic congestions, reducing emissions</p></li><li><p><i>Arising opportunities</i>: reallocation of driving time, transporting the mobility impaired</p></li><li><p><i>New trends</i>: consuming Mobility as a Service (MaaS), logistics revolution</p></li></ol><p></p><p>Widespread deployment of ADSs can reduce the societal loss caused by erroneous human behavior such as distraction, driving under influence and speeding <a ref-type="bibr" anchor="ref3" id="context_ref_3_2a">[3]</a>.</p><p>Globally, the elder group (over 60 years old) is growing faster than the younger groups <a ref-type="bibr" anchor="ref33" id="context_ref_33_2a">[33]</a>. Increasing the mobility of elderly with ADSs can have a huge impact on the quality of life and productivity of a large portion of the population.</p><p>A shift from personal vehicle-ownership towards consuming Mobility as a Service (MaaS) is an emerging trend. Currently, ride-sharing has lower costs compared to vehicle-ownership under 1000 km annual mileage <a ref-type="bibr" anchor="ref34" id="context_ref_34_2a">[34]</a>. The ratio of owned to shared vehicles is expected to be 50:50 by 2030 <a ref-type="bibr" anchor="ref35" id="context_ref_35_2a">[35]</a>. Large scale deployment of ADSs can accelerate this trend.</p></div><div class="section_2" id="sec2b"><h3>B. Challenges</h3><p>ADSs are complicated robotic systems that operate in indeterministic environments. As such, there are myriad scenarios with unsolved issues. This section discusses the high level challenges of driving automation in general. More minute, task-specific details are discussed in corresponding sections.</p><p>The Society of Automotive Engineers (SAE) defined five levels of driving automation in <a ref-type="bibr" anchor="ref32" id="context_ref_32_2b">[32]</a>. In this taxonomy, <i>level zero</i> stands for no automation at all. Primitive driver assistance systems such as adaptive cruise control, anti-lock braking systems and stability control start with <i>level one</i> <a ref-type="bibr" anchor="ref36" id="context_ref_36_2b">[36]</a>. <i>Level two</i> is partial automation to which advanced assistance systems such as emergency braking or collision avoidance <a ref-type="bibr" anchor="ref37" id="context_ref_37_2b">[37]</a>, <a ref-type="bibr" anchor="ref38" id="context_ref_38_2b">[38]</a> are integrated. With the accumulated knowledge in the vehicle control field and the experience of the industry, level two automation became a feasible technology. The real challenge starts above this level.</p><p><i>Level three</i> is conditional automation; the driver could focus on tasks other than driving during normal operation, however, s/he has to quickly respond to an emergency alert from the vehicle and be ready to take over. In addition, level three ADS operate only in limited operational design domains (ODDs) such as highways. Audi claims to be the first production car to achieve level 3 automation in limited highway conditions <a ref-type="bibr" anchor="ref39" id="context_ref_39_2b">[39]</a>. However, taking over the control manually from the automated mode by the driver raises another issue. Recent studies <a ref-type="bibr" anchor="ref40" id="context_ref_40_2b">[40]</a>, <a ref-type="bibr" anchor="ref41" id="context_ref_41_2b">[41]</a> investigated this problem and found that the takeover situation increases the collision risk with surrounding vehicles. The increased likelihood of an accident during a takeover is a problem that is yet to be solved.</p><p>Human attention is not needed in any degree at level four and five. However, <i>level four</i> can only operate in limited ODDs where special infrastructure or detailed maps exist. In the case of departure from these areas, the vehicle must stop the trip by automatically parking itself. The fully automated system, <i>level five</i>, can operate in any road network and any weather condition. No production vehicle is capable of level four or level five driving automation yet. Moreover, Toyota Research Institute stated that no one in the industry is even close to attaining level five automation <a ref-type="bibr" anchor="ref42" id="context_ref_42_2b">[42]</a>.</p><p>Level four and above driving automation in urban road networks is an open and challenging problem. The environmental variables, from weather conditions to surrounding human behavior, are highly indeterministic and difficult to predict. Furthermore, system failures lead to accidents: in the Hyundai competition one of the ADSs crashed because of rain <a ref-type="bibr" anchor="ref7" id="context_ref_7_2b">[7]</a>, Google’s ADS hit a bus while lane changing because it failed to estimate the speed of a bus <a ref-type="bibr" anchor="ref8" id="context_ref_8_2b">[8]</a>, and Tesla’s Autopilot failed to recognize a white truck and collided with it, killing the driver <a ref-type="bibr" anchor="ref9" id="context_ref_9_2b">[9]</a>.</p><p>Fatalities <a ref-type="bibr" anchor="ref9" id="context_ref_9_2b">[9]</a>, <a ref-type="bibr" anchor="ref10" id="context_ref_10_2b">[10]</a> caused by immature technology undermine public acceptance of ADSs. According to a recent survey <a ref-type="bibr" anchor="ref34" id="context_ref_34_2b">[34]</a>, the majority of consumers question the safety of the technology, and want a significant amount of control over the development and use of ADS. On the other hand, extremely cautious ADSs are also making a negative impression <a ref-type="bibr" anchor="ref43" id="context_ref_43_2b">[43]</a>.</p><p>Ethical dilemmas pose another set of challenges. In an inevitable accident situation, how should the system behave <a ref-type="bibr" anchor="ref44" id="context_ref_44_2b">[44]</a>? Experimental ethics were proposed regarding this issue <a ref-type="bibr" anchor="ref45" id="context_ref_45_2b">[45]</a>.</p><p>Risk and reliability certification is another task yet to be solved. Like in aircraft, ADSs need to be designed with high redundancies that will minimize the chance of a catastrophic failure. Even though there is promising projects in this regard such as DeepTest <a ref-type="bibr" anchor="ref46" id="context_ref_46_2b">[46]</a>, the design-simulation-test-redesign-certification procedure is still not established by the industry nor the rule-makers.</p><p>Finally, various optimization goals such as time to reach the destination, fuel efficiency, comfort, and ride-sharing optimization increases the complexity of an already difficult to solve problem. As such, carrying all of the dynamic driving tasks safely under strict conditions outside a well defined, geofenced area remains as an open problem.</p></div></div>
<div class="section" id="sec3"><div class="header article-hdr"><div class="kicker">
		                        SECTION III.</div><h2>System Components and Architecture</h2></div><div class="section_2" id="sec3a"><h3>A. System Architecture</h3><p>Classification of system architectures is shown in <a ref-type="fig" anchor="fig1" class="fulltext-link">Figure 1</a>. ADSs are designed either as standalone, ego-only systems <a ref-type="bibr" anchor="ref15" id="context_ref_15_3a">[15]</a>, <a ref-type="bibr" anchor="ref47" id="context_ref_47_3a">[47]</a> or connected multi-agent systems <a ref-type="bibr" anchor="ref48" id="context_ref_48_3a">[48]</a>–<a ref-type="bibr" anchor="ref49" id="context_ref_49_3a">[49]</a><a ref-type="bibr" anchor="ref50" id="context_ref_50_3a">[50]</a>. Furthermore, these design philosophies are realized with two alternative approaches: modular <a ref-type="bibr" anchor="ref15" id="context_ref_15_3a">[15]</a>, <a ref-type="bibr" anchor="ref47" id="context_ref_47_3a">[47]</a>, <a ref-type="bibr" anchor="ref51" id="context_ref_51_3a">[51]</a>–<a ref-type="bibr" anchor="ref52" id="context_ref_52_3a">[52]</a><a ref-type="bibr" anchor="ref53" id="context_ref_53_3a">[53]</a><a ref-type="bibr" anchor="ref54" id="context_ref_54_3a">[54]</a><a ref-type="bibr" anchor="ref55" id="context_ref_55_3a">[55]</a><a ref-type="bibr" anchor="ref56" id="context_ref_56_3a">[56]</a><a ref-type="bibr" anchor="ref57" id="context_ref_57_3a">[57]</a><a ref-type="bibr" anchor="ref58" id="context_ref_58_3a">[58]</a> or end-to-end driving <a ref-type="bibr" anchor="ref59" id="context_ref_59_3a">[59]</a>–<a ref-type="bibr" anchor="ref60" id="context_ref_60_3a">[60]</a><a ref-type="bibr" anchor="ref61" id="context_ref_61_3a">[61]</a><a ref-type="bibr" anchor="ref62" id="context_ref_62_3a">[62]</a><a ref-type="bibr" anchor="ref63" id="context_ref_63_3a">[63]</a><a ref-type="bibr" anchor="ref64" id="context_ref_64_3a">[64]</a><a ref-type="bibr" anchor="ref65" id="context_ref_65_3a">[65]</a><a ref-type="bibr" anchor="ref66" id="context_ref_66_3a">[66]</a><a ref-type="bibr" anchor="ref67" id="context_ref_67_3a">[67]</a>.
</p><div class="figure figure-full" id="fig1"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts1-2983149-large.gif" data-fig-id="fig1"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts1-2983149-small.gif" alt="FIGURE 1. - A high level classification of automated driving system architectures." data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts1-2983149-small.gif" data-alt="FIGURE 1. - A high level classification of automated driving system architectures."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 1. </b><fig><p>A high level classification of automated driving system architectures.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><div class="section_2" id="sec3a1"><h4>1) Ego-Only Systems</h4><p>The ego-only approach is to carry all of the necessary automated driving operations on a single self-sufficient vehicle at all times, whereas a connected ADS may or may not depend on other vehicles and infrastructure elements given the situation. Ego-only is the most common approach amongst the state-of-the-art ADSs <a ref-type="bibr" anchor="ref15" id="context_ref_15_3a1">[15]</a>, <a ref-type="bibr" anchor="ref47" id="context_ref_47_3a1">[47]</a>, <a ref-type="bibr" anchor="ref51" id="context_ref_51_3a1">[51]</a>–<a ref-type="bibr" anchor="ref52" id="context_ref_52_3a1">[52]</a><a ref-type="bibr" anchor="ref53" id="context_ref_53_3a1">[53]</a><a ref-type="bibr" anchor="ref54" id="context_ref_54_3a1">[54]</a><a ref-type="bibr" anchor="ref55" id="context_ref_55_3a1">[55]</a><a ref-type="bibr" anchor="ref56" id="context_ref_56_3a1">[56]</a>, <a ref-type="bibr" anchor="ref56" id="context_ref_56_3a1">[56]</a>–<a ref-type="bibr" anchor="ref57" id="context_ref_57_3a1">[57]</a><a ref-type="bibr" anchor="ref58" id="context_ref_58_3a1">[58]</a>. We believe this is due to the practicality of having a self-sufficient platform for development and the additional challenges of connected systems.</p></div><div class="section_2" id="sec3a2"><h4>2) Modular Systems</h4><p>Modular systems, referred as the mediated approach in some works <a ref-type="bibr" anchor="ref59" id="context_ref_59_3a2">[59]</a>, are structured as a pipeline of separate components linking sensory inputs to actuator outputs <a ref-type="bibr" anchor="ref31" id="context_ref_31_3a2">[31]</a>. Core functions of a modular ADS can be summarized as: localization and mapping, perception, assessment, planning and decision making, vehicle control, and human-machine interface. Typical pipelines <a ref-type="bibr" anchor="ref15" id="context_ref_15_3a2">[15]</a>, <a ref-type="bibr" anchor="ref47" id="context_ref_47_3a2">[47]</a>, <a ref-type="bibr" anchor="ref51" id="context_ref_51_3a2">[51]</a>–<a ref-type="bibr" anchor="ref52" id="context_ref_52_3a2">[52]</a><a ref-type="bibr" anchor="ref53" id="context_ref_53_3a2">[53]</a><a ref-type="bibr" anchor="ref54" id="context_ref_54_3a2">[54]</a><a ref-type="bibr" anchor="ref55" id="context_ref_55_3a2">[55]</a><a ref-type="bibr" anchor="ref56" id="context_ref_56_3a2">[56]</a>, <a ref-type="bibr" anchor="ref56" id="context_ref_56_3a2">[56]</a>–<a ref-type="bibr" anchor="ref57" id="context_ref_57_3a2">[57]</a><a ref-type="bibr" anchor="ref58" id="context_ref_58_3a2">[58]</a> start with feeding raw sensor inputs to localization and object detection modules, followed by scene prediction and decision making. Finally, motor commands are generated at the end of the stream by the control module <a ref-type="bibr" anchor="ref31" id="context_ref_31_3a2">[31]</a>, <a ref-type="bibr" anchor="ref68" id="context_ref_68_3a2">[68]</a>.</p><p>Developing individual modules separately divides the challenging task of automated driving into an easier-to-solve set of problems <a ref-type="bibr" anchor="ref69" id="context_ref_69_3a2">[69]</a>. These sub-tasks have their corresponding literature in robotics <a ref-type="bibr" anchor="ref70" id="context_ref_70_3a2">[70]</a>, computer vision <a ref-type="bibr" anchor="ref71" id="context_ref_71_3a2">[71]</a> and vehicle dynamics <a ref-type="bibr" anchor="ref36" id="context_ref_36_3a2">[36]</a>, which makes the accumulated know-how and expertise directly transferable. This is a major advantage of modular systems. In addition, functions and algorithms can be integrated or built upon each other in a modular design. E.g, a safety constraint <a ref-type="bibr" anchor="ref72" id="context_ref_72_3a2">[72]</a> can be implemented on top of a sophisticated planning module to force some hard-coded emergency rules without modifying the inner workings of the planner. This enables designing redundant but reliable architectures.</p><p>The major disadvantages of modular systems are being prone to error propagation <a ref-type="bibr" anchor="ref31" id="context_ref_31_3a2">[31]</a> and over-complexity. In the unfortunate Tesla accident, an error in the perception module in the form of a misclassification of a white trailer as sky, propagated down the pipeline until failure, causing the first ADS related fatality <a ref-type="bibr" anchor="ref46" id="context_ref_46_3a2">[46]</a>.</p></div><div class="section_2" id="sec3a3"><h4>3) End-to-End Driving</h4><p>End-to-end driving, referred as direct perception in some studies <a ref-type="bibr" anchor="ref59" id="context_ref_59_3a3">[59]</a>, generate ego-motion directly from sensory inputs. Ego-motion can be either the continuous operation of steering wheel and pedals or a discrete set of actions, e.g, acceleration and turning left. There are three main approaches for end-to-end driving: direct supervised deep learning <a ref-type="bibr" anchor="ref59" id="context_ref_59_3a3">[59]</a>–<a ref-type="bibr" anchor="ref60" id="context_ref_60_3a3">[60]</a><a ref-type="bibr" anchor="ref61" id="context_ref_61_3a3">[61]</a><a ref-type="bibr" anchor="ref62" id="context_ref_62_3a3">[62]</a><a ref-type="bibr" anchor="ref63" id="context_ref_63_3a3">[63]</a>, neuroevolution <a ref-type="bibr" anchor="ref66" id="context_ref_66_3a3">[66]</a>, <a ref-type="bibr" anchor="ref67" id="context_ref_67_3a3">[67]</a> and the more recent deep reinforcement learning <a ref-type="bibr" anchor="ref64" id="context_ref_64_3a3">[64]</a>, <a ref-type="bibr" anchor="ref65" id="context_ref_65_3a3">[65]</a>. The flow diagram of a generic end-to-end driving system is shown in <a ref-type="fig" anchor="fig2" class="fulltext-link">Figure 2</a> and comparison of the approaches is given in <a ref-type="table" anchor="table2" class="fulltext-link">Table 2</a>.</p><div class="figure figure-full table" id="table2"><div class="figcaption"><b class="title">TABLE 2 </b>
Common End-to-End Driving Approaches</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t2-2983149-large.gif"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts.t2-2983149-small.gif" alt="Table 2- 
Common End-to-End Driving Approaches" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t2-2983149-small.gif" data-alt="Table 2- 
Common End-to-End Driving Approaches"><div class="zoom" title="View Larger Image"></div></a></div></div>
<div class="figure figure-full" id="fig2"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts2ab-2983149-large.gif" data-fig-id="fig2"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts2ab-2983149-small.gif" alt="FIGURE 2. - Information flow diagrams of: (a) a generic modular system, and (b) an end-to-end driving system." data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts2ab-2983149-small.gif" data-alt="FIGURE 2. - Information flow diagrams of: (a) a generic modular system, and (b) an end-to-end driving system."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 2. </b><fig><p>Information flow diagrams of: (a) a generic modular system, and (b) an end-to-end driving system.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>The earliest end-to-end driving attempt dates back to ALVINN <a ref-type="bibr" anchor="ref60" id="context_ref_60_3a3">[60]</a>, where a 3-layer fully connected network was trained to output the direction that the vehicle should follow. An end-to-end driving system for off-road driving was introduced in <a ref-type="bibr" anchor="ref61" id="context_ref_61_3a3">[61]</a>. With the advances in artificial neural network research, deep convolutional and temporal networks became feasible for automated driving tasks. A deep convolutional neural network that takes image as input and outputs steering was proposed in <a ref-type="bibr" anchor="ref62" id="context_ref_62_3a3">[62]</a>. A spatiotemporal network, an FCN-LSTM architecture, was developed for predicting ego-vehicle motion in <a ref-type="bibr" anchor="ref63" id="context_ref_63_3a3">[63]</a>. DeepDriving is another convolutional model that tries to learn a set of discrete perception indicators from the image input <a ref-type="bibr" anchor="ref59" id="context_ref_59_3a3">[59]</a>. This approach is not entirely end-to-end though, the proper driving actions in the perception indicators have to be generated by another module. All of the mentioned methods follow direct supervised training strategies. As such, ground truth is required for training. Usually, the ground truth is the ego-action sequence of an expert human driver and the network learns to imitate the driver. This raises an import design question: should the ADS drive like a human?</p><p>A novel deep reinforcement learning model, Deep Q Networks (DQN), combined reinforcement learning with deep learning <a ref-type="bibr" anchor="ref73" id="context_ref_73_3a3">[73]</a>. In summary, the goal of the network is to select a set of actions that maximize cumulative future rewards. A deep convolutional neural network was used to approximate the optimal action reward function. Actions are generated first with random initialization. Then, the network adjust its parameters with experience instead of direct supervised learning. An automated driving framework using DQN was introduced in <a ref-type="bibr" anchor="ref64" id="context_ref_64_3a3">[64]</a>, where the network was tested in a simulation environment. The first real world run with DQN was achieved in a countryside road without traffic <a ref-type="bibr" anchor="ref65" id="context_ref_65_3a3">[65]</a>. DQN based systems do not imitate the human driver, instead, they learn the optimum way of driving.</p><p>Neuroevolution refers to using evolutionary algorithms to train artificial neural networks <a ref-type="bibr" anchor="ref74" id="context_ref_74_3a3">[74]</a>. End-to-end driving with neuroevolution is not popular as DQN and direct supervised learning. To the best of our knowledge, real world end-to-end driving with neuroevolution is not achieved yet. However, some promising simulation results were obtained <a ref-type="bibr" anchor="ref66" id="context_ref_66_3a3">[66]</a>, <a ref-type="bibr" anchor="ref67" id="context_ref_67_3a3">[67]</a>. ALVINN was trained with neuroevolution and outperformed the direct supervised learning version <a ref-type="bibr" anchor="ref66" id="context_ref_66_3a3">[66]</a>. A RNN was trained with neuroevolution in <a ref-type="bibr" anchor="ref67" id="context_ref_67_3a3">[67]</a> using a driving simulator. The biggest advantage of neuroevolution is the removal of backpropagation, hence, the need for direct supervision.</p><p>End-to-end driving is promising, however it has not been implemented in real-world urban scenes yet, except limited demonstrations. The biggest shortcomings of end-to-end driving in general are the lack of hard coded safety measures and interpretability <a ref-type="bibr" anchor="ref69" id="context_ref_69_3a3">[69]</a>. In addition, DQN and neuroevolution has one major disadvantage over direct supervised learning: these networks must interact with the environment online and fail repeatedly to learn the desired behavior. On the contrary, direct supervised networks can be trained offline with human driving data, and once the training is done, the system is not expected to fail during operation.</p></div><div class="section_2" id="sec3a4"><h4>4) Connected Systems</h4><p>There is no operational connected ADS in use yet, however, some researchers believe this emerging technology will be the future of driving automation <a ref-type="bibr" anchor="ref48" id="context_ref_48_3a4">[48]</a>–<a ref-type="bibr" anchor="ref49" id="context_ref_49_3a4">[49]</a><a ref-type="bibr" anchor="ref50" id="context_ref_50_3a4">[50]</a>. With the use of Vehicular Ad hoc NETwork (VANETs), the basic operations of automated driving can be distributed amongst agents. V2X is a term that stands for “vehicle to everything.” From mobile devices of pedestrians to stationary sensors on a traffic light, an immense amount of data can be accessed by the vehicle with V2X <a ref-type="bibr" anchor="ref22" id="context_ref_22_3a4">[22]</a>. By sharing detailed information of the traffic network amongst peers <a ref-type="bibr" anchor="ref75" id="context_ref_75_3a4">[75]</a>, shortcomings of the ego-only platforms such as sensing range, blind spots, and computational limits may be eliminated. More V2X applications that will increase safety and traffic efficiency are expected to emerge in the foreseeable future <a ref-type="bibr" anchor="ref76" id="context_ref_76_3a4">[76]</a>.</p><p>VANETs can be realized in two different ways: conventional IP based networking and Information-Centric Networking (ICN) <a ref-type="bibr" anchor="ref48" id="context_ref_48_3a4">[48]</a>. For vehicular applications, lots of data have to be distributed amongst agents with intermittent and in less than ideal connections while maintaining high mobility <a ref-type="bibr" anchor="ref50" id="context_ref_50_3a4">[50]</a>. Conventional IP-host based Internet protocol cannot function properly under these conditions. On the other hand, in information-centric networking, vehicles stream query messages to an area instead of a direct address and they accept corresponding responses from any sender <a ref-type="bibr" anchor="ref49" id="context_ref_49_3a4">[49]</a>. Since vehicles are highly mobile and dispersed on the road network, the identity of the information source becomes less relevant. In addition, local data often carries more crucial information for immediate driving tasks such as avoiding a rapidly approaching vehicle on a blind spot.</p><p>Early works, such as the CarSpeak system <a ref-type="bibr" anchor="ref82" id="context_ref_82_3a4">[82]</a>, proved that vehicles can utilize each other’s sensors and use the shared information to execute some dynamic driving tasks. However, without reducing huge amounts of continuous driving data, sharing information between hundreds of thousand vehicles in a city could not become feasible. A semiotic framework that integrates different sources of information and converts raw sensor data into meaningful descriptions was introduced in <a ref-type="bibr" anchor="ref83" id="context_ref_83_3a4">[83]</a> for this purpose. In <a ref-type="bibr" anchor="ref84" id="context_ref_84_3a4">[84]</a>, the term Vehicular Cloud Computing (VCC) was coined and the main advantages of it over conventional Internet cloud applications was introduced. Sensors are the primary cause of the difference. In VCC, sensor information is kept on the vehicle and only shared if there is a local query from another vehicle. This potentially saves the cost of uploading/downloading a constant stream of sensor data to the web. Besides, the high relevance of local data increases the feasibility of VCC. Regular cloud computing was compared to vehicular cloud computing and it was reported that VCC is technologically feasible <a ref-type="bibr" anchor="ref85" id="context_ref_85_3a4">[85]</a>. The term “Internet of Vehicles” (IoV) was proposed for describing a connected ADS <a ref-type="bibr" anchor="ref48" id="context_ref_48_3a4">[48]</a> and the term “vehicular fog” was introduced in <a ref-type="bibr" anchor="ref49" id="context_ref_49_3a4">[49]</a>.</p><p>Establishing an efficient VANET with thousands of vehicles in a city is a huge challenge. For an ICN based VANET, some of the challenging topics are security, mobility, routing, naming, caching, reliability and multi-access computing <a ref-type="bibr" anchor="ref86" id="context_ref_86_3a4">[86]</a>. In summary, even though the potential benefits of a connected system is huge, the additional challenges increase the complexity of the problem to a significant degree. As such, there is no operational connected system yet.</p></div></div><div class="section_2" id="sec3b"><h3>B. Sensors and Hardware</h3><p>State-of-the-art ADSs employ a wide selection of onboard sensors. High sensor redundancy is needed in most of the tasks for robustness and reliability. Hardware units can be categorized into five: exteroceptive sensors for perception, proprioceptive sensors for internal vehicle state monitoring tasks, communication arrays, actuators, and computational units.</p><p>Exteroceptive sensors are mainly used for perceiving the environment, which includes dynamic and static objects, e.g., drivable areas, buildings, pedestrian crossings. Camera, lidar, radar and ultrasonic sensors are the most commonly used modalities for this task. A detailed comparison of exteroceptive sensors is given in <a ref-type="table" anchor="table3" class="fulltext-link">Table 3</a>.</p><div class="figure figure-full table" id="table3"><div class="figcaption"><b class="title">TABLE 3 </b>
Exteroceptive Sensors</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t3-2983149-large.gif"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts.t3-2983149-small.gif" alt="Table 3- 
Exteroceptive Sensors" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t3-2983149-small.gif" data-alt="Table 3- 
Exteroceptive Sensors"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><div class="section_2" id="sec3b1"><h4>1) Monocular Cameras</h4><p>Cameras can sense color and are passive, i.e. they do not emit any signal for measurements. Sensing color is extremely important for tasks such as traffic light recognition. Furthermore, 2D computer vision is an established field with remarkable state-of-the-art algorithms. Moreover, a passive sensor does not interfere with other systems since it does not emit any signals. However, cameras have certain shortcomings. Illumination conditions affect their performance drastically, and depth information is difficult to obtain from a single camera. There are promising studies <a ref-type="bibr" anchor="ref87" id="context_ref_87_3b1">[87]</a> to improve monocular camera based depth perception, but modalities that are not negatively affected by illumination and weather conditions are still necessary for dynamic driving tasks. Other camera types gaining interest for ADS include flash cameras <a ref-type="bibr" anchor="ref77" id="context_ref_77_3b1">[77]</a>, thermal cameras <a ref-type="bibr" anchor="ref79" id="context_ref_79_3b1">[79]</a>, <a ref-type="bibr" anchor="ref80" id="context_ref_80_3b1">[80]</a>, and event cameras <a ref-type="bibr" anchor="ref78" id="context_ref_78_3b1">[78]</a>.</p></div><div class="section_2" id="sec3b2"><h4>2) Omnidirectional Camera</h4><p>For 360° 2D vision, omnidirectional cameras are used as an alternative to camera arrays. They have seen widespread use, with increasingly compact and high performance hardware being constantly released. Panoramic view is particularly desirable for applications such as navigation, localization and mapping <a ref-type="bibr" anchor="ref88" id="context_ref_88_3b2">[88]</a>. An example panoramic image is shown in <a ref-type="fig" anchor="fig3" class="fulltext-link">Figure 3</a>.
</p><div class="figure figure-full" id="fig3"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="./全景摄像头_files/yurts3-2983149-large.gif" data-fig-id="fig3"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts3-2983149-small.gif" alt="FIGURE 3. - Ricoh Tetha V panoramic images collected using our data collection platform, in Nagoya University campus. Note some distortion still remains on the periphery of the image." data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts3-2983149-small.gif" data-alt="FIGURE 3. - Ricoh Tetha V panoramic images collected using our data collection platform, in Nagoya University campus. Note some distortion still remains on the periphery of the image."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 3. </b><fig><p>Ricoh Tetha V panoramic images collected using our data collection platform, in Nagoya University campus. Note some distortion still remains on the periphery of the image.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p></div><div class="section_2" id="sec3b3"><h4>3) Event Cameras</h4><p>Event cameras are among the newer sensing modalities that have seen use in ADS <a ref-type="bibr" anchor="ref89" id="context_ref_89_3b3">[89]</a>. Event cameras record data asynchronously for individual pixels with respect to visual stimulus. The output is therefore an irregular sequence of data points, or <i>events</i> triggered by changes in brightness. The response time is in the order of microseconds <a ref-type="bibr" anchor="ref90" id="context_ref_90_3b3">[90]</a>. The main limitation of current event cameras is pixel size and image resolution. For example, the DAVIS40 image shown in <a ref-type="fig" anchor="fig4" class="fulltext-link">Figure 4</a> has a pixel size of <inline-formula id=""><tex-math notation="LaTeX"><span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-1" style="width: 7.26em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.509em; height: 0px; font-size: 111%;"><span style="position: absolute; clip: rect(1.354em, 1006.51em, 2.505em, -999.997em); top: -2.15em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3" style="font-family: MathJax_Main;">18.5</span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main; padding-left: 0.203em;">×</span><span class="mn" id="MathJax-Span-5" style="font-family: MathJax_Main; padding-left: 0.203em;">18.5</span><span class="mspace" id="MathJax-Span-6" style="height: 0em; vertical-align: 0em; width: 0.153em; display: inline-block; overflow: hidden;"></span><span class="mspace" id="MathJax-Span-7" style="height: 0em; vertical-align: 0em; width: 0.153em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-italic;">μ</span><span class="mtext" id="MathJax-Span-9" style="font-family: MathJax_Main;">m</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-1">18.5 \times 18.5\,\,\mu \text{m}</script>
</tex-math></inline-formula> and a resolution of <inline-formula id=""><tex-math notation="LaTeX"><span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-10" style="width: 4.707em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.207em; height: 0px; font-size: 111%;"><span style="position: absolute; clip: rect(1.304em, 1004.16em, 2.305em, -999.997em); top: -2.15em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mn" id="MathJax-Span-12" style="font-family: MathJax_Main;">240</span><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main; padding-left: 0.203em;">×</span><span class="mn" id="MathJax-Span-14" style="font-family: MathJax_Main; padding-left: 0.203em;">180</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-2">240\times 180</script>
</tex-math></inline-formula>. Recently, a driving dataset with event camera data has been published <a ref-type="bibr" anchor="ref89" id="context_ref_89_3b3">[89]</a>.
</p><div class="figure figure-full" id="fig4"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts4ab-2983149-large.gif" data-fig-id="fig4"><img class="document-ft-image fadeIn" src="./全景摄像头_files/yurts4ab-2983149-small.gif" alt="FIGURE 4. - DAVIS240 events, overlayed on the image (left) and corresponding RBG image from a different camera (right), collected by our data collection platform, at a road crossing near Nagoya University. The motion of the cyclist and vehicle causes brightness changes which trigger events." data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts4ab-2983149-small.gif" data-alt="FIGURE 4. - DAVIS240 events, overlayed on the image (left) and corresponding RBG image from a different camera (right), collected by our data collection platform, at a road crossing near Nagoya University. The motion of the cyclist and vehicle causes brightness changes which trigger events."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 4. </b><fig><p>DAVIS240 events, overlayed on the image (left) and corresponding RBG image from a different camera (right), collected by our data collection platform, at a road crossing near Nagoya University. The motion of the cyclist and vehicle causes brightness changes which trigger events.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p></div><div class="section_2" id="sec3b4"><h4>4) Radar</h4><p>Radar, lidar and ultrasonic sensors are very useful in covering the shortcomings of cameras. Depth information, i.e. distance to objects, can be measured effectively to retrieve 3D information with these sensors, and they are not affected by illumination conditions. However, they are active sensors. Radars emit radio waves that bounce back from objects and measure the time of each bounce. Emissions from active sensors can interfere with other systems. Radar is a well-established technology that is both lightweight and cost-effective. For example, radars can fit inside side-mirrors. Radars are cheaper and can detect objects at longer distances than lidars, but the latter are more accurate.</p></div><div class="section_2" id="sec3b5"><h4>5) Lidar</h4><p>Lidar operates with a similar principle that of radar but it emits infrared light waves instead of radio waves. It has much higher accuracy than radar under 200 meters. Weather conditions such as fog or snow have a negative impact on the performance of lidar. Another aspect is the sensor size: smaller sensors are preferred on the vehicle because of limited space and aerodynamic restraints and lidars are generally larger than radars.</p><p>In <a ref-type="bibr" anchor="ref91" id="context_ref_91_3b5">[91]</a>, human sensing performance is compared to ADS. One of the key findings of this study is that even though human drivers are still better at reasoning in general, the perception capability of ADSs with sensor-fusion can exceed humans, especially in degraded conditions such as insufficient illumination.</p></div><div class="section_2" id="sec3b6"><h4>6) Proprioceptive Sensors</h4><p>Proprioceptive sensing is another crucial category. Vehicle states such as speed, acceleration and yaw must be continuously measured in order to operate the platform safely with feedback. Almost all of the modern production cars are equipped with proprioceptive sensors. Wheel encoders are mainly used for odometry, Inertial Measurement Units (IMU) are employed for monitoring the velocity and position changes, tachometers are utilized for measuring speed and altimeters for altitude. These signals can be accessed through the CAN protocol of modern cars.</p><p>Besides sensors, an ADS needs actuators to manipulate the vehicle and advanced computational units for processing and storing sensor data.</p></div><div class="section_2" id="sec3b7"><h4>7) Full Size Cars</h4><p>There are numerous instrumented vehicles introduced by different research groups, such as Stanford’s Junior <a ref-type="bibr" anchor="ref15" id="context_ref_15_3b7">[15]</a>, which employs an array of sensors with different modalities for perceiving external and internal variables. Boss won the DARPA Urban Challenge with an abundance of sensors <a ref-type="bibr" anchor="ref47" id="context_ref_47_3b7">[47]</a>. RobotCar <a ref-type="bibr" anchor="ref53" id="context_ref_53_3b7">[53]</a> is a cheaper research platform aimed for data collection. In addition, different levels of driving automation have been introduced by the industry; Tesla’s Autopilot <a ref-type="bibr" anchor="ref92" id="context_ref_92_3b7">[92]</a> and Google’s self driving car <a ref-type="bibr" anchor="ref93" id="context_ref_93_3b7">[93]</a> are some examples. Bertha <a ref-type="bibr" anchor="ref57" id="context_ref_57_3b7">[57]</a> is developed by Daimler and has 4 120° short-range radars, two long-range range radar on the sides, stereo camera, wide angle-monocular color camera on the dashboard, another wide-angle camera for the back. Our vehicle is shown in <a ref-type="fig" anchor="fig5" class="fulltext-link">Figure 5</a>. A detailed comparison of sensor setups of 10 different full-size ADSs is given in <a ref-type="table" anchor="table4" class="fulltext-link">Table 4</a>.</p><div class="figure figure-full table" id="table4"><div class="figcaption"><b class="title">TABLE 4 </b>
Onboard Sensor Setup of ADS Equipped Vehicles</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t4-2983149-large.gif"><img class="document-ft-image fadeIn" src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t4-2983149-small.gif" alt="Table 4- 
Onboard Sensor Setup of ADS Equipped Vehicles" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t4-2983149-small.gif" data-alt="Table 4- 
Onboard Sensor Setup of ADS Equipped Vehicles"><div class="zoom" title="View Larger Image"></div></a></div></div>
<div class="figure figure-full" id="fig5"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts5-2983149-large.gif" data-fig-id="fig5"><img class="document-ft-image fadeIn" src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts5-2983149-small.gif" alt="FIGURE 5. - The ADS equipped Prius of Nagoya University. We have used this vehicle to perform core automated driving operations." data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts5-2983149-small.gif" data-alt="FIGURE 5. - The ADS equipped Prius of Nagoya University. We have used this vehicle to perform core automated driving operations."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 5. </b><fig><p>The ADS equipped Prius of Nagoya University. We have used this vehicle to perform core automated driving operations.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p></div><div class="section_2" id="sec3b8"><h4>8) Large Vehicles and Trailers</h4><p>Earliest intelligent trucks were developed for the PATH program in California <a ref-type="bibr" anchor="ref102" id="context_ref_102_3b8">[102]</a>, which utilized magnetic markers on the road. Fuel economy is an essential topic in freight transportation and methods such as platooning has been developed for this purpose. Platooning is a well-studied phenomenon; it reduces drag and therefore fuel consumption <a ref-type="bibr" anchor="ref103" id="context_ref_103_3b8">[103]</a>. In semi-autonomous truck platooning, the lead truck is driven by a human driver, and several automated trucks follow it; forming a semi-autonomous road-train as defined in <a ref-type="bibr" anchor="ref104" id="context_ref_104_3b8">[104]</a>. Sartre European Union project <a ref-type="bibr" anchor="ref105" id="context_ref_105_3b8">[105]</a> introduced such a system that satisfies three core conditions: using the already existing public road network, sharing the traffic with non-automated vehicles and not modifying the road infrastructure. A platoon consisting of three automated trucks was formed in <a ref-type="bibr" anchor="ref103" id="context_ref_103_3b8">[103]</a> and significant fuel savings were reported.</p><p>Tractor-trailer setup poses an additional challenge for automated freight transport. Conventional control methods such as feedback linearization <a ref-type="bibr" anchor="ref106" id="context_ref_106_3b8">[106]</a> and fuzzy control <a ref-type="bibr" anchor="ref107" id="context_ref_107_3b8">[107]</a> were used for path tracking without considering the jackknifing constraint. The possibility of jackknifing, the collision of the truck and the trailer with each other, increases the difficulty of the task <a ref-type="bibr" anchor="ref108" id="context_ref_108_3b8">[108]</a>. A control safety governor design was proposed in <a ref-type="bibr" anchor="ref108" id="context_ref_108_3b8">[108]</a> to prevent jackknifing while reversing.</p></div></div></div>
<div class="section" id="sec4"><div class="header article-hdr"><div class="kicker">
		                        SECTION IV.</div><h2>Localization and Mapping</h2></div><p>Localization is the task of finding ego-position relative to a reference frame in an environment <a ref-type="bibr" anchor="ref17" id="context_ref_17_4">[17]</a>, and it is fundamental to any mobile robot. It is especially crucial for ADSs <a ref-type="bibr" anchor="ref21" id="context_ref_21_4">[21]</a>; the vehicle must use the correct lane and position itself in it accurately. Furthermore, localization is an elemental requirement for global navigation.</p><p>The reminder of this section details the three most common approaches that use solely on-board sensors: Global Positioning System and Inertial Measurement Unit (GPS-IMU) fusion, Simultaneous Localization And Mapping (SLAM), and state-of-the-art a priori map-based localization. Readers are referred to <a ref-type="bibr" anchor="ref17" id="context_ref_17_4">[17]</a> for a broader localization overview. A comparison of localization methods is given in <a ref-type="table" anchor="table5" class="fulltext-link">Table 5</a>.</p><div class="figure figure-full table" id="table5"><div class="figcaption"><b class="title">TABLE 5 </b>
Localization Techniques</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t5-2983149-large.gif"><img class="document-ft-image fadeIn" src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t5-2983149-small.gif" alt="Table 5- 
Localization Techniques" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t5-2983149-small.gif" data-alt="Table 5- 
Localization Techniques"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><div class="section_2" id="sec4a"><h3>A. GPS-IMU Fusion</h3><p>The main principle of GPS-IMU fusion is correcting accumulated errors of dead reckoning in intervals with absolute position readings <a ref-type="bibr" anchor="ref109" id="context_ref_109_4a">[109]</a>. In a GPS-IMU system, changes in position and orientation are measured by IMU, and this information is processed for localizing the vehicle with dead reckoning. There is a significant drawback of IMU, and in general dead reckoning: errors accumulate with time and they often lead to failure in long-term operations <a ref-type="bibr" anchor="ref110" id="context_ref_110_4a">[110]</a>. With the integration of GPS readings, the accumulated errors of the IMU can be corrected in intervals.</p><p>GPS-IMU systems by themselves cannot be used for vehicle localization as they do not meet the performance criteria <a ref-type="bibr" anchor="ref111" id="context_ref_111_4a">[111]</a>. In the 2004 DARPA Grand Challenge, the red team from Carnegie Mellon University <a ref-type="bibr" anchor="ref96" id="context_ref_96_4a">[96]</a> failed the race because of a GPS error. The accuracy required for urban automated driving is too high for the current GPS-IMU systems used in production cars. Moreover, in dense urban environments, the accuracy drops further, and the GPS stops functioning from time to time because of tunnels <a ref-type="bibr" anchor="ref109" id="context_ref_109_4a">[109]</a> and high buildings.</p><p>Even though GPS-IMU systems by themselves do not meet the performance requirements and can only be utilized for high-level route planning, they are used for initial pose estimation in tandem with lidar and other sensors in state-of-the-art localization systems <a ref-type="bibr" anchor="ref111" id="context_ref_111_4a">[111]</a>.</p></div><div class="section_2" id="sec4b"><h3>B. Simultaneous Localization and Mapping</h3><p>Simultaneous localization and mapping (SLAM) is the act of online map making and localizing the vehicle in it at the same time. A priori information about the environment is not required in SLAM. It is a common practice in robotics, especially in indoor environments. However, due to the high computational requirements and environmental challenges, running SLAM algorithms outdoors, which is the operational domain of ADSs, is less efficient than localization with a pre-built map <a ref-type="bibr" anchor="ref112" id="context_ref_112_4b">[112]</a>.</p><p>Team MIT used a SLAM approach in DARPA urban challenge <a ref-type="bibr" anchor="ref113" id="context_ref_113_4b">[113]</a> and finished it in the 4th place. Whereas, the winner, Carnegie Mellon‘s Boss <a ref-type="bibr" anchor="ref47" id="context_ref_47_4b">[47]</a> and the runner-up, Stanford‘s Junior <a ref-type="bibr" anchor="ref15" id="context_ref_15_4b">[15]</a>, both utilized a priori information. In spite of not having the same level of accuracy and efficiency, SLAM techniques have one major advantage over a priori methods: they can work anywhere.</p><p>SLAM based methods have the potential to replace a priori techniques if their performances can be increased further <a ref-type="bibr" anchor="ref20" id="context_ref_20_4b">[20]</a>. We refer the readers to <a ref-type="bibr" anchor="ref21" id="context_ref_21_4b">[21]</a> for a detailed SLAM survey in the intelligent vehicle domain.</p></div><div class="section_2" id="sec4c"><h3>C. A Priori Map-Based Localization</h3><p>The core idea of a priori map-based localization techniques is matching: localization is achieved through the comparison of online readings to the information on a detailed pre-built map and finding the location of the best possible match <a ref-type="bibr" anchor="ref111" id="context_ref_111_4c">[111]</a>. Often an initial pose estimation, for example with a GPS, is used at the beginning of the matching process. There are various approaches to map building and preferred modalities.</p><p>Changes in the environment affect the performance of map-based methods negatively. This effect is prevalent especially in rural areas where past information of the map can deviate from the actual environment because of changes in roadside vegetation and constructions <a ref-type="bibr" anchor="ref114" id="context_ref_114_4c">[114]</a>. Moreover, this method requires an additional step of map making.</p><p>There are two different map-based approaches; landmark search and matching.</p><div class="section_2" id="sec4c1"><h4>1) Landmark Search</h4><p>Landmark search is computationally less expensive in comparison to point cloud matching. It is a robust localization technique as long as a sufficient amount of landmarks exists. In an urban environment, poles, curbs, signs and road markers can be used as landmarks.</p><p>A road marking detection method using lidar and Monte Carlo Localization (MCL) was used in <a ref-type="bibr" anchor="ref98" id="context_ref_98_4c1">[98]</a>. In this method, road markers and curbs were matched to a 3D map to find the location of the vehicle. A vision based road marking detection method was introduced in <a ref-type="bibr" anchor="ref115" id="context_ref_115_4c1">[115]</a>. Road markings detected by a single front camera were compared and matched to a low-volume digital marker map with global coordinates. Then, a particle filter was employed to update the position and heading of the vehicle with the detected road markings and GPS-IMU output. A road marking detection based localization technique using; two cameras directed towards the ground, GPS-IMU dead reckoning, odometry, and a precise marker location map was proposed in <a ref-type="bibr" anchor="ref116" id="context_ref_116_4c1">[116]</a>. Another vision based method with a single camera and geo-referenced traffic signs was presented in <a ref-type="bibr" anchor="ref117" id="context_ref_117_4c1">[117]</a>.</p><p>This approach has one major disadvantage; landmark dependency makes the system prone to fail where landmark amount is insufficient.</p></div><div class="section_2" id="sec4c2"><h4>2) Point Cloud Matching</h4><p>The state-of-the-art localization systems use multi-modal point cloud matching based approaches. In summary, the online-scanned point cloud, which covers a smaller area, is translated and rotated around its center iteratively to be compared against the larger a priori point cloud map. The position and orientation that gives the best match between the two point clouds give the localized position of the sensor relative to the map. For initial pose estimation, GPS is used commonly along dead reckoning. We used this approach to localize our vehicle. The matching process is shown in <a ref-type="fig" anchor="fig6" class="fulltext-link">Figure 6</a> and the map-making in <a ref-type="fig" anchor="fig7" class="fulltext-link">Figure 7</a>.
</p><div class="figure figure-full" id="fig6"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts6-2983149-large.gif" data-fig-id="fig6"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts6-2983149-small.gif" data-alt="FIGURE 6. - We used NDT matching [101], [118] to localize our vehicle in the Nagoya University campus. White points belong to the offline pre-built map and the colored ones were obtained from online scans. The objective is to find the best match between colored points and white points, thus localizing the vehicle."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 6. </b><fig><p>We used NDT matching <a ref-type="bibr" anchor="ref101" id="context_ref_101_4c2">[101]</a>, <a ref-type="bibr" anchor="ref118" id="context_ref_118_4c2">[118]</a> to localize our vehicle in the Nagoya University campus. White points belong to the offline pre-built map and the colored ones were obtained from online scans. The objective is to find the best match between colored points and white points, thus localizing the vehicle.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div>
<div class="figure figure-full" id="fig7"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts7-2983149-large.gif" data-fig-id="fig7"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts7-2983149-small.gif" data-alt="FIGURE 7. - Creating a 3D pointcloud map with congregation of scans. We used Autoware [122] for mapping."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 7. </b><fig><p>Creating a 3D pointcloud map with congregation of scans. We used Autoware <a ref-type="bibr" anchor="ref122" id="context_ref_122_4c2">[122]</a> for mapping.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>In the seminal work of <a ref-type="bibr" anchor="ref111" id="context_ref_111_4c2">[111]</a>, a point cloud map collected with lidar was used to augment inertial navigation and localization. A particle filter maintained a three-dimensional vector of 2D coordinates and the yaw angle. A multi-modal approach with probabilistic maps was utilized in <a ref-type="bibr" anchor="ref100" id="context_ref_100_4c2">[100]</a> to achieve localization in urban environments with less than 10 cm RMS error. Instead of comparing two point clouds point by point and discarding the mismatched reads, the variance of all observed data was modeled and used for the matching task. A matching algorithm for lidar scans using multi-resolution Gaussian Mixture Maps (GMM) was proposed in <a ref-type="bibr" anchor="ref119" id="context_ref_119_4c2">[119]</a>. Iterative Closest Point (ICP) was compared against Normal Distribution Transform (NDT) in <a ref-type="bibr" anchor="ref118" id="context_ref_118_4c2">[118]</a>, <a ref-type="bibr" anchor="ref120" id="context_ref_120_4c2">[120]</a>. In NDT, accumulated sensor readings are transformed into a grid that is represented by the mean and covariance obtained from the scanned points that fall into its’ cells/voxels. NDT proved to be more robust than point-to-point ICP matching. An improved version of 3D NDT matching was proposed in <a ref-type="bibr" anchor="ref101" id="context_ref_101_4c2">[101]</a>, and <a ref-type="bibr" anchor="ref114" id="context_ref_114_4c2">[114]</a> augmented NDT with road marker matching. An NDT-based Monte Carlo Localization (MCL) method that utilizes an offline static map and a constantly updated short-term map was developed by <a ref-type="bibr" anchor="ref121" id="context_ref_121_4c2">[121]</a>. In this method, NDT occupancy grid was used for the short-term map and it was utilized only when and where the static map failed to give sufficient explanations.</p><p>Map-making and maintaining is time and resource consuming. Therefore some researchers such as <a ref-type="bibr" anchor="ref99" id="context_ref_99_4c2">[99]</a> argue that methods with a priori maps are not feasible given the size of road networks and rapid changes.</p></div><div class="section_2" id="sec4c3"><h4>3) 2D to 3D Matching</h4><p>Matching online 2D readings to a 3D a priori map is an emerging technology. This approach requires only a camera on the ADS equipped vehicle instead of the more expensive lidar. The a priori map still needs to be created with a lidar.</p><p>A monocular camera was used to localize the vehicle in a point cloud map in <a ref-type="bibr" anchor="ref123" id="context_ref_123_4c3">[123]</a>. With an initial pose estimation, 2D synthetic images were created from the offline 3D point cloud map and they were compared with normalized mutual information to the online images received from the camera. This method increases the computational load of the localization task. Another vision matching algorithm was introduced in <a ref-type="bibr" anchor="ref124" id="context_ref_124_4c3">[124]</a> where a stereo camera setup was utilized to compare online readings to synthetic depth images generated from 3D prior.</p><p>Camera based localization approaches could become popular in the future as the hardware requirement is cheaper than lidar based systems.</p></div></div></div>
<div class="section" id="sec5"><div class="header article-hdr"><div class="kicker">
		                        SECTION V.</div><h2>Perception</h2></div><p>Perceiving the surrounding environment and extracting information which may be critical for safe navigation is a critical objective for ADS. A variety of tasks, using different sensing modalities, fall under the category of perception. Building on decades of computer vision research, cameras are the most commonly used sensor for perception, with 3D vision becoming a strong alternative/supplement.</p><p>The reminder of this section is divided into core perception tasks. We discuss image-based object detection in <a ref-type="sec" anchor="sec5a1" class="fulltext-link">Section V-A1</a>, semantic segmentation in <a ref-type="sec" anchor="sec5a2" class="fulltext-link">Section V-A2</a>, 3D object detection in <a ref-type="sec" anchor="sec5a3" class="fulltext-link">Section V-A3</a>, road and lane detection in <a ref-type="sec" anchor="sec5c" class="fulltext-link">Section V-C</a> and object tracking in <a ref-type="sec" anchor="sec5b" class="fulltext-link">Section V-B</a>.</p><div class="section_2" id="sec5a"><h3>A. Detection</h3><div class="section_2" id="sec5a1"><h4>1) Image-Based Object Detection</h4><p>Object detection refers to identifying the location and size of objects of interest. Both static objects, from traffic lights and signs to road crossings, and dynamic objects such as other vehicles, pedestrians or cyclists are of concern to ADSs. Generalized object detection has a long-standing history as a central problem in computer vision, where the goal is to determine if objects of specific classes are present in an image, then to determine their size via a rectangular bounding box. This section mainly discusses state-of-the-art object detection methods, as they represent the starting point of several other tasks in an ADS pipe, such as object tracking and scene understanding.</p><p>Object recognition research started more than 50 years ago, but only recently, in the late 1990s and early 2000s, has algorithm performance reached a level of relevance for driving automation. In 2012, the deep convolutional neural network (DCNN) AlexNet <a ref-type="bibr" anchor="ref4" id="context_ref_4_5a1">[4]</a> shattered the ImageNet image recognition challenge <a ref-type="bibr" anchor="ref131" id="context_ref_131_5a1">[131]</a>. This resulted in a near complete shift of focus to supervised learning and in particular deep learning for object detection. There exists a number of extensive surveys on general image-based object detection <a ref-type="bibr" anchor="ref132" id="context_ref_132_5a1">[132]</a>–<a ref-type="bibr" anchor="ref133" id="context_ref_133_5a1">[133]</a><a ref-type="bibr" anchor="ref134" id="context_ref_134_5a1">[134]</a>. Here, the focus is on the state-of-the-art methods that could be applied to ADS.</p><p>While state-of-the-art methods all rely on DCNNs, there currently exist a clear distinction between them:
</p><ol><li><p>Single stage detection frameworks use a single network to produce object detection locations and class prediction simultaneously.</p></li><li><p>Region proposal detection frameworks use two distinct stages, where general regions of interest are first proposed, then categorized by separate classifier networks.</p></li></ol><p></p><p>Region proposal methods are currently leading detection benchmarks, but at the cost requiring high computation power, and generally being difficult to implement, train and fine-tune. Meanwhile, single stage detection algorithms tend to have fast inference time and low memory cost, which is well-suited for real-time driving automation. YOLO (You Only Look Once) <a ref-type="bibr" anchor="ref135" id="context_ref_135_5a1">[135]</a> is a popular single stage detector, which has been improved continuously <a ref-type="bibr" anchor="ref128" id="context_ref_128_5a1">[128]</a>, <a ref-type="bibr" anchor="ref136" id="context_ref_136_5a1">[136]</a>. Their network uses a DCNN to extract image features on a coarse grid, significantly reducing the resolution of the input image. A fully-connected neural network then predicts class probabilities and bounding box parameters for each grid cell and class. This design makes YOLO very fast, the full model operating at 45 FPS and a smaller model operating at 155 FPS for a small accuracy trade-off. More recent versions of this method, YOLOv2, YOLO9000 <a ref-type="bibr" anchor="ref136" id="context_ref_136_5a1">[136]</a> and YOLOv3 <a ref-type="bibr" anchor="ref128" id="context_ref_128_5a1">[128]</a> briefly took over the PASCAL VOC and MS COCO benchmarks while maintaining low computation and memory cost. Another widely used algorithm, even faster than YOLO, is the Single Shot Detector (SSD) <a ref-type="bibr" anchor="ref137" id="context_ref_137_5a1">[137]</a>, which uses standard DCNN architectures such as VGG <a ref-type="bibr" anchor="ref130" id="context_ref_130_5a1">[130]</a> to achieve competitive results on public benchmarks. SSD performs detection on a coarse grid similar to YOLO, but also uses higher resolution features obtained early in the DCNN to improve detection and localization of small objects.</p><p>Considering both accuracy and computational cost is essential for detection in ADS; the detection needs to be reliable, but also operate better than real-time, to allow as much time as possible for the planning and control modules to react to those objects. As such, single stage detectors are often the detection algorithms of choice for ADSs. However, as shown in <a ref-type="table" anchor="table6" class="fulltext-link">Table 6</a>, region proposal networks (RPN), used in two-stage detection frameworks, have proven to be unmatched in terms of object recognition and localization accuracy, and computational cost has improved greatly in recent years. They are also better suited for other tasks related to detection, such as semantic segmentation as discussed in <a ref-type="sec" anchor="sec5a2" class="fulltext-link">Section V-A2</a>. Through transfer learning, RPNs achieving multiple perception tasks simultaneously are become increasingly feasible for online applications <a ref-type="bibr" anchor="ref138" id="context_ref_138_5a1">[138]</a>. RPNs can replace single stage detection networks for ADS applications in the near future.</p><div class="figure figure-full table" id="table6"><div class="figcaption"><b class="title">TABLE 6 </b>
Comparison of 2D Bounding Box Estimation Architectures on the Test Set of ImageNet1K, Ordered by Top 5% Error. Number of Parameters (Num. Params) and Number of Layers (Num. Layers), Hints at the Computational Cost of the Algorithm</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t6-2983149-large.gif"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t6-2983149-small.gif" data-alt="Table 6- 
Comparison of 2D Bounding Box Estimation Architectures on the Test Set of ImageNet1K, Ordered by Top 5% Error. Number of Parameters (Num. Params) and Number of Layers (Num. Layers), Hints at the Computational Cost of the Algorithm"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><div class="section_2" id="sec5a1a"><h5>a: Omnidirectional and Event Camera-Based Perception</h5><p>360 degree vision, or at least panoramic vision, is necessary for higher levels of automation. This can be achieved through camera arrays, though precise extrinsic calibration between each camera is then necessary to make image stitching possible. Alternatively, omnidirectional cameras can be used, or a smaller array of cameras with very wide angle <i>fisheye</i> lenses. These are however difficult to intrinsically calibrate; the spherical images are highly distorted and the camera model used must account for mirror reflections or fisheye lens distortions, depending on the camera model producing the panoramic images <a ref-type="bibr" anchor="ref141" id="context_ref_141_5a1a">[141]</a>, <a ref-type="bibr" anchor="ref142" id="context_ref_142_5a1a">[142]</a>. The accuracy of the model and calibration dictates the quality of undistorted images produced, on which the aforementioned 2D vision algorithms are used. An example of fisheye lenses producing two spherical images then combined into one panoramic image is shown in <a ref-type="fig" anchor="fig3" class="fulltext-link">Figure 3</a>. Some distortions inevitably remain, but despite these challenges in calibration, omnidirectional cameras have been used for many applications such as SLAM <a ref-type="bibr" anchor="ref143" id="context_ref_143_5a1a">[143]</a> and 3D reconstruction <a ref-type="bibr" anchor="ref144" id="context_ref_144_5a1a">[144]</a>.</p><p>Event cameras are a fairly new modality which output asynchronous <i>events</i> usually caused by movement in the observed scene, as shown in <a ref-type="fig" anchor="fig4" class="fulltext-link">Figure 4</a>. This makes the sensing modality interesting for dynamic object detection. The other appealing factor is their response time on the order of microseconds <a ref-type="bibr" anchor="ref90" id="context_ref_90_5a1a">[90]</a>, as frame rate is a significant limitation for high-speed driving. The sensor resolution remains an issue, but new models are rapidly improving. They have been used for a variety of applications closely related to ADS. A recent survey outlines progress in pose estimation and SLAM, visual-inertial odometry and 3D reconstruction, as well as other applications <a ref-type="bibr" anchor="ref145" id="context_ref_145_5a1a">[145]</a>. Most notably, a dataset for end-to-end driving with event cameras was recently published, with preliminary experiments showing that the output of an event camera can, to some extent, be used to predict car steering angle <a ref-type="bibr" anchor="ref89" id="context_ref_89_5a1a">[89]</a>.</p></div><div class="section_2" id="sec5a1b"><h5>b: Poor Illumination and Changing Appearance</h5><p>The main drawback with using camera is that changes in lighting conditions can significantly affect their performance. Low light conditions are inherently difficult to deal with, while changes in illumination due to shifting shadows, intemperate weather, or seasonal changes, can cause algorithms to fail, in particular supervised learning methods. For example, snow drastically alters the appearance of scenes and hides potentially key features such as lane markings. An easy alternative is to use an alternate sensing modalities for perception, but lidar also has difficulties with some weather conditions like fog and snow <a ref-type="bibr" anchor="ref146" id="context_ref_146_5a1b">[146]</a>, and radars lack the necessary resolution for many perception tasks <a ref-type="bibr" anchor="ref51" id="context_ref_51_5a1b">[51]</a>. A sensor fusion strategy is often employed to avoid any single point of failure <a ref-type="bibr" anchor="ref147" id="context_ref_147_5a1b">[147]</a>.</p><p>Thermal imaging through infrared sensors are also used for object detection in low light conditions, which is particularly effective for pedestrian detection <a ref-type="bibr" anchor="ref148" id="context_ref_148_5a1b">[148]</a>. Camera-only methods which attempt to deal with dynamic lighting conditions directly have also been developed. Both attempting to extract lighting invariant features <a ref-type="bibr" anchor="ref149" id="context_ref_149_5a1b">[149]</a> and assessing the quality of features <a ref-type="bibr" anchor="ref150" id="context_ref_150_5a1b">[150]</a> have been proposed. Pre-processed, <i>illumination invariant</i> images have applied to ADS <a ref-type="bibr" anchor="ref151" id="context_ref_151_5a1b">[151]</a> and were shown to improve localization, mapping and scene classification capabilities over long periods of time. Still, dealing with the unpredictable conditions brought forth by inadequate or changing illumination remains a central challenge preventing the widespread implementation of ADS.</p></div></div><div class="section_2" id="sec5a2"><h4>2) Semantic Segmentation</h4><p>Beyond image classification and object detection, computer vision research has also tackled the task of image segmentation. This consists of classifying each pixel of an image with a class label. This task is of particular importance to driving automation as some objects of interest are poorly defined by bounding boxes, in particular roads, traffic lines, sidewalks and buildings. A segmented scene in an urban area can be seen in <a ref-type="fig" anchor="fig8" class="fulltext-link">Figure 8</a>. As opposed to semantic segmentation, which labels pixels based on a class, instance segmentation algorithms further separates instances of the same class, which is important in the context of driving automation. In other words, objects which may have different trajectories and behaviors must be differentiated from each other. We used the COCO dataset <a ref-type="bibr" anchor="ref152" id="context_ref_152_5a2">[152]</a> to train the instance segmentation algorithm Mask R-CNN <a ref-type="bibr" anchor="ref138" id="context_ref_138_5a2">[138]</a> with the sample result shown in <a ref-type="fig" anchor="fig8" class="fulltext-link">Figure 8</a>.
</p><div class="figure figure-full" id="fig8"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts8abcd-2983149-large.gif" data-fig-id="fig8"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts8abcd-2983149-small.gif" data-alt="FIGURE 8. - An urban scene near Nagoya University, with camera and lidar data collected by our experimental vehicle and object detection outputs from state-of-the-art perception algorithms. (a) A front facing camera’s view, with bounding box results from YOLOv3 [128] and (b) instance segmentation results from MaskRCNN [138]. (c) Semantic segmentation masks produced by DeepLabv3 [139]. (d) The 3D Lidar data with object detection results from SECOND [140]. Amongst the four, only the 3D perception algorithm outputs range to detected objects."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 8. </b><fig><p>An urban scene near Nagoya University, with camera and lidar data collected by our experimental vehicle and object detection outputs from state-of-the-art perception algorithms. (a) A front facing camera’s view, with bounding box results from YOLOv3 <a ref-type="bibr" anchor="ref128" id="context_ref_128_5a2">[128]</a> and (b) instance segmentation results from MaskRCNN <a ref-type="bibr" anchor="ref138" id="context_ref_138_5a2">[138]</a>. (c) Semantic segmentation masks produced by DeepLabv3 <a ref-type="bibr" anchor="ref139" id="context_ref_139_5a2">[139]</a>. (d) The 3D Lidar data with object detection results from SECOND <a ref-type="bibr" anchor="ref140" id="context_ref_140_5a2">[140]</a>. Amongst the four, only the 3D perception algorithm outputs range to detected objects.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>Segmentation has recently become feasible for real-time applications. Generally, developments in this field progress in parallel with image-based object detection. The aforementioned Mask R-CNN <a ref-type="bibr" anchor="ref138" id="context_ref_138_5a2">[138]</a> is a generalization of Faster R-CNN <a ref-type="bibr" anchor="ref153" id="context_ref_153_5a2">[153]</a>. The multi-task R-CNN network can achieve accurate bounding box estimation and instance segmentation simultaneously and can also be generalized to other tasks like pedestrian pose estimation with minimal domain knowledge. Running at 5 fps means it is approaching the area of real-time use for ADS.</p><p>Unlike Mask-RCNN’s architecture which is more akin to those used for object detection through its use of region proposal networks, segmentation networks usually employ a combination of convolutions for feature extraction. Those are followed by deconvolutions, also called transposed convolutions, to obtain pixel resolution labels <a ref-type="bibr" anchor="ref154" id="context_ref_154_5a2">[154]</a>, <a ref-type="bibr" anchor="ref155" id="context_ref_155_5a2">[155]</a>. Feature pyramid networks are also commonly used, for example in PSPNet <a ref-type="bibr" anchor="ref156" id="context_ref_156_5a2">[156]</a>, which also introduced dilated convolutions for segmentation. This idea of sparse convolutions was then used to develop DeepLab <a ref-type="bibr" anchor="ref157" id="context_ref_157_5a2">[157]</a>, with the most recent version being the current state-of-the-art for object segmentation <a ref-type="bibr" anchor="ref139" id="context_ref_139_5a2">[139]</a>. We employed DeepLab with our ADS and a segmented frame is shown in <a ref-type="fig" anchor="fig8" class="fulltext-link">Figure 8</a>.</p><p>While most segmentation networks are as of yet too slow and computationally expensive to be used in ADS, it is important to notice that many of these segmentations networks are initially trained for different tasks, such as bounding box estimation, then generalized to segmentation networks. Furthermore, these networks were shown to learn universal feature representations of images, and can be generalized for many tasks. This suggests the possibility that single, generalized perception networks may be able to tackle all perception tasks required for an ADS.</p></div><div class="section_2" id="sec5a3"><h4>3) 3D Object Detection</h4><p>Given their affordability, availability and widespread research, cameras are used by nearly all algorithms presented so far as the primary perception modality. However, cameras have limitations that are critical to ADS. Aside from illumination which was previously discussed, camera-based object detection occurs in the projected image space and therefore the scale of the scene is unknown. To make use of this information for dynamic driving tasks like obstacle avoidance, it is necessary to bridge the gap from 2D image-based detection to the 3D, metric space. Depth estimation is therefore necessary, which is in fact possible with a single camera <a ref-type="bibr" anchor="ref158" id="context_ref_158_5a3">[158]</a> though stereo or multi-view systems are more robust <a ref-type="bibr" anchor="ref159" id="context_ref_159_5a3">[159]</a>. These algorithms necessarily need to solve an expensive image matching problem, which adds a significant amount of processing cost to an already complex perception pipeline.</p><p>A relatively new sensing modality, the 3D lidar, offers an alternative for 3D perception. The 3D data collected inherently solves the scale problem, and since they have their own emission source, they are far less dependable on lighting condition, and less susceptible to intemperate weather. The sensing modality collects sparse 3D points representing the surfaces of the scene, as shown in <a ref-type="fig" anchor="fig9" class="fulltext-link">Figure 9</a>, which are challenging to use for object detection and classification. The appearance of objects change with range, and after some distance, very few data points per objects are available to detect an object. This poses some challenges for detection, but since the data is a direct representation of the world, it is more easily separable. Traditional methods often use euclidean clustering <a ref-type="bibr" anchor="ref160" id="context_ref_160_5a3">[160]</a> or region-growing methods <a ref-type="bibr" anchor="ref161" id="context_ref_161_5a3">[161]</a> for grouping points into objects. This approach has been made much more robust through various filtering techniques, such as ground filtering <a ref-type="bibr" anchor="ref162" id="context_ref_162_5a3">[162]</a> and map-based filtering <a ref-type="bibr" anchor="ref163" id="context_ref_163_5a3">[163]</a>. We implemented a 3D object detection pipeline to get clustered objects from raw point cloud input. An example of this process is shown in <a ref-type="fig" anchor="fig9" class="fulltext-link">Figure 9</a>.
</p><div class="figure figure-full" id="fig9"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts9-2983149-large.gif" data-fig-id="fig9"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts9-2983149-small.gif" data-alt="FIGURE 9. - Outline of a traditional method for object detection from 3D pointcloud data. Various filtering and data reduction methods are used first, followed by clustering. The resulting clusters are shown by the different colored points in the 3D lidar data of pedestrians collected by our data collection platform."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 9. </b><fig><p>Outline of a traditional method for object detection from 3D pointcloud data. Various filtering and data reduction methods are used first, followed by clustering. The resulting clusters are shown by the different colored points in the 3D lidar data of pedestrians collected by our data collection platform.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>As with image-based methods, machine learning has also recently taken over 3D detection methods. These methods have also notably been applied to RGB-D <a ref-type="bibr" anchor="ref165" id="context_ref_165_5a3">[165]</a>, which produce similar, but colored, point clouds; with their limited range and unreliability outdoors, RGB-D have not been used for ADS applications. A 3D representation of point data, through a 3D occupancy grid called voxel grids, was first applied for object detection in RGB-D data <a ref-type="bibr" anchor="ref165" id="context_ref_165_5a3">[165]</a>. Shortly thereafter, a similar approach was used on point clouds created by lidars <a ref-type="bibr" anchor="ref166" id="context_ref_166_5a3">[166]</a>. Inspired by image-based methods, 3D CNNs are used, despite being computationally very expensive.</p><p>The first convincing results for point cloud-only 3D bounding box estimation were produced by VoxelNet <a ref-type="bibr" anchor="ref167" id="context_ref_167_5a3">[167]</a>. Instead of hand-crafting input features computed during the discretization process, VoxelNet learned an encoding from raw point cloud data to voxel grid. Their voxel feature encoder (VFE) uses a fully connected neural network to convert the variable number of points in each occupied voxel to a feature vector of fixed size. The voxel grid encoded with feature vectors was then used as input to an aforementioned RPN for multi-class object detection. This work was then improved both in terms of accuracy and computational efficiency by SECOND <a ref-type="bibr" anchor="ref140" id="context_ref_140_5a3">[140]</a> by exploiting the natural sparsity of lidar data. We employed SECOND and a sample result is shown in <a ref-type="fig" anchor="fig8" class="fulltext-link">Figure 8</a>. Several algorithms have been produced recently, with accuracy constantly improving as shown in <a ref-type="table" anchor="table7" class="fulltext-link">Table 7</a>, yet the computational complexity of 3D convolutions remains an issue for real-time use.</p><div class="figure figure-full table" id="table7"><div class="figcaption"><b class="title">TABLE 7 </b>
Average Precision (AP) in % on the KITTI 3D Object Detection Test Set 
<span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-15" style="width: 2.225em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.749em; height: 0px; font-size: 126%;"><span style="position: absolute; clip: rect(1.378em, 1001.75em, 2.384em, -999.997em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-16"><span class="mi" id="MathJax-Span-17" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.056em;"></span></span><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-19" style="font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.063em; border-left: 0px solid; width: 0px; height: 1.07em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-3">Car</script>
 Class, Ordered Based on 
<span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-20" style="width: 5.505em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.341em; height: 0px; font-size: 126%;"><span style="position: absolute; clip: rect(1.378em, 1004.29em, 2.384em, -999.997em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-21"><span class="mi" id="MathJax-Span-22" style="font-family: MathJax_Math-italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.108em;"></span></span><span class="mi" id="MathJax-Span-23" style="font-family: MathJax_Math-italic;">o</span><span class="mi" id="MathJax-Span-24" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-26" style="font-family: MathJax_Math-italic;">r</span><span class="mi" id="MathJax-Span-27" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-28" style="font-family: MathJax_Math-italic;">t</span><span class="mi" id="MathJax-Span-29" style="font-family: MathJax_Math-italic;">e</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.063em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-4">Moderate</script>
 Category Accuracy. These Algorithms Only Use Pointcloud Data</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t7-2983149-large.gif"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t7-2983149-small.gif" data-alt="Table 7- 
Average Precision (AP) in % on the KITTI 3D Object Detection Test Set 
$Car$
 Class, Ordered Based on 
$Moderate$
 Category Accuracy. These Algorithms Only Use Pointcloud Data"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><p>Another option for lidar-based perception is 2D projection of point cloud data. There are two main representations of point cloud data in 2D, the first being a so-called depth image shown in <a ref-type="fig" anchor="fig10" class="fulltext-link">Figure 10</a>, largely inspired by camera-based methods that perform 3D object detection through depth estimation <a ref-type="bibr" anchor="ref168" id="context_ref_168_5a3">[168]</a> and methods that operate on RGB-D data <a ref-type="bibr" anchor="ref169" id="context_ref_169_5a3">[169]</a>. The VeloFCN network <a ref-type="bibr" anchor="ref170" id="context_ref_170_5a3">[170]</a> proposed to use single-channel depth image as input to a shallow, single-stage convolutional neural network which produced 3D vehicle proposals, with many other algorithms adopting this approach. Another use of depth image was shown for semantic classification of lidar points <a ref-type="bibr" anchor="ref171" id="context_ref_171_5a3">[171]</a>.
</p><div class="figure figure-full" id="fig10"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts10-2983149-large.gif" data-fig-id="fig10"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts10-2983149-small.gif" data-alt="FIGURE 10. - A depth image produced from synthetic lidar data, generated in the CARLA [164] simulator."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 10. </b><fig><p>A depth image produced from synthetic lidar data, generated in the CARLA <a ref-type="bibr" anchor="ref164" id="context_ref_164_5a3">[164]</a> simulator.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>The other 2D projection that has seen increasing popularity, in part due to the new KITTI benchmark, is projection to bird’s eye view (BV) image. This is a top-view image of point clouds as shown in <a ref-type="fig" anchor="fig11" class="fulltext-link">Figure 11</a>. Bird’s eye view images discretize space purely in 2D, so lidar points which vary in height alone occlude each other. The MV3D algorithm <a ref-type="bibr" anchor="ref172" id="context_ref_172_5a3">[172]</a> used camera images, depth images, as well as multi-channel BV images; each channel corresponding to a different range of heights, so as to minimize these occlusions. Several other works have reused camera-based algorithms and trained efficient networks for 3D object detection on 2D BV images <a ref-type="bibr" anchor="ref173" id="context_ref_173_5a3">[173]</a>–<a ref-type="bibr" anchor="ref174" id="context_ref_174_5a3">[174]</a><a ref-type="bibr" anchor="ref175" id="context_ref_175_5a3">[175]</a><a ref-type="bibr" anchor="ref176" id="context_ref_176_5a3">[176]</a>. State-of-the-art algorithms are currently being evaluated on the KITTI dataset <a ref-type="bibr" anchor="ref177" id="context_ref_177_5a3">[177]</a> and nuScenes dataset <a ref-type="bibr" anchor="ref178" id="context_ref_178_5a3">[178]</a> as they offer labeled 3D scenes. <a ref-type="table" anchor="table7" class="fulltext-link">Table 7</a> shows the leading methods on the KITTI benchmark, alongside detection times. 2D methods are far less computationally expensive, but recent methods that take point sparsity into account <a ref-type="bibr" anchor="ref140" id="context_ref_140_5a3">[140]</a> are real-time viable and rapidly approaching the accuracy necessary for integration in ADSs.
</p><div class="figure figure-full" id="fig11"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts11-2983149-large.gif" data-fig-id="fig11"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts11-2983149-small.gif" data-alt="FIGURE 11. - Bird’s eye view perspective of 3D lidar data, a sample from the KITTI dataset [177]."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 11. </b><fig><p>Bird’s eye view perspective of 3D lidar data, a sample from the KITTI dataset <a ref-type="bibr" anchor="ref177" id="context_ref_177_5a3">[177]</a>.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p><i>Radar:</i> Radar sensors have already been used for various perception applications, in various types of vehicles, with different models operating at complementary ranges. While not as accurate as the lidar, it can detect at object at high range and estimate their velocity <a ref-type="bibr" anchor="ref113" id="context_ref_113_5a3">[113]</a>. The lack of precision for estimating shape of objects is a major drawback when it is used in perception systems <a ref-type="bibr" anchor="ref51" id="context_ref_51_5a3">[51]</a>, the resolution is simply too low. As such, it can be used for range estimation to large objects like vehicles, but it is challenging for pedestrians or static objects. Another issue is the very limited field of view of most radars, forcing a complicated array of radar sensors to cover the full field of view. Nevertheless, radar have seen widespread use as an ADAS component, for applications including proximity warning and adaptive cruise control <a ref-type="bibr" anchor="ref146" id="context_ref_146_5a3">[146]</a>. While radar and lidar are often seen as competing sensing modalities, they will likely be used in tandem in fully automated driving systems. Radars are very long range, have low cost and are robust to poor weather, while lidar offer precise object localization capabilities, as discussed in <a ref-type="sec" anchor="sec4" class="fulltext-link">Section IV</a>.</p><p>Another similar sensor to the radar are sonar devices, though their extremely short range of <inline-formula id=""><tex-math notation="LaTeX"><span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-30" style="width: 2.755em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.455em; height: 0px; font-size: 111%;"><span style="position: absolute; clip: rect(1.354em, 1002.46em, 2.355em, -999.997em); top: -2.15em; left: 0em;"><span class="mrow" id="MathJax-Span-31"><span class="mo" id="MathJax-Span-32" style="font-family: MathJax_Main;">&lt;</span><span class="mn" id="MathJax-Span-33" style="font-family: MathJax_Main; padding-left: 0.303em;">2</span><span class="mi" id="MathJax-Span-34" style="font-family: MathJax_Math-italic;">m</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.108em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-5">< 2m</script>
</tex-math></inline-formula> and poor angular resolution makes their use limited to very near obstacle detection <a ref-type="bibr" anchor="ref146" id="context_ref_146_5a3">[146]</a>.</p></div></div><div class="section_2" id="sec5b"><h3>B. Object Tracking</h3><p>Object tracking is also often referred to as multiple object tracking (MOT) <a ref-type="bibr" anchor="ref183" id="context_ref_183_5b">[183]</a> and detection and tracking of multiple objects (DATMO) <a ref-type="bibr" anchor="ref184" id="context_ref_184_5b">[184]</a>. For fully automated driving in complex and high speed scenarios, estimating location alone is insufficient. It is necessary to estimate dynamic objects’ heading and velocity so that a motion model can be applied to track the object over time and predict future trajectory to avoid collisions. These trajectories must be estimated in the vehicle frame to be used by planning, so range information must be obtained through multiple camera systems, lidars or radar sensors. 3D lidars are often used for their precise range information and large field of view, allowing tracking over longer periods of time. To better cope with the limitations and uncertainties of different sensing modalities, a sensor fusion strategy is often use for tracking <a ref-type="bibr" anchor="ref47" id="context_ref_47_5b">[47]</a>.</p><p>Commonly used object trackers rely on simple data association techniques followed by traditional filtering methods. When objects are tracked in 3D space at high frame rate, nearest neighbor methods are often sufficient for establishing associations between objects. Image-based methods, however, need to establish some appearance model, which may consider the use of color histograms, gradients and other features such as KLT to evaluate the similarity <a ref-type="bibr" anchor="ref185" id="context_ref_185_5b">[185]</a>. Point cloud based methods may also use similarity metrics such as point density and Hausdorff distance <a ref-type="bibr" anchor="ref163" id="context_ref_163_5b">[163]</a>, <a ref-type="bibr" anchor="ref186" id="context_ref_186_5b">[186]</a>. Since association errors are always a possibility, multiple hypothesis tracking algorithms <a ref-type="bibr" anchor="ref187" id="context_ref_187_5b">[187]</a> are often employed, which ensures tracking algorithms can recover from poor data association at any single time step. Using occupancy maps as a frame for all sensors to contribute to and then doing data association in that frame is common, especially when using multiple sensors <a ref-type="bibr" anchor="ref188" id="context_ref_188_5b">[188]</a>. To obtain smooth dynamics, the detection results are filtered by traditional Bayes filters. Kalman filtering is sufficient for simple linear models, while the extended and and unscented Kalman filters <a ref-type="bibr" anchor="ref189" id="context_ref_189_5b">[189]</a> are used to handle nonlinear dynamic models <a ref-type="bibr" anchor="ref190" id="context_ref_190_5b">[190]</a>. We implemented a basic particle filter based object-tracking algorithm, and an example of tracked pedestrians in contrasting camera and 3D lidar perspective is shown in <a ref-type="fig" anchor="fig12" class="fulltext-link">Figure 12</a>.
</p><div class="figure figure-full" id="fig12"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts12-2983149-large.gif" data-fig-id="fig12"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts12-2983149-small.gif" data-alt="FIGURE 12. - A scene with several tracked pedestrians and cyclist with a basic particle filter on an urban road intersection. Past trajectories are shown in white with current heading and speed shown by the direction and magnitude of the arrow, sample collected by our data collection platform."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 12. </b><fig><p>A scene with several tracked pedestrians and cyclist with a basic particle filter on an urban road intersection. Past trajectories are shown in white with current heading and speed shown by the direction and magnitude of the arrow, sample collected by our data collection platform.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>Physical models for the object being tracked are also often used for more robust tracking. In that case, non-parametric methods such as particle filters are used, and physical parameters such as the size of the object are tracked alongside dynamics <a ref-type="bibr" anchor="ref191" id="context_ref_191_5b">[191]</a>. More involved filtering methods such as Rao-Blackwellized particle filters have also been used to keep track of both dynamic variables and vehicle geometry variables for an L-shape vehicle model <a ref-type="bibr" anchor="ref192" id="context_ref_192_5b">[192]</a>. Various models have been proposed for vehicles and pedestrians, while some models generalize to any dynamic object <a ref-type="bibr" anchor="ref193" id="context_ref_193_5b">[193]</a>.</p><p>Finally, deep learning has also been applied to the problem of tracking, particularly for images. Tracking in monocular images was achieved in real-time through a CNN-based method <a ref-type="bibr" anchor="ref194" id="context_ref_194_5b">[194]</a>, <a ref-type="bibr" anchor="ref195" id="context_ref_195_5b">[195]</a>. Multi-task network which estimate object dynamics are also emerging <a ref-type="bibr" anchor="ref196" id="context_ref_196_5b">[196]</a> which further suggests that generalized networks tackling multiple perception tasks may be the future of ADS perception.</p></div><div class="section_2" id="sec5c"><h3>C. Road and Lane Detection</h3><p>Bounding box estimation methods previously covered are useful for defining some objects of interest but are inadequate for continuous surfaces like roads. Determining the drivable surface is critical for ADSs and has been specifically researched as a subset of the detection problem. While drivable surfaces can be determined through semantic segmentation, automated vehicles need to understand road semantics to properly negotiate the road. An understanding of lanes, and how they are connected through merges and intersections remains a challenge from the perspective of perception. In this section, we provide an overview of current methods used for road and lane detection, and refer the reader to in-depth surveys of traditional methods <a ref-type="bibr" anchor="ref198" id="context_ref_198_5c">[198]</a> and the state-of-the-art methods <a ref-type="bibr" anchor="ref199" id="context_ref_199_5c">[199]</a>, <a ref-type="bibr" anchor="ref200" id="context_ref_200_5c">[200]</a>.</p><p>This problem is usually subdivided in several tasks, each unlocking some level of automation. The simplest is determining the drivable area from the perspective of the ego-vehicle. The road can then be divided into lanes, and the vehicles’ host lane can be determined. Host lane estimation over a reasonable distance allows ADAS technology such as lane departure warning, lane keeping and adaptive cruise control <a ref-type="bibr" anchor="ref198" id="context_ref_198_5c">[198]</a>, <a ref-type="bibr" anchor="ref201" id="context_ref_201_5c">[201]</a>. Even more challenging is determining other lanes and their direction <a ref-type="bibr" anchor="ref202" id="context_ref_202_5c">[202]</a>, and finally understanding complex semantics, as in their current and future direction, or merging and turning lanes <a ref-type="bibr" anchor="ref47" id="context_ref_47_5c">[47]</a>. These ADAS or ADS technologies have different criteria both in terms of task, detection distance and reliability rates, but fully automated driving will require a complete, semantic understanding of road structures and the ability to detect several lanes at long ranges <a ref-type="bibr" anchor="ref199" id="context_ref_199_5c">[199]</a>. Annotated maps as shown in <a ref-type="fig" anchor="fig13" class="fulltext-link">Figure 13</a> are extremely useful for understanding lane semantics.
</p><div class="figure figure-full" id="fig13"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts13ab-2983149-large.gif" data-fig-id="fig13"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts13ab-2983149-small.gif" data-alt="FIGURE 13. - Annotating a 3D point cloud map with topological information. A large number of annotators were employed to build the map shown on the right-hand side. The point-cloud and annotated maps are available on [197]."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 13. </b><fig><p>Annotating a 3D point cloud map with topological information. A large number of annotators were employed to build the map shown on the right-hand side. The point-cloud and annotated maps are available on <a ref-type="bibr" anchor="ref197" id="context_ref_197_5c">[197]</a>.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>Current methods on road understanding typically first rely on exteroceptive data preprocessing. When cameras are used, this usually means performing image color corrections to normalize lighting conditions <a ref-type="bibr" anchor="ref203" id="context_ref_203_5c">[203]</a>. For lidar, several filtering methods can be used to reduce clutter in the data such as ground extraction <a ref-type="bibr" anchor="ref162" id="context_ref_162_5c">[162]</a> or map-based filtering <a ref-type="bibr" anchor="ref163" id="context_ref_163_5c">[163]</a>. For any sensing modality, identifying dynamic objects which conflicts with the static road scene is an important pre-processing step. Then, road and lane feature extraction is performed on the corrected data. Color statistics and intensity information <a ref-type="bibr" anchor="ref204" id="context_ref_204_5c">[204]</a>, gradient information <a ref-type="bibr" anchor="ref205" id="context_ref_205_5c">[205]</a>, and various other filters have been used to detect lane markings. Similar methods have been used for road estimation, where the usual uniformity of roads and elevation gap at the edge allows for region growing methods to be applied <a ref-type="bibr" anchor="ref206" id="context_ref_206_5c">[206]</a>. Stereo camera systems <a ref-type="bibr" anchor="ref207" id="context_ref_207_5c">[207]</a>, as well as 3D lidars <a ref-type="bibr" anchor="ref204" id="context_ref_204_5c">[204]</a>, have been used determine the 3D structure of roads directly. More recently, machine learning-based methods which either fuse maps with vision <a ref-type="bibr" anchor="ref200" id="context_ref_200_5c">[200]</a> or use fully appearance-based segmentation <a ref-type="bibr" anchor="ref208" id="context_ref_208_5c">[208]</a> have been used.</p><p>Once surfaces are estimated, model fitting is used to establish the continuity of the road and lanes. Geometric fitting through parametric models such as lines <a ref-type="bibr" anchor="ref209" id="context_ref_209_5c">[209]</a> and splines <a ref-type="bibr" anchor="ref204" id="context_ref_204_5c">[204]</a> have been used, as well as non-parametric continuous models <a ref-type="bibr" anchor="ref210" id="context_ref_210_5c">[210]</a>. Models that assume parallel lanes have been used <a ref-type="bibr" anchor="ref201" id="context_ref_201_5c">[201]</a>, and more recently models integrating topological elements such as lane splitting and merging were proposed <a ref-type="bibr" anchor="ref204" id="context_ref_204_5c">[204]</a>.</p><p>Temporal integration completes the road and lane segmentation pipeline. Here, vehicle dynamics are used in combination with a road tracking system to achieve smooth results. Dynamic information can also be used alongside Kalman filtering <a ref-type="bibr" anchor="ref201" id="context_ref_201_5c">[201]</a> or particle filtering <a ref-type="bibr" anchor="ref207" id="context_ref_207_5c">[207]</a> to achieve smoother results.</p><p>Road and lane estimation is a well-researched field and many methods have already been integrated successfully for lane keeping assistance systems. However, most methods remain riddled with assumptions and limitations, and truly general systems which can handle complex road topologies have yet to be developed. Through standardized road maps which encode topology and emerging machine learning-based road and lane classification methods, robust systems for driving automation are slowly taking shape.</p></div></div>
<div class="section" id="sec6"><div class="header article-hdr"><div class="kicker">
		                        SECTION VI.</div><h2>Assessment</h2></div><p>A robust ADS should constantly evaluate the overall risk level of the situation and predict the intentions of surrounding human drivers and pedestrians. A lack of acute assessment mechanism can lead to accidents. This section discusses assessment under three subcategories: overall risk and uncertainty assessment, human driving behavior assessment, and driving style recognition.</p><div class="section_2" id="sec6a"><h3>A. Risk and Uncertainty Assessment</h3><p>Overall assessment can be summarized as quantifying the uncertainties and the risk level of the driving scene. It is a promising methodology that can increase the safety of ADS pipelines <a ref-type="bibr" anchor="ref31" id="context_ref_31_6a">[31]</a>.</p><p>Using Bayesian methods to quantify and measure uncertainties of deep neural networks was proposed in <a ref-type="bibr" anchor="ref212" id="context_ref_212_6a">[212]</a>. A Bayesian deep learning architecture was designed for propagating uncertainty throughout an ADS pipeline, and the advantage of it over conventional approaches was shown in a hypothetical scenario <a ref-type="bibr" anchor="ref31" id="context_ref_31_6a">[31]</a>. In summary, each module conveys and accepts probability distributions instead of exact outcomes throughout the pipeline, which increases the overall robustness of the system.</p><p>An alternative approach is to assess the overall risk level of the driving scene separately, i.e outside the pipeline. Sensory inputs were fed into a risk inference framework in <a ref-type="bibr" anchor="ref83" id="context_ref_83_6a">[83]</a>, <a ref-type="bibr" anchor="ref213" id="context_ref_213_6a">[213]</a> to detect unsafe lane change events using Hidden Markov Models (HMMs) and language models. Recently, a deep spatiotemporal network that infers the overall risk level of a driving scene was introduced in <a ref-type="bibr" anchor="ref211" id="context_ref_211_6a">[211]</a>. Implementation of this method is available open-source.<a ref-type="fn" anchor="fn1" class="footnote-link"><sup>1</sup></a> We employed this method to assess the risk level of a lane change as shown in <a ref-type="fig" anchor="fig14" class="fulltext-link">Figure 14</a>.
</p><div class="figure figure-full" id="fig14"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts14-2983149-large.gif" data-fig-id="fig14"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts14-2983149-small.gif" data-alt="FIGURE 14. - Assessing the overall risk level of driving scenes. We employed an open-source1 deep spatiotemporal video-based risk detection framework [211] to assess the image sequences shown in this figure."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 14. </b><fig><p>Assessing the overall risk level of driving scenes. We employed an open-source<a ref-type="fn" anchor="fn1" class="footnote-link"><sup>1</sup></a> deep spatiotemporal video-based risk detection framework <a ref-type="bibr" anchor="ref211" id="context_ref_211_6a">[211]</a> to assess the image sequences shown in this figure.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p></div><div class="section_2" id="sec6b"><h3>B. Surrounding Driving Behavior Assessment</h3><p>Understanding surrounding human driver intention is most relevant to medium to long term prediction and decision making. In order to increase the prediction horizon of surrounding object behavior, human traits should be considered and incorporated into the prediction and evaluation steps. Understanding surrounding driver intention from the perspective of an ADS is not a common practice in the field, as such, state-of-the-art is not established yet.</p><p>In <a ref-type="bibr" anchor="ref214" id="context_ref_214_6b">[214]</a>, a target vehicle’s future behavior was predicted with a hidden Markov model (HMM) and the prediction time horizon was extended 56% by learning human driving traits. The proposed system tagged observations with predefined maneuvers. Then, the features of each type were learned in a data-centric manner with HMMs. Another learning based approach was proposed in <a ref-type="bibr" anchor="ref215" id="context_ref_215_6b">[215]</a>, where a Bayesian network classifier was used to predict maneuvers of individual drivers on highways. A framework for long term driver behavior prediction using a combination of a hybrid state system and HMM was introduced in <a ref-type="bibr" anchor="ref216" id="context_ref_216_6b">[216]</a>. Surrounding vehicle information was integrated with ego-behavior through a symbolization framework in <a ref-type="bibr" anchor="ref83" id="context_ref_83_6b">[83]</a>, <a ref-type="bibr" anchor="ref213" id="context_ref_213_6b">[213]</a>. Detecting dangerous cut in maneuvers was achieved with an HMM framework that was trained on safe and dangerous data in <a ref-type="bibr" anchor="ref217" id="context_ref_217_6b">[217]</a>. Lane change events were predicted 1.3 seconds in advance with support vector machines (SVM) and Bayesian filters <a ref-type="bibr" anchor="ref218" id="context_ref_218_6b">[218]</a>.</p><p>The main challenges are the short observation window for understanding the intention of humans and real-time high-frequency computation requirements. ADSs can typically only observe a surrounding vehicle only for seconds. Complicated driving behavior models that require longer observation periods cannot be utilized under these circumstances.</p></div><div class="section_2" id="sec6c"><h3>C. Driving Style Recognition</h3><p>In 2016, Google’s self-driving car collided with an oncoming bus <a ref-type="bibr" anchor="ref8" id="context_ref_8_6c">[8]</a> during a lane change where it assumed that the bus driver was going to yield. However, the bus driver accelerated instead. This accident may have been prevented if the ADS understood the bus driver’s individual, unique driving style and predicted his behavior.</p><p>Driving style is a broad term without an established common definition. Furthermore, recognizing the surrounding human driving styles is a severely understudied topic. However, thorough reviews of driving style categorization of <i>human-driven</i> ego vehicles can be found in <a ref-type="bibr" anchor="ref220" id="context_ref_220_6c">[220]</a> and in <a ref-type="bibr" anchor="ref221" id="context_ref_221_6c">[221]</a>. Readers are referred to these papers for a complete review. The remainder of this subsection gives a brief overview of <i>human-driven</i> ego vehicle-based driving style recognition.</p><p>Typically, driving style is defined with respect to either aggressiveness <a ref-type="bibr" anchor="ref222" id="context_ref_222_6c">[222]</a>–<a ref-type="bibr" anchor="ref223" id="context_ref_223_6c">[223]</a><a ref-type="bibr" anchor="ref224" id="context_ref_224_6c">[224]</a><a ref-type="bibr" anchor="ref225" id="context_ref_225_6c">[225]</a><a ref-type="bibr" anchor="ref226" id="context_ref_226_6c">[226]</a> or fuel consumption <a ref-type="bibr" anchor="ref227" id="context_ref_227_6c">[227]</a>–<a ref-type="bibr" anchor="ref228" id="context_ref_228_6c">[228]</a><a ref-type="bibr" anchor="ref229" id="context_ref_229_6c">[229]</a><a ref-type="bibr" anchor="ref230" id="context_ref_230_6c">[230]</a><a ref-type="bibr" anchor="ref231" id="context_ref_231_6c">[231]</a>. For example, <a ref-type="bibr" anchor="ref232" id="context_ref_232_6c">[232]</a> introduced a rule-based model that classified driving styles with respect to jerk. This model decides whether a maneuver is aggressive or calm by a set of rules and jerk thresholds. Drivers were categorized with respect to their average speed in <a ref-type="bibr" anchor="ref233" id="context_ref_233_6c">[233]</a>. In conventional methods, total number and meaning of driving style classes are predefined beforehand. The vast majority of driving style recognition literature uses two <a ref-type="bibr" anchor="ref83" id="context_ref_83_6c">[83]</a>, <a ref-type="bibr" anchor="ref213" id="context_ref_213_6c">[213]</a>, <a ref-type="bibr" anchor="ref222" id="context_ref_222_6c">[222]</a>, <a ref-type="bibr" anchor="ref223" id="context_ref_223_6c">[223]</a>, <a ref-type="bibr" anchor="ref227" id="context_ref_227_6c">[227]</a> or three <a ref-type="bibr" anchor="ref234" id="context_ref_234_6c">[234]</a>–<a ref-type="bibr" anchor="ref235" id="context_ref_235_6c">[235]</a><a ref-type="bibr" anchor="ref236" id="context_ref_236_6c">[236]</a> classes. Representing driving style in a continuous domain is uncommon, but there are some studies. In <a ref-type="bibr" anchor="ref237" id="context_ref_237_6c">[237]</a>, driving style was depicted as a continuous value between −1 and +1, which stands for mild and active respectively. Details of classification methods are given in <a ref-type="table" anchor="table8" class="fulltext-link">Table 8</a>.</p><div class="figure figure-full table" id="table8"><div class="figcaption"><b class="title">TABLE 8 </b>
Driving Style Categorization of Human-Driven Ego Vehicles</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t8-2983149-large.gif"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t8-2983149-small.gif" data-alt="Table 8- 
Driving Style Categorization of Human-Driven Ego Vehicles"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><p>More recently, machine learning based approaches have been utilized for driving style recognition. Principal component analysis was used and five distinct driving classes were detected in an unsupervised manner in <a ref-type="bibr" anchor="ref238" id="context_ref_238_6c">[238]</a> and a GMM based driver model was used to identify individual drivers with success in <a ref-type="bibr" anchor="ref241" id="context_ref_241_6c">[241]</a>. Car-following and pedal operation behavior was investigated separately in the latter study. Another GMM based driving style recognition model was proposed for electric vehicle range prediction in <a ref-type="bibr" anchor="ref242" id="context_ref_242_6c">[242]</a>. In <a ref-type="bibr" anchor="ref222" id="context_ref_222_6c">[222]</a>, aggressive event detection with dynamic time warping was presented where the authors reported a high success score. Bayesian approaches were utilized in <a ref-type="bibr" anchor="ref243" id="context_ref_243_6c">[243]</a> for modeling driving style on roundabouts and in <a ref-type="bibr" anchor="ref244" id="context_ref_244_6c">[244]</a> to asses critical braking situations. Bag-of-words and K-means clustering was used to represent individual driving features in <a ref-type="bibr" anchor="ref245" id="context_ref_245_6c">[245]</a>. A stacked autoencoder was used to extract unique driving signatures from different drivers, and then macro driving style centroids were found with clustering <a ref-type="bibr" anchor="ref240" id="context_ref_240_6c">[240]</a>. Another autoencoder network was used to extract road-type specific driving features <a ref-type="bibr" anchor="ref246" id="context_ref_246_6c">[246]</a>. Similarly, driving behavior was encoded in a 3-channel RGB space with a deep sparse autoencoder to visualize individual driving styles <a ref-type="bibr" anchor="ref247" id="context_ref_247_6c">[247]</a>.</p><p>A successful integration of driving style recognition into a real world ADS pipeline is not reported yet. However, these studies are promising and point to a possible new direction in ADS development.</p></div></div>
<div class="section" id="sec7"><div class="header article-hdr"><div class="kicker">
		                        SECTION VII.</div><h2>Planning and Decision Making</h2></div><p>Planning can be divided into two sub-tasks: global route planning and local path planning. <a ref-type="fig" anchor="fig15" class="fulltext-link">Figure 15</a> illustrates a typical planning approach in detail.
</p><div class="figure figure-full" id="fig15"><!--
          Workaround for combined images.Eg.- 1000116 Fig. 5
        --><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts15-2983149-large.gif" data-fig-id="fig15"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts15-2983149-small.gif" data-alt="FIGURE 15. - Global plan and the local paths. The annotated vector map shown in Figure 13 was utilized by the planner. We employed OpenPlanner [219], which is a graph-based planner, to illustrate a typical planning approach."><div class="zoom" title="View Larger Image"></div></a></div><div class="figcaption"><b class="title">FIGURE 15. </b><fig><p>Global plan and the local paths. The annotated vector map shown in <a ref-type="fig" anchor="fig13" class="fulltext-link">Figure 13</a> was utilized by the planner. We employed OpenPlanner <a ref-type="bibr" anchor="ref219" id="context_ref_219_7">[219]</a>, which is a graph-based planner, to illustrate a typical planning approach.</p></fig></div><p class="links"><a href="https://ieeexplore.ieee.org/document/9046805/all-figures" class="all">Show All</a></p></div><p></p><p>The remainder of this section gives a brief overview of the subject. For more information studies such as <a ref-type="bibr" anchor="ref18" id="context_ref_18_7">[18]</a>, <a ref-type="bibr" anchor="ref23" id="context_ref_23_7">[23]</a>, <a ref-type="bibr" anchor="ref248" id="context_ref_248_7">[248]</a> can be referred.</p><div class="section_2" id="sec7a"><h3>A. Global Planning</h3><p>The global planner is responsible for finding the route on the road network from origin to the final destination. The user usually defines the final destination. Global navigation is a well-studied subject, and high performance has become an industry standard for more than a decade. Almost all modern production cars are equipped with navigation systems that utilize GPS and offline maps to plan a global route.</p><p>Route planning is formulated as finding the point-to-point shortest path in a directed graph, and conventional methods are examined under four categories in <a ref-type="bibr" anchor="ref248" id="context_ref_248_7a">[248]</a>. These are; goal-directed, separator-based, hierarchical and bounded-hop techniques. A* search <a ref-type="bibr" anchor="ref249" id="context_ref_249_7a">[249]</a> is a standard goal-directed path planning algorithm and used extensively in various fields for almost 50 years.</p><p>The main idea of separator-based techniques is to remove a subset of vertices <a ref-type="bibr" anchor="ref250" id="context_ref_250_7a">[250]</a> or arcs from the graph and compute an overlay graph over it. Using the overlayed graph to calculate the shortest path results in faster queries.</p><p>Hierarchical techniques take advantage of the road hierarchy. For example, the road hierarchy in the US can be listed from top to bottom as freeways, arterials, collectors and local roads respectively. For a route query, the importance of hierarchy increases as the distance between origin and destination gets longer. The shortest path may not be the fastest nor the most desirable route anymore. Getting away from the destination thus making the route a bit longer to take the closest highway ramp may result in faster travel time in comparison to following the shortest path of local roads. Contraction Hierarchies (CH) method was proposed in <a ref-type="bibr" anchor="ref251" id="context_ref_251_7a">[251]</a> for exploiting road hierarchy.</p><p>Precomputing distances between selected vertexes and utilizing them on the query time is the basis of bounded-hop techniques. Precomputed shortcuts can be utilized partly or exclusively for navigation. However, the naive approach of precomputing all possible routes from every pair of vertices is impractical in most cases with large networks. One possible solution to this is to use hub labeling (HL) <a ref-type="bibr" anchor="ref252" id="context_ref_252_7a">[252]</a>. This approach requires preprocessing also. A label associated with a vertex consists of nearby <i>hub</i> vertices and the distance to them. These labels satisfy the condition that at least one shared hub vertex must exist between the labels of any given two vertices. HL is the fastest query time algorithm for route planning <a ref-type="bibr" anchor="ref248" id="context_ref_248_7a">[248]</a>, in the expense of high storage usage.</p><p>A combination of the above algorithms are popular in state-of-the-art systems. For example, <a ref-type="bibr" anchor="ref253" id="context_ref_253_7a">[253]</a> combined a separator with a bounded-hop method and created the Transit Node Routing with Arc Flags (TNR + AF) algorithm. Modern route planners can make a query in milliseconds.</p></div><div class="section_2" id="sec7b"><h3>B. Local Planning</h3><p>The objective of the local planner is to execute a global plan without <i>failing</i>. In other words, in order to complete its trip, the ADS must find trajectories to avoid obstacles and satisfy optimization criteria in the configuration space (C-space), given a starting and destination point. A detailed local planning review is presented in <a ref-type="bibr" anchor="ref19" id="context_ref_19_7b">[19]</a> where the taxonomy of motion planning was divided into four groups; graph-based planners, sampling-based planners, interpolating curve planners and numerical optimization approaches. After a summary of these conventional planners, the emerging deep learning-based planners are introduced at the end of this section. <a ref-type="table" anchor="table9" class="fulltext-link">Table 9</a> gives a brief summary of local planning methods.</p><div class="figure figure-full table" id="table9"><div class="figcaption"><b class="title">TABLE 9 </b>
Local Planning Techniques</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t9-2983149-large.gif"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t9-2983149-small.gif" data-alt="Table 9- 
Local Planning Techniques"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p><p>Graph-based local planners use the same techniques as graph-based global planners such as Dijkstra <a ref-type="bibr" anchor="ref254" id="context_ref_254_7b">[254]</a> and A* <a ref-type="bibr" anchor="ref249" id="context_ref_249_7b">[249]</a>, which output discrete paths rather than continuous ones. This can lead to jerky trajectories <a ref-type="bibr" anchor="ref19" id="context_ref_19_7b">[19]</a>. A more advanced graph-based planner is the state lattice algorithm. As all graph-based methods, state lattice discretizes the decision space. High dimensional lattice nodes, which typically encode 2D position, heading and curvature <a ref-type="bibr" anchor="ref255" id="context_ref_255_7b">[255]</a>, are used to create a grid first. Then, the connections between the nodes are precomputed with an inverse path generator to build the state lattice. During the planning phase, a cost function, which usually considers proximity to obstacles and deviation from the goal, is utilized for finding the best path with the precomputed path primitives. State lattices can handle high dimensions and is good for local planning in dynamical environments, however, the computational load is high and the discretization resolution limits the planners’ capacity <a ref-type="bibr" anchor="ref19" id="context_ref_19_7b">[19]</a>.</p><p>A detailed overview of Sampling Based Planning (SBP) methods can be found in <a ref-type="bibr" anchor="ref267" id="context_ref_267_7b">[267]</a>. In summary, SBP tries to build the connectivity of the C-space by randomly sampling paths in it. Randomized Potential Planner (RPP) <a ref-type="bibr" anchor="ref256" id="context_ref_256_7b">[256]</a> is one of the earliest SBP approaches, where random walks are generated for escaping local minimums. Probabilistic roadmap method (PRM) <a ref-type="bibr" anchor="ref259" id="context_ref_259_7b">[259]</a> and rapidly-exploring random tree (RRT) <a ref-type="bibr" anchor="ref257" id="context_ref_257_7b">[257]</a> are the most commonly used SBP algorithms. PRM first samples the C-space during its learning phase and then makes a query with the predefined origin and destination points on the roadmap. RRT, on the other hand, is a single query planner. The path between start and goal configuration is incrementally built with random tree-like branches. RRT is faster than PRM and both are probabilistically complete <a ref-type="bibr" anchor="ref257" id="context_ref_257_7b">[257]</a>, which means a path that satisfies the given conditions will be guaranteed to be found with <i>enough</i> runtime. RRT* <a ref-type="bibr" anchor="ref258" id="context_ref_258_7b">[258]</a>, an extension of the RRT, provides more optimal paths instead of completely random ones while sacrificing computational efficiency. The main disadvantage of SBP in general is, again, the jerky trajectories <a ref-type="bibr" anchor="ref19" id="context_ref_19_7b">[19]</a>.</p><p>Interpolating curve planners fit a curve to a known set of points <a ref-type="bibr" anchor="ref19" id="context_ref_19_7b">[19]</a>, e.g. way-points generated from the global plan or a discrete set of future points from another local planner. The main obstacle avoidance strategy is to interpolate new collision-free paths that first deviate from, and then re-enter back to the initial planned trajectory. The new path is generated by fitting a curve to a new set of points: an exit point from the currently traversed trajectory, newly sampled collision free points, and a re-entry point on the initial trajectory. The resultant trajectory is smooth, however, the computational load is usually higher compared to other methods. There are various curve families that are used commonly such as clothoids <a ref-type="bibr" anchor="ref260" id="context_ref_260_7b">[260]</a>, polynomials <a ref-type="bibr" anchor="ref261" id="context_ref_261_7b">[261]</a>, Bezier curves <a ref-type="bibr" anchor="ref262" id="context_ref_262_7b">[262]</a> and splines <a ref-type="bibr" anchor="ref104" id="context_ref_104_7b">[104]</a>.</p><p>Optimization based motion planners improve the quality of already existing paths with optimization functions. A* trajectories were optimized with numeric non-linear functions in <a ref-type="bibr" anchor="ref263" id="context_ref_263_7b">[263]</a>. Potential Field Method (PFM) was improved by solving the inherent oscillation problem using Newton’s method with obtaining <inline-formula id=""><tex-math notation="LaTeX"><span class="MathJax_Preview" style="display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-35" style="width: 1.304em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.154em; height: 0px; font-size: 111%;"><span style="position: absolute; clip: rect(1.254em, 1001.15em, 2.405em, -999.997em); top: -2.1em; left: 0em;"><span class="mrow" id="MathJax-Span-36"><span class="msubsup" id="MathJax-Span-37"><span style="display: inline-block; position: relative; width: 1.154em; height: 0px;"><span style="position: absolute; clip: rect(3.156em, 1000.75em, 4.157em, -999.997em); top: -4.002em; left: 0em;"><span class="mi" id="MathJax-Span-38" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.053em;"></span></span><span style="display: inline-block; width: 0px; height: 4.007em;"></span></span><span style="position: absolute; top: -3.851em; left: 0.703em;"><span class="texatom" id="MathJax-Span-39"><span class="mrow" id="MathJax-Span-40"><span class="mn" id="MathJax-Span-41" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.007em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.105em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.058em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-6">C_{1}</script>
</tex-math></inline-formula> continuity in <a ref-type="bibr" anchor="ref264" id="context_ref_264_7b">[264]</a>.</p><p>Recently, Deep Learning (DL) and reinforcement learning based local planners started to emerge as an alternative. Fully convolutional 3D neural networks can generate future paths from sensory input such as lidar point clouds <a ref-type="bibr" anchor="ref265" id="context_ref_265_7b">[265]</a>. An interesting take on the subject is to segment image data with path proposals using a deep segmentation network <a ref-type="bibr" anchor="ref266" id="context_ref_266_7b">[266]</a>. Planning a safe path in occluded intersections was achieved in a simulation environment using deep reinforcement learning in <a ref-type="bibr" anchor="ref268" id="context_ref_268_7b">[268]</a>. The main difference between end-to-end driving and deep learning based local planners is the output: the former outputs direct vehicle control signals such as steering and pedal operation, whereas the latter generates a trajectory. This enables DL planners to be integrated into conventional pipelines <a ref-type="bibr" anchor="ref24" id="context_ref_24_7b">[24]</a>.</p><p>Deep learning based planners are promising, but they are not widely used in real-world systems yet. Lack of hard-coded safety measures, generalization issues, need for labeled data are some of the issues that need to be addressed.</p></div></div>
<div class="section" id="sec8"><div class="header article-hdr"><div class="kicker">
		                        SECTION VIII.</div><h2>Human Machine Interaction</h2></div><p>Vehicles communicate with their drivers/passengers through their HMI module. The nature of this communication greatly depends on the objective, which can be divided into two: primary driving tasks and secondary tasks. The interaction intensity of these tasks depend on the automation level. Where a manually operated, level zero conventional car requires constant user input for operation, a level five ADS may need user input only at the beginning of the trip. Furthermore, the purpose of interaction may affect intensity. A shift from executing primary driving tasks to monitoring the automation process raises new HMI design requirements.</p><p>There are several investigations such as <a ref-type="bibr" anchor="ref269" id="context_ref_269_8">[269]</a>, <a ref-type="bibr" anchor="ref270" id="context_ref_270_8">[270]</a> about automotive HMI technologies, mostly from the distraction point of view. Manual user interfaces for secondary tasks are more desired than their visual counterparts <a ref-type="bibr" anchor="ref269" id="context_ref_269_8">[269]</a>. The main reason is vision is absolutely necessary and has no alternative for primary driving tasks. Visual interface interactions require glances with durations between 0.6 and 1.6 seconds with a mean of 1.2 seconds <a ref-type="bibr" anchor="ref269" id="context_ref_269_8">[269]</a>. As such, secondary task interfaces that require vision is distracting and detrimental for driving.</p><p>Auditory User Interfaces (AUI) are good alternatives to visually taxing HMI designs. AUIs are omni-directional: even if the user is not attending, the auditory cues are hard to miss <a ref-type="bibr" anchor="ref271" id="context_ref_271_8">[271]</a>. The main challenge of audio interaction is automatic speech recognition (ASR). ASR is a very mature field. However, in vehicle domain there are additional challenges; low performance caused by uncontrollable cabin conditions such as wind and road noise <a ref-type="bibr" anchor="ref272" id="context_ref_272_8">[272]</a>. Beyond simple voice commands, conversational natural language interaction with an ADS is still an unrealized concept with many unsolved challenges <a ref-type="bibr" anchor="ref273" id="context_ref_273_8">[273]</a>.</p><p>The biggest HMI challenge is at automation level three and four. The user and the ADS need to have a mutual understanding, otherwise, they will not be able to grasp each other’s intentions <a ref-type="bibr" anchor="ref270" id="context_ref_270_8">[270]</a>. The transition from manual to automated driving and vice versa is prone to fail in the state-of-the-art. Recent research showed that drivers exhibit low cognitive load when monitoring automated driving compared to doing a secondary task <a ref-type="bibr" anchor="ref288" id="context_ref_288_8">[288]</a>. Even though some experimental systems can recognize driver-activity with a driver facing camera based on head and eye-tracking <a ref-type="bibr" anchor="ref289" id="context_ref_289_8">[289]</a>, and prepare the driver for handover with visual and auditory cues <a ref-type="bibr" anchor="ref290" id="context_ref_290_8">[290]</a> in simulation environments, a real world system with an efficient handover interaction module does not exist at the moment. This is an open problem <a ref-type="bibr" anchor="ref291" id="context_ref_291_8">[291]</a> and future research should focus on delivering better methods to inform/prepare the driver for easing the transition <a ref-type="bibr" anchor="ref41" id="context_ref_41_8">[41]</a>.</p></div>
<div class="section" id="sec9"><div class="header article-hdr"><div class="kicker">
		                        SECTION IX.</div><h2>Datasets and Available Tools</h2></div><div class="section_2" id="sec9a"><h3>A. Datasets and Benchmarks</h3><p>Datasets are crucial for researchers and developers because most of the algorithms and tools have to be tested and trained before going on road.</p><p>Typically, sensory inputs are fed into a stack of algorithms with various objectives. A common practice is to test and validate these functions separately on annotated datasets. For example, the output of cameras, 2D vision, can be fed into an object detection algorithm to detect surrounding vehicles and pedestrians. Then, this information can be used in another algorithm for planning purposes. Even though these two algorithms are connected in the stack of this example, the object detection part can be worked on and validated separately during the development process. Since computer vision is a well-studied field, there are annotated datasets for object detection and tracking specifically. The existence of these datasets increases the development process and enables interdisciplinary research teams to work with each other much more efficiently. For end-to-end driving, the dataset has to include additional ego-vehicle signals, chiefly steering and longitudinal control signals.</p><p>As learning approaches emerged, so did training datasets to support them. The PASCAL VOC dataset <a ref-type="bibr" anchor="ref292" id="context_ref_292_9a">[292]</a>, which grew from 2005 to 2012, was one of the first dataset featuring a large amount of data with relevant classes for ADS. However, the images often featured single objects, in scenes and scales that are not representative of what is encountered in driving scenarios. In 2012, the KITTI Vision Benchmark <a ref-type="bibr" anchor="ref177" id="context_ref_177_9a">[177]</a> remedied this situation by providing a relatively large amount of labeled driving scenes. It remains as one of the most widely used datasets for applications related to driving automation. Yet in terms of quantity of data and number of labeled classes, it is far inferior to generic image databases such as ImageNet <a ref-type="bibr" anchor="ref131" id="context_ref_131_9a">[131]</a> and COCO <a ref-type="bibr" anchor="ref152" id="context_ref_152_9a">[152]</a>. While no doubt useful for training, these generic image databases lack the adequate context to test the capabilities of ADS. UC Berkeley DeepDrive <a ref-type="bibr" anchor="ref275" id="context_ref_275_9a">[275]</a> is a recent dataset with annotated image data. The Oxford RobotCar <a ref-type="bibr" anchor="ref53" id="context_ref_53_9a">[53]</a> was used to collect over 1000 km of driving data with six cameras, lidar, GPS and INS in the UK but is not annotated. ApolloScape is a very recent dataset that is not fully public yet <a ref-type="bibr" anchor="ref278" id="context_ref_278_9a">[278]</a>. Cityscapes <a ref-type="bibr" anchor="ref274" id="context_ref_274_9a">[274]</a> is commonly used for computer vision algorithms as a benchmark set. Mapillary Vistas is a big image dataset with annotations <a ref-type="bibr" anchor="ref276" id="context_ref_276_9a">[276]</a>. TorontoCity benchmark <a ref-type="bibr" anchor="ref286" id="context_ref_286_9a">[286]</a> is a very detailed dataset; however it is not public yet. The nuScenes dataset is the most recent urban driving dataset with lidar and image sensors <a ref-type="bibr" anchor="ref178" id="context_ref_178_9a">[178]</a>. Comma.ai has released a part of their dataset <a ref-type="bibr" anchor="ref293" id="context_ref_293_9a">[293]</a> which includes 7.25 hours of driving. In DDD17 <a ref-type="bibr" anchor="ref89" id="context_ref_89_9a">[89]</a> around 12 hours of driving data is recorded. The LiVi-Set <a ref-type="bibr" anchor="ref281" id="context_ref_281_9a">[281]</a> is a new dataset that has lidar, image and driving behavior. CommonRoad <a ref-type="bibr" anchor="ref294" id="context_ref_294_9a">[294]</a> is a new benchmark for motion-planning.</p><p>Naturalistic driving data is another type of dataset that concentrates on the individual element of the driving: the driver. SHRP2 <a ref-type="bibr" anchor="ref283" id="context_ref_283_9a">[283]</a> includes over 3000 volunteer participants’ driving data over a 3-year collection period. Other naturalistic driving datasets are the 100-Car study <a ref-type="bibr" anchor="ref284" id="context_ref_284_9a">[284]</a>, euroFOT <a ref-type="bibr" anchor="ref285" id="context_ref_285_9a">[285]</a> and NUDrive <a ref-type="bibr" anchor="ref282" id="context_ref_282_9a">[282]</a>. <a ref-type="table" anchor="table10" class="fulltext-link">Table 10</a> shows the comparison of these datasets.</p><div class="figure figure-full table" id="table10"><div class="figcaption"><b class="title">TABLE 10 </b>
Driving Datasets</div><div class="img-wrap"><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t10-2983149-large.gif"><img class="document-ft-image load-shimmer" src="https://ieeexplore.ieee.org/document/" alt="" data-lazy="/mediastore_new/IEEE/content/media/6287639/8948470/9046805/yurts.t10-2983149-small.gif" data-alt="Table 10- 
Driving Datasets"><div class="zoom" title="View Larger Image"></div></a></div></div><p></p></div><div class="section_2" id="sec9b"><h3>B. Open-Source Frameworks and Simulators</h3><p>Open source frameworks are very useful for both researchers and the industry. These frameworks can “democratize” ADS development. Autoware <a ref-type="bibr" anchor="ref122" id="context_ref_122_9b">[122]</a>, Apollo <a ref-type="bibr" anchor="ref295" id="context_ref_295_9b">[295]</a>, Nvidia DriveWorks <a ref-type="bibr" anchor="ref296" id="context_ref_296_9b">[296]</a> and openpilot <a ref-type="bibr" anchor="ref297" id="context_ref_297_9b">[297]</a> are amongst the most used software-stacks capable of running an ADS platform in real world. We utilized Autoware <a ref-type="bibr" anchor="ref122" id="context_ref_122_9b">[122]</a> to realize core automated driving functions in this study.</p><p>Simulations also have an important place for ADS development. Since the instrumentation of an experimental vehicle still has a high cost and conducting experiments on public road networks are highly regulated, a simulation environment is beneficial for developing certain algorithms/modules before road tests. Furthermore, highly dangerous scenarios such as a collision with pedestrian can be tested in simulations with ease. CARLA <a ref-type="bibr" anchor="ref164" id="context_ref_164_9b">[164]</a> is an urban driving simulator developed for this purpose. TORCS <a ref-type="bibr" anchor="ref298" id="context_ref_298_9b">[298]</a> was developed for race track simulation. Some researchers even used computer games such as Grand Theft Auto V <a ref-type="bibr" anchor="ref299" id="context_ref_299_9b">[299]</a>. Gazebo <a ref-type="bibr" anchor="ref300" id="context_ref_300_9b">[300]</a> is a common simulation environment for robotics. For traffic simulations, SUMO <a ref-type="bibr" anchor="ref301" id="context_ref_301_9b">[301]</a> is a widely used open-source platform. <a ref-type="bibr" anchor="ref302" id="context_ref_302_9b">[302]</a> proposed different concepts of integrating real-world measurements into the simulation environment.</p></div></div>
<div class="section" id="sec10"><div class="header article-hdr"><div class="kicker">
		                        SECTION X.</div><h2>Conclusions</h2></div><p>In this survey on automated driving systems, we outlined some of the key innovations as well as existing systems. While the promise of automated driving is enticing and already marketed to consumers, this survey has shown there remains clear gaps in the research. Several architecture models have been proposed, from fully modular to completely end-to-end, each with their own shortcomings. The optimal sensing modality for localization, mapping and perception is still disagreed upon, algorithms still lack accuracy and efficiency, and the need for a proper online assessment has become apparent. Less than ideal road conditions are still an open problem, as well as dealing with intemperate weather. Vehicle-to-vehicle communication is still in its infancy, while centralized, cloud-based information management has yet to be implemented due to the complex infrastructure required. Human-machine interaction is an under-researched field with many open problems.</p><p>The development of automated driving systems relies on the advancements of both scientific disciplines and new technologies. As such, we discussed the recent research developments which are likely to have a significant impact on automated driving technology, either by overcoming the weakness of previous methods or by proposing an alternative. This survey has shown that through inter-disciplinary academic collaboration and support from industries and the general public, the remaining challenges can be addressed. With directed efforts towards ensuring robustness at all levels of automated driving systems, safe and efficient roads are just beyond the horizon.</p></div>
</div></div></response>
</div><!----><xpl-reference-pop-up _ngcontent-eae-c156="" _nghost-eae-c146=""><!----></xpl-reference-pop-up><!----><span _ngcontent-eae-c156="" id="full-text-footer"></span></div><!----></div><!----><div _ngcontent-eae-c156="" class="col-3-24 u-pr-1 u-pl-1 col-buttons stats-document-container-rh u-printing-invisible-ie u-printing-invisible-ff"><xpl-document-buttons _ngcontent-eae-c156="" _nghost-eae-c155=""><div _ngcontent-eae-c155="" xplscrollsnapmigr="" cssclasstostick="document-side-menu-stick" fromelementid="toc-wrapper" tillelementid="full-text-footer" offsetfrom="150" offsetto="-800" scrollreset="true" id="scroll-snap-buttons-container" class="document-doc-buttons stats-document-container-buttons document-side-menu-stick"><ul _ngcontent-eae-c155="" class="tools"><li _ngcontent-eae-c155="" id="search-popover" xplpopoveranimateonscroll="" animateelementidposition="1" animatecontainerelementid="search-popover" class="blue-tooltip increased-width special-left-tooltip"><a _ngcontent-eae-c155="" triggers="click" placement="left"><i _ngcontent-eae-c155="" class="fas fa-search icon-size-md color-gray-dark"></i></a><!----></li><li _ngcontent-eae-c155="" id="resizer-popover" xplpopoveranimateonscroll="" animateelementidposition="1" animatecontainerelementid="resizer-popover" class="blue-tooltip special-left-tooltip"><a _ngcontent-eae-c155="" min-size="10" max-size="20" placement="left" triggers="click"><i _ngcontent-eae-c155="" class="far fa-text-size icon-size-md color-gray-dark"></i></a><!----></li></ul><!----><!----><xpl-back-to-top-button _ngcontent-eae-c155="" section="full-text-header" _nghost-eae-c81=""><ul _ngcontent-eae-c81="" class="back-to-top"><li _ngcontent-eae-c81=""><a _ngcontent-eae-c81="" class="btn-document back-to-top-btn"><i _ngcontent-eae-c81="" class="icon--min icon-back-to-top-arrow"></i></a></li></ul></xpl-back-to-top-button></div></xpl-document-buttons></div><!----></div><!----></section></xpl-document-full-text><xpl-accordian-section _ngcontent-eae-c189="" _nghost-eae-c181=""><div _ngcontent-eae-c181="" role="tablist" class="document-accordion-section-container hide-mobile"><xpl-document-accordion _ngcontent-eae-c181="" class="accordion-panel-container" _nghost-eae-c161=""><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="authors-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="authors" class="text-base-md-lh">Authors</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="figures-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="figures" class="text-base-md-lh">Figures</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="references-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="references" class="text-base-md-lh">References</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="citations-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="citations" class="text-base-md-lh">Citations</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="keywords-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="keywords" class="text-base-md-lh">Keywords</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="metrics-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="metrics" class="text-base-md-lh">Metrics</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><div _ngcontent-eae-c161=""><div _ngcontent-eae-c161="" role="tab" class="accordion-header accordion-button" id="footnotes-header" aria-expanded="false" aria-disabled="false"><a _ngcontent-eae-c181="" href="javascript:void()" id="footnotes" class="text-base-md-lh">Footnotes</a><!----><div _ngcontent-eae-c161="" class="accordion-chevron"><i _ngcontent-eae-c161="" class="fa fa-angle-down"></i></div><!----></div><!----></div><!----></xpl-document-accordion></div></xpl-accordian-section></div><!----></section></div><div _ngcontent-eae-c189="" class="document-disqus-container col-24-24"><div _ngcontent-eae-c189="" class="row"><div _ngcontent-eae-c189="" class="col-12 disqus-container"><xpl-disqus-migr _ngcontent-eae-c189="" page="document" _nghost-eae-c186=""><div id="disqus_recommendations" style="margin-bottom: 12px;"><iframe id="dsq-app9654" name="dsq-app9654" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./全景摄像头_files/saved_resource(1).html" style="width: 100% !important; border: none !important; overflow: hidden !important; height: 269px !important; display: inline !important; box-sizing: border-box !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></div><div _ngcontent-eae-c186="" id="disqus_thread"><iframe id="dsq-app2024" name="dsq-app2024" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./全景摄像头_files/saved_resource(2).html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 558px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></div></xpl-disqus-migr></div><!----></div></div></div></div><div _ngcontent-eae-c189="" class="document-sidebar global-right-rail top-spacing"><div _ngcontent-eae-c189="" class="header-rel-art-toggle-mobile"><i _ngcontent-eae-c189="" class="header-rel-art-toggle-icon"></i></div><!----><div _ngcontent-eae-c189="" class="document-sidebar-content"><!----><div _ngcontent-eae-c189="" class="hide-mobile"><xpl-leaderboard-ad _ngcontent-eae-c189="" _nghost-eae-c121=""><div _ngcontent-eae-c121="" class="Ads-leaderboard ad-panel"><div _ngcontent-eae-c121="" class="row u-flex-wrap-nowrap"><!----></div><div _ngcontent-eae-c121="" class="ad-leaderboard-ad-container"><div _ngcontent-eae-c121="" xplgoogleadmigr=""><div id="div-gpt-ad-1606861783116-0" style="width:300px;
			height:250px; display:block; margin: 0 auto; undefined"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861783116-0"); });</script></div><!----><!----><!----></div></div><!----></xpl-leaderboard-ad><!----></div><div _ngcontent-eae-c189="" class="document-sidebar-rel-art"><xpl-related-article-list _ngcontent-eae-c189="" _nghost-eae-c187=""><div _ngcontent-eae-c187="" class="stats-document-header-relatedArticles"><div _ngcontent-eae-c187="" class="header-rel-art"><div _ngcontent-eae-c187="" class="header-rel-art-title text-base-md-lh"> More Like This </div><div _ngcontent-eae-c187="" class="header-rel-art-list"><div _ngcontent-eae-c187="" class="header-rel-art-item"><div _ngcontent-eae-c187="" class="row text-base-md-lh"><a _ngcontent-eae-c187="" target="_self" href="https://ieeexplore.ieee.org/document/5170035/"><span _ngcontent-eae-c187="">Position Control of a Wheeled Mobile Robot Including Tire Behavior</span></a><!----></div><p _ngcontent-eae-c187="" class="header-rel-art-pub text-sm-md-lh">IEEE Transactions on Intelligent Transportation Systems</p><!----><p _ngcontent-eae-c187="" class="header-rel-art-pub text-sm-md-lh">Published: 2009</p></div><div _ngcontent-eae-c187="" class="header-rel-art-item"><div _ngcontent-eae-c187="" class="row text-base-md-lh"><a _ngcontent-eae-c187="" target="_self" href="https://ieeexplore.ieee.org/document/6957824/"><span _ngcontent-eae-c187="">A fast RRT algorithm for motion planning of autonomous road vehicles</span></a><!----></div><p _ngcontent-eae-c187="" class="header-rel-art-pub text-sm-md-lh">17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</p><!----><p _ngcontent-eae-c187="" class="header-rel-art-pub text-sm-md-lh">Published: 2014</p></div><!----></div><div _ngcontent-eae-c187="" class="header-rel-art-action text-base-md-lh"><a _ngcontent-eae-c187="" href="javascript:void()">Show More</a><!----><!----></div><!----></div></div></xpl-related-article-list></div><!----><div _ngcontent-eae-c189="" class="hide-mobile"><xpl-leaderboard-middle-ad _ngcontent-eae-c189="" _nghost-eae-c185=""><div _ngcontent-eae-c185="" class="Ads-leaderboard ad-panel"><div _ngcontent-eae-c185="" class="row u-flex-wrap-nowrap"><!----></div><div _ngcontent-eae-c185="" class="ad-leaderboard-ad-container"><div _ngcontent-eae-c185="" xplgoogleadmigr=""><div id="div-gpt-ad-1606861708157-0" style="width:300px;
			height:600px; display:block; margin: 0 auto; undefined"></div><script type="text/javascript">googletag.cmd.push(function() { googletag.display("div-gpt-ad-1606861708157-0"); });</script></div><!----><!----><!----></div></div><!----></xpl-leaderboard-middle-ad><!----></div></div></div><xpl-reference-panel _ngcontent-eae-c189="" _nghost-eae-c188=""><section _ngcontent-eae-c188="" id="references-anchor" class="document-all-references hide-mobile panel-closed"><div _ngcontent-eae-c188="" class="header"><h1 _ngcontent-eae-c188="">References</h1><a _ngcontent-eae-c188=""><i _ngcontent-eae-c188="" class="fas fa-times"></i></a></div><div _ngcontent-eae-c188="" id="references-section-container" class="document-ft-section-container"><!----><div _ngcontent-eae-c188=""><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref1"><b _ngcontent-eae-c168="">1.</b></span><span _ngcontent-eae-c168="">S. Singh, "Critical reasons for crashes investigated in the national motor vehicle crash causation survey", 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Critical+reasons+for+crashes+investigated+in+the+national+motor+vehicle+crash+causation+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref2"><b _ngcontent-eae-c168="">2.</b></span><span _ngcontent-eae-c168="">T. J. Crayton and B. M. Meier, "Autonomous vehicles: Developing a public health research agenda to frame the future of transportation policy", <em>J. Transp. Health</em>, vol. 6, pp. 245-252, Sep. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.jth.2017.04.004"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+vehicles%3A+Developing+a+public+health+research+agenda+to+frame+the+future+of+transportation+policy&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref3"><b _ngcontent-eae-c168="">3.</b></span><span _ngcontent-eae-c168="">W. D. Montgomery, R. Mudge, E. L. Groshen, S. Helper, J. P. MacDuffie and C. Carson, "America’s workforce and the self-driving future: Realizing productivity gains and spurring economic growth", 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=America%E2%80%99s+workforce+and+the+self-driving+future%3A+Realizing+productivity+gains+and+spurring+economic+growth&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref4"><b _ngcontent-eae-c168="">4.</b></span><span _ngcontent-eae-c168="">A. Krizhevsky, I. Sutskever and G. E. Hinton, "ImageNet classification with deep convolutional neural networks", <em>Proc. Adv. Neural Inf. Process. Syst.</em>, pp. 1097-1105, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=ImageNet+classification+with+deep+convolutional+neural+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref5"><b _ngcontent-eae-c168="">5.</b></span><span _ngcontent-eae-c168="">B. Schwarz, "LIDAR: Mapping the world in 3D", <em>Nature Photon.</em>, vol. 4, pp. 429, 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1038/nphoton.2010.148"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=LIDAR%3A+Mapping+the+world+in+3D&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref6"><b _ngcontent-eae-c168="">6.</b></span><span _ngcontent-eae-c168="">S. Hecker, D. Dai and L. Van Gool, "End-to-end learning of driving models with surround-view cameras and route planners", <em>Proc. Eur. Conf. Comput. Vis. (ECCV)</em>, pp. 435-453, 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-030-01234-2_27"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=End-to-end+learning+of+driving+models+with+surround-view+cameras+and+route+planners&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref7"><b _ngcontent-eae-c168="">7.</b></span><span _ngcontent-eae-c168="">D. Lavrinc, This is How Bad Self-Driving Cars Suck in Rain, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://jalopnik.com/this-is-how-bad-self-driving-cars-suck-in-the-rain-1666268433" rel="nofollow"><span>https</span><span>://</span><span>jalopnik</span><span>.</span><span>com</span><span>/</span><span>this</span><span>-</span><span>is</span><span>-</span><span>how</span><span>-</span><span>bad</span><span>-</span><span>self</span><span>-</span><span>driving</span><span>-</span><span>cars</span><span>-</span><span>suck</span><span>-</span><span>in</span><span>-</span><span>the</span><span>-</span><span>rain</span><span>-</span><span>1666268433</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=This+is+How+Bad+Self-Driving+Cars+Suck+in+Rain&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref8"><b _ngcontent-eae-c168="">8.</b></span><span _ngcontent-eae-c168="">A. Davies, Google’s Self-Driving Car Caused Its First Crash, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://www.wired.com/2016/02/googles-self-driving-car-may-caused-first-crash/" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>wired</span><span>.</span><span>com</span><span>/</span><span>2016</span><span>/</span><span>02</span><span>/</span><span>googles</span><span>-</span><span>self</span><span>-</span><span>driving</span><span>-</span><span>car</span><span>-</span><span>may</span><span>-</span><span>caused</span><span>-</span><span>first</span><span>-</span><span>crash</span><span>/</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Google%E2%80%99s+Self-Driving+Car+Caused+Its+First+Crash&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref9"><b _ngcontent-eae-c168="">9.</b></span><span _ngcontent-eae-c168="">M. McFarland, Who’s Responsible When an Autonomous Car Crashes?, Jun. 2019,  [online]  Available: <a class="vglnk" href="https://money.cnn.com/2016/07/07/technology/tesla-liability-risk/index.html" rel="nofollow"><span>https</span><span>://</span><span>money</span><span>.</span><span>cnn</span><span>.</span><span>com</span><span>/</span><span>2016</span><span>/</span><span>07</span><span>/</span><span>07</span><span>/</span><span>technology</span><span>/</span><span>tesla</span><span>-</span><span>liability</span><span>-</span><span>risk</span><span>/</span><span>index</span><span>.</span><span>html</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Who%E2%80%99s+Responsible+When+an+Autonomous+Car+Crashes%3F&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref10"><b _ngcontent-eae-c168="">10.</b></span><span _ngcontent-eae-c168="">T. B. Lee, Autopilot Was Active When a Tesla Crashed Into a Truck Killing Driver, May 2019,  [online]  Available: <a class="vglnk" href="https://arstechnica.com/cars/2019/05/feds-autopilot-was-active-during-deadly-march-tesla-crash/" rel="nofollow"><span>https</span><span>://</span><span>arstechnica</span><span>.</span><span>com</span><span>/</span><span>cars</span><span>/</span><span>2019</span><span>/</span><span>05</span><span>/</span><span>feds</span><span>-</span><span>autopilot</span><span>-</span><span>was</span><span>-</span><span>active</span><span>-</span><span>during</span><span>-</span><span>deadly</span><span>-</span><span>march</span><span>-</span><span>tesla</span><span>-</span><span>crash</span><span>/</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autopilot+Was+Active+When+a+Tesla+Crashed+Into+a+Truck%2C+Killing+Driver&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref11"><b _ngcontent-eae-c168="">11.</b></span><span _ngcontent-eae-c168=""><em>E! 45: Programme For a European Traffic System With Highest Efficiency and Unprecedented Safety</em>, May 2019,  [online]  Available: <a class="vglnk" href="https://www.eurekanetwork.org/project/id/45" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>eurekanetwork</span><span>.</span><span>org</span><span>/</span><span>project</span><span>/</span><span>id</span><span>/</span><span>45</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=E%21+45%3A+Programme+For+a+European+Traffic+System+With+Highest+Efficiency+and+Unprecedented+Safety&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref12"><b _ngcontent-eae-c168="">12.</b></span><span _ngcontent-eae-c168="">B. Ulmer, "VITA II-active collision avoidance in real traffic", <em>Proc. Intell. Vehicles Symp.</em>, pp. 1-6, 1994.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/639460"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=639460"> Full Text: PDF </a><span _ngcontent-eae-c168="">(619KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=VITA+II-active+collision+avoidance+in+real+traffic&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref13"><b _ngcontent-eae-c168="">13.</b></span><span _ngcontent-eae-c168="">The 2005 DARPA Grand Challenge: The Great Robot Race, Berlin, Germany:Springer, vol. 36, 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-540-73429-1"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+2005+DARPA+Grand+Challenge%3A+The+Great+Robot+Race&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref14"><b _ngcontent-eae-c168="">14.</b></span><span _ngcontent-eae-c168="">D. Feng, C. Haase-Schuetz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm, et al., "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets methods and challenges" in arXiv:1902.07830, 2019,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1902.07830" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1902</span><span>.</span><span>07830</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Deep+multi-modal+object+detection+and+semantic+segmentation+for+autonomous+driving%3A+Datasets%2C+methods%2C+and+challenges&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref15"><b _ngcontent-eae-c168="">15.</b></span><span _ngcontent-eae-c168="">J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel, et al., "Towards fully autonomous driving: Systems and algorithms", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 163-168, Jun. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5940562"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5940562"> Full Text: PDF </a><span _ngcontent-eae-c168="">(823KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Towards+fully+autonomous+driving%3A+Systems+and+algorithms&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref16"><b _ngcontent-eae-c168="">16.</b></span><span _ngcontent-eae-c168="">M. Campbell, M. Egerstedt, J. P. How and R. M. Murray, "Autonomous driving in urban environments: Approaches lessons and challenges", <em>Phil. Trans. Roy. Soc. A Math. Phys. Eng. Sci.</em>, vol. 368, no. 1928, pp. 4649-4672, Oct. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1098/rsta.2010.0110"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+driving+in+urban+environments%3A+Approaches%2C+lessons+and+challenges&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref17"><b _ngcontent-eae-c168="">17.</b></span><span _ngcontent-eae-c168="">S. Kuutti, S. Fallah, K. Katsaros, M. Dianati, F. Mccullough and A. Mouzakitis, "A survey of the state-of-the-art localization techniques and their potentials for autonomous vehicle applications", <em>IEEE Internet Things J.</em>, vol. 5, no. 2, pp. 829-846, Apr. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8306879"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8306879"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3416KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+survey+of+the+state-of-the-art+localization+techniques+and+their+potentials+for+autonomous+vehicle+applications&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref18"><b _ngcontent-eae-c168="">18.</b></span><span _ngcontent-eae-c168="">B. Paden, M. Cap, S. Z. Yong, D. Yershov and E. Frazzoli, "A survey of motion planning and control techniques for self-driving urban vehicles", <em>IEEE Trans. Intell. Vehicles</em>, vol. 1, no. 1, pp. 33-55, Mar. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7490340"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7490340"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1405KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+survey+of+motion+planning+and+control+techniques+for+self-driving+urban+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref19"><b _ngcontent-eae-c168="">19.</b></span><span _ngcontent-eae-c168="">D. Gonzalez, J. Perez, V. Milanes and F. Nashashibi, "A review of motion planning techniques for automated vehicles", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 17, no. 4, pp. 1135-1145, Apr. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7339478"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7339478"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3110KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+review+of+motion+planning+techniques+for+automated+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref20"><b _ngcontent-eae-c168="">20.</b></span><span _ngcontent-eae-c168="">J. Van Brummelen, M. O’Brien, D. Gruyer and H. Najjaran, "Autonomous vehicle perception: The technology of today and tomorrow", <em>Transp. Res. C Emerg. Technol.</em>, vol. 89, pp. 384-406, Apr. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.trc.2018.02.012"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+vehicle+perception%3A+The+technology+of+today+and+tomorrow&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref21"><b _ngcontent-eae-c168="">21.</b></span><span _ngcontent-eae-c168="">G. Bresson, Z. Alsayed, L. Yu and S. Glaser, "Simultaneous localization and mapping: A survey of current trends in autonomous driving", <em>IEEE Trans. Intell. Vehicles</em>, vol. 2, no. 3, pp. 194-220, Sep. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8025618"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8025618"> Full Text: PDF </a><span _ngcontent-eae-c168="">(680KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Simultaneous+localization+and+mapping%3A+A+survey+of+current+trends+in+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref22"><b _ngcontent-eae-c168="">22.</b></span><span _ngcontent-eae-c168="">K. Abboud, H. A. Omar and W. Zhuang, "Interworking of DSRC and cellular network technologies for V2X communications: A survey", <em>IEEE Trans. Veh. Technol.</em>, vol. 65, no. 12, pp. 9457-9470, Dec. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7513432"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7513432"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1902KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Interworking+of+DSRC+and+cellular+network+technologies+for+V2X+communications%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref23"><b _ngcontent-eae-c168="">23.</b></span><span _ngcontent-eae-c168="">C. Badue, R. Guidolini, R. Vivacqua Carneiro, P. Azevedo, V. B. Cardoso, A. Forechi, et al., "Self-driving cars: A survey" in arXiv:1901.04407, 2019,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1901.04407" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1901</span><span>.</span><span>04407</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Self-driving+cars%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref24"><b _ngcontent-eae-c168="">24.</b></span><span _ngcontent-eae-c168="">W. Schwarting, J. Alonso-Mora and D. Rus, "Planning and decision-making for autonomous vehicles", <em>Annu. Rev. Control Robot. Auto. Syst.</em>, vol. 1, no. 1, pp. 187-210, May 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1146/annurev-control-060117-105157"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Planning+and+decision-making+for+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref25"><b _ngcontent-eae-c168="">25.</b></span><span _ngcontent-eae-c168="">S. Lefèvre, D. Vasquez and C. Laugier, "A survey on motion prediction and risk assessment for intelligent vehicles", <em>ROBOMECH J.</em>, vol. 1, no. 1, pp. 1, Dec. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1186/s40648-014-0001-z"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+survey+on+motion+prediction+and+risk+assessment+for+intelligent+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref26"><b _ngcontent-eae-c168="">26.</b></span><span _ngcontent-eae-c168="">The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, Berlin, Germany:Springer, vol. 56, 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-642-03991-1"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+DARPA+Urban+Challenge%3A+Autonomous+Vehicles+in+City+Traffic&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref27"><b _ngcontent-eae-c168="">27.</b></span><span _ngcontent-eae-c168="">A. Broggi, P. Cerri, M. Felisa, M. C. Laghi, L. Mazzei and P. P. Porta, "The VisLab intercontinental autonomous challenge: An extensive test for a platoon of intelligent vehicles", <em>Int. J. Vehicle Auto. Syst.</em>, vol. 10, no. 3, pp. 147, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1504/IJVAS.2012.051250"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+VisLab+intercontinental+autonomous+challenge%3A+An+extensive+test+for+a+platoon+of+intelligent+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref28"><b _ngcontent-eae-c168="">28.</b></span><span _ngcontent-eae-c168="">A. Broggi, P. Cerri, S. Debattisti, M. Chiara Laghi, P. Medici, D. Molinari, et al., "PROUD—Public road urban driverless-car test", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 16, no. 6, pp. 3508-3519, Dec. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7274741"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7274741"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2007KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PROUD%E2%80%94Public+road+urban+driverless-car+test&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref29"><b _ngcontent-eae-c168="">29.</b></span><span _ngcontent-eae-c168="">P. Cerri, G. Soprani, P. Zani, J. Choi, J. Lee, D. Kim, et al., "Computer vision at the Hyundai autonomous challenge", <em>Proc. 14th Int. IEEE Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 777-783, Oct. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6082859"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6082859"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1740KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Computer+vision+at+the+Hyundai+autonomous+challenge&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref30"><b _ngcontent-eae-c168="">30.</b></span><span _ngcontent-eae-c168="">C. Englund, L. Chen, J. Ploeg, E. Semsar-Kazerooni, A. Voronov, H. H. Bengtsson, et al., "The grand cooperative driving challenge 2016: Boosting the introduction of cooperative automated vehicles", <em>IEEE Wireless Commun.</em>, vol. 23, no. 4, pp. 146-152, Aug. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7553038"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7553038"> Full Text: PDF </a><span _ngcontent-eae-c168="">(679KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+grand+cooperative+driving+challenge+2016%3A+Boosting+the+introduction+of+cooperative+automated+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref31"><b _ngcontent-eae-c168="">31.</b></span><span _ngcontent-eae-c168="">R. McAllister, Y. Gal, A. Kendall, M. van der Wilk, A. Shah, R. Cipolla, et al., "Concrete problems for autonomous vehicle safety: Advantages of Bayesian deep learning", <em>Proc. 26th Int. Joint Conf. Artif. Intell.</em>, pp. 1-9, Aug. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.24963/ijcai.2017/661"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Concrete+problems+for+autonomous+vehicle+safety%3A+Advantages+of+Bayesian+deep+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref32"><b _ngcontent-eae-c168="">32.</b></span><span _ngcontent-eae-c168="">Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles, 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Taxonomy+and+definitions+for+terms+related+to+driving+automation+systems+for+on-road+motor+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref33"><b _ngcontent-eae-c168="">33.</b></span><span _ngcontent-eae-c168="">"World population prospects the 2017 revision key findings and advance tables", <em>Proc. World Popul. Prospect.</em>, pp. 1-46, 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=World+population+prospects+the+2017+revision+key+findings+and+advance+tables&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref34"><b _ngcontent-eae-c168="">34.</b></span><span _ngcontent-eae-c168=""><em>2019 Deloitte Global Automotive Consumer Study—Advanced Vehicle Technologies and Multimodal Transportation Global Focus Countries</em>, May 2019,  [online]  Available: <a class="vglnk" href="https://www2.deloitte.com/content/dam/Deloitte/us/Documents/manufacturing/us-global-automotive-consumer-study-2019.pdf" rel="nofollow"><span>https</span><span>://</span><span>www2</span><span>.</span><span>deloitte</span><span>.</span><span>com</span><span>/</span><span>content</span><span>/</span><span>dam</span><span>/</span><span>Deloitte</span><span>/</span><span>us</span><span>/</span><span>Documents</span><span>/</span><span>manufacturing</span><span>/</span><span>us</span><span>-</span><span>global</span><span>-</span><span>automotive</span><span>-</span><span>consumer</span><span>-</span><span>study</span><span>-</span><span>2019</span><span>.</span><span>pdf</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=2019+Deloitte+Global+Automotive+Consumer+Study%E2%80%94Advanced+Vehicle+Technologies+and+Multimodal+Transportation%2C+Global+Focus+Countries&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref35"><b _ngcontent-eae-c168="">35.</b></span><span _ngcontent-eae-c168=""><em>The Automotive Digital Transformation and the Economic Impacts of Existing Data Access Models</em>, May 2019,  [online]  Available: <a class="vglnk" href="https://www.fiaregion1.com/wp-content/uploads/2019/03/The-Automotive-Digital-Transformation_Full-study.pdf" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>fiaregion1</span><span>.</span><span>com</span><span>/</span><span>wp</span><span>-</span><span>content</span><span>/</span><span>uploads</span><span>/</span><span>2019</span><span>/</span><span>03</span><span>/</span><span>The</span><span>-</span><span>Automotive</span><span>-</span><span>Digital</span><span>-</span><span>Transformation</span><span>_</span><span>Full</span><span>-</span><span>study</span><span>.</span><span>pdf</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+Automotive+Digital+Transformation+and+the+Economic+Impacts+of+Existing+Data+Access+Models&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref36"><b _ngcontent-eae-c168="">36.</b></span><span _ngcontent-eae-c168="">R. Rajamani, Vehicle Dynamics and Control, Berlin, Germany:Springer, vol. 25, pp. 471, 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Vehicle+Dynamics+and+Control&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref37"><b _ngcontent-eae-c168="">37.</b></span><span _ngcontent-eae-c168="">M. R. Hafner, D. Cunningham, L. Caminiti and D. Del Vecchio, "Cooperative collision avoidance at intersections: Algorithms and experiments", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 14, no. 3, pp. 1162-1175, Sep. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6495719"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6495719"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1378KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Cooperative+collision+avoidance+at+intersections%3A+Algorithms+and+experiments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref38"><b _ngcontent-eae-c168="">38.</b></span><span _ngcontent-eae-c168="">A. Colombo and D. Del Vecchio, "Efficient algorithms for collision avoidance at intersections", <em>Proc. 15th ACM Int. Conf. Hybrid Syst. Comput. Control (HSCC)</em>, pp. 145-154, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Efficient+algorithms+for+collision+avoidance+at+intersections&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref39"><b _ngcontent-eae-c168="">39.</b></span><span _ngcontent-eae-c168="">P. E. Ross, "The audi a8: The World’s first production car to achieve level 3 autonomy", <em>IEEE Spectr.</em>, vol. 1, 2017,  [online]  Available: <a class="vglnk" href="https://spectrum.ieee.org/cars-that-think/transportation/self-driving/the-audi-a8-the-worlds-first-production-car-to-achieve-level-3-autonomy" rel="nofollow"><span>https</span><span>://</span><span>spectrum</span><span>.</span><span>ieee</span><span>.</span><span>org</span><span>/</span><span>cars</span><span>-</span><span>that</span><span>-</span><span>think</span><span>/</span><span>transportation</span><span>/</span><span>self</span><span>-</span><span>driving</span><span>/</span><span>the</span><span>-</span><span>audi</span><span>-</span><span>a8</span><span>-</span><span>the</span><span>-</span><span>worlds</span><span>-</span><span>first</span><span>-</span><span>production</span><span>-</span><span>car</span><span>-</span><span>to</span><span>-</span><span>achieve</span><span>-</span><span>level</span><span>-</span><span>3</span><span>-</span><span>autonomy</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+audi+a8%3A+The+World%E2%80%99s+first+production+car+to+achieve+level+3+autonomy&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref40"><b _ngcontent-eae-c168="">40.</b></span><span _ngcontent-eae-c168="">C. Gold, M. Körber, D. Lechner and K. Bengler, "Taking over control from highly automated vehicles in complex traffic situations: The role of traffic density", <em>Hum. Factors J. Hum. Factors Ergonom. Soc.</em>, vol. 58, no. 4, pp. 642-652, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0018720816634226"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Taking+over+control+from+highly+automated+vehicles+in+complex+traffic+situations%3A+The+role+of+traffic+density&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref41"><b _ngcontent-eae-c168="">41.</b></span><span _ngcontent-eae-c168="">N. Merat, A. H. Jamson, F. C. H. Lai, M. Daly and O. M. J. Carsten, "Transition to manual: Driver behaviour when resuming control from a highly automated vehicle", <em>Transp. Res. F Traffic Psychol. Behaviour</em>, vol. 27, pp. 274-282, Nov. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.trf.2014.09.005"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Transition+to+manual%3A+Driver+behaviour+when+resuming+control+from+a+highly+automated+vehicle&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref42"><b _ngcontent-eae-c168="">42.</b></span><span _ngcontent-eae-c168="">E. Ackerman, "Toyota’s gill pratt on self-driving cars and the reality of full autonomy", <em>IEEE Spectr.</em>, May 2017,  [online]  Available: <a class="vglnk" href="https://spectrum.ieee.org/cars-that-think/transportation/self-driving/toyota-gill-pratt-on-the-reality-of-full-autonomy" rel="nofollow"><span>https</span><span>://</span><span>spectrum</span><span>.</span><span>ieee</span><span>.</span><span>org</span><span>/</span><span>cars</span><span>-</span><span>that</span><span>-</span><span>think</span><span>/</span><span>transportation</span><span>/</span><span>self</span><span>-</span><span>driving</span><span>/</span><span>toyota</span><span>-</span><span>gill</span><span>-</span><span>pratt</span><span>-</span><span>on</span><span>-</span><span>the</span><span>-</span><span>reality</span><span>-</span><span>of</span><span>-</span><span>full</span><span>-</span><span>autonomy</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Toyota%E2%80%99s+gill+pratt+on+self-driving+cars+and+the+reality+of+full+autonomy&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref43"><b _ngcontent-eae-c168="">43.</b></span><span _ngcontent-eae-c168="">J. D’Onfro, <em>‘I Hate Them’: Locals Reportedly Are Frustrated With Alphabet’s Self-Driving Cars</em>, May 2019,  [online]  Available: <a class="vglnk" href="https://www.cnbc.com/2018/08/28/locals-reportedly-frustrated-with-alphabets-waymo-self-driving-cars.html" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>cnbc</span><span>.</span><span>com</span><span>/</span><span>2018</span><span>/</span><span>08</span><span>/</span><span>28</span><span>/</span><span>locals</span><span>-</span><span>reportedly</span><span>-</span><span>frustrated</span><span>-</span><span>with</span><span>-</span><span>alphabets</span><span>-</span><span>waymo</span><span>-</span><span>self</span><span>-</span><span>driving</span><span>-</span><span>cars</span><span>.</span><span>html</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=%E2%80%98I+Hate+Them%E2%80%99%3A+Locals+Reportedly+Are+Frustrated+With+Alphabet%E2%80%99s+Self-Driving+Cars&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref44"><b _ngcontent-eae-c168="">44.</b></span><span _ngcontent-eae-c168="">J.-F. Bonnefon, A. Shariff and I. Rahwan, "The social dilemma of autonomous vehicles", <em>Science</em>, vol. 352, no. 6293, pp. 1573-1576, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1126/science.aaf2654"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+social+dilemma+of+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref45"><b _ngcontent-eae-c168="">45.</b></span><span _ngcontent-eae-c168="">J.-F. Bonnefon, A. Shariff and I. Rahwan, "The social dilemma of autonomous vehicles" in arXiv:1510.03346, 2015,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1510.03346" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1510</span><span>.</span><span>03346</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+social+dilemma+of+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref46"><b _ngcontent-eae-c168="">46.</b></span><span _ngcontent-eae-c168="">Y. Tian, K. Pei, S. Jana and B. Ray, "DeepTest: Automated testing of deep-neural-network-driven autonomous cars", <em>Proc. 40th Int. Conf. Softw. Eng. (ICSE)</em>, pp. 303-314, 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8453089"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8453089"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1476KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=DeepTest%3A+Automated+testing+of+deep-neural-network-driven+autonomous+cars&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref47"><b _ngcontent-eae-c168="">47.</b></span><span _ngcontent-eae-c168="">C. Urmson et al., "Autonomous driving in urban environments: Boss and the urban challenge", <em>J. Field Robot.</em>, vol. 25, no. 8, pp. 425-466, 2008.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1002/rob.20255"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+driving+in+urban+environments%3A+Boss+and+the+urban+challenge&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref48"><b _ngcontent-eae-c168="">48.</b></span><span _ngcontent-eae-c168="">M. Gerla, E.-K. Lee, G. Pau and U. Lee, "Internet of vehicles: From intelligent grid to autonomous cars and vehicular clouds", <em>Proc. IEEE World Forum Internet Things (WF-IoT)</em>, pp. 241-246, Mar. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6803166"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6803166"> Full Text: PDF </a><span _ngcontent-eae-c168="">(692KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Internet+of+vehicles%3A+From+intelligent+grid+to+autonomous+cars+and+vehicular+clouds&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref49"><b _ngcontent-eae-c168="">49.</b></span><span _ngcontent-eae-c168="">E.-K. Lee, M. Gerla, G. Pau, U. Lee and J.-H. Lim, "Internet of vehicles: From intelligent grid to autonomous cars and vehicular fogs", <em>Int. J. Distrib. Sensor Netw.</em>, vol. 12, no. 9, Sep. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/1550147716665500"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Internet+of+vehicles%3A+From+intelligent+grid+to+autonomous+cars+and+vehicular+fogs&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref50"><b _ngcontent-eae-c168="">50.</b></span><span _ngcontent-eae-c168="">M. Amadeo, C. Campolo and A. Molinaro, "Information-centric networking for connected vehicles: A survey and future perspectives", <em>IEEE Commun. Mag.</em>, vol. 54, no. 2, pp. 98-104, Feb. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7402268"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7402268"> Full Text: PDF </a><span _ngcontent-eae-c168="">(399KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Information-centric+networking+for+connected+vehicles%3A+A+survey+and+future+perspectives&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref51"><b _ngcontent-eae-c168="">51.</b></span><span _ngcontent-eae-c168="">J. Wei, J. M. Snider, J. Kim, J. M. Dolan, R. Rajkumar and B. Litkouhi, "Towards a viable autonomous driving research platform", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 763-770, Jun. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6629559"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6629559"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2907KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Towards+a+viable+autonomous+driving+research+platform&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref52"><b _ngcontent-eae-c168="">52.</b></span><span _ngcontent-eae-c168="">A. Broggi, M. Buzzoni, S. Debattisti, P. Grisleri, M. C. Laghi, P. Medici, et al., "Extensive tests of autonomous driving technologies", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 14, no. 3, pp. 1403-1415, Sep. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6522193"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6522193"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1331KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Extensive+tests+of+autonomous+driving+technologies&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref53"><b _ngcontent-eae-c168="">53.</b></span><span _ngcontent-eae-c168="">W. Maddern, G. Pascoe, C. Linegar and P. Newman, "1 year 1000 km: The Oxford RobotCar dataset", <em>Int. J. Robot. Res.</em>, vol. 36, no. 1, pp. 3-15, Jan. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0278364916679498"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=1+year%2C+1000+km%3A+The+Oxford+RobotCar+dataset&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref54"><b _ngcontent-eae-c168="">54.</b></span><span _ngcontent-eae-c168="">N. Akai, L. Y. Morales, T. Yamaguchi, E. Takeuchi, Y. Yoshihara, H. Okuda, et al., "Autonomous driving based on accurate localization using multilayer LiDAR and dead reckoning", <em>Proc. IEEE 20th Int. Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 1-6, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8317797"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8317797"> Full Text: PDF </a><span _ngcontent-eae-c168="">(717KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+driving+based+on+accurate+localization+using+multilayer+LiDAR+and+dead+reckoning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref55"><b _ngcontent-eae-c168="">55.</b></span><span _ngcontent-eae-c168="">E. Guizzo, "How Google’s self-driving car works", <em>IEEE Spectr.</em>, vol. 18, no. 7, pp. 1132-1141, Oct. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=How+Google%E2%80%99s+self-driving+car+works&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref56"><b _ngcontent-eae-c168="">56.</b></span><span _ngcontent-eae-c168="">H. Somerville, P. Lienert and A. Sage, <em>Uber’s Use of Fewer Safety Sensors Prompts Questions After Arizona Crash. Business News Reuters</em>, Mar. 2018,  [online]  Available: <a class="vglnk" href="https://www.reuters.com/article/us-uber-selfdriving-sensors-insight/ubers-use-of-fewer-safety-sensors-prompts-questions-after-arizona-crash-idUSKBN1H337Q" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>reuters</span><span>.</span><span>com</span><span>/</span><span>article</span><span>/</span><span>us</span><span>-</span><span>uber</span><span>-</span><span>selfdriving</span><span>-</span><span>sensors</span><span>-</span><span>insight</span><span>/</span><span>ubers</span><span>-</span><span>use</span><span>-</span><span>of</span><span>-</span><span>fewer</span><span>-</span><span>safety</span><span>-</span><span>sensors</span><span>-</span><span>prompts</span><span>-</span><span>questions</span><span>-</span><span>after</span><span>-</span><span>arizona</span><span>-</span><span>crash</span><span>-</span><span>idUSKBN1H337Q</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Uber%E2%80%99s+Use+of+Fewer+Safety+Sensors+Prompts+Questions+After+Arizona+Crash.+Business+News%2C+Reuters&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref57"><b _ngcontent-eae-c168="">57.</b></span><span _ngcontent-eae-c168="">J. Ziegler et al., "Making Bertha drive—An autonomous journey on a historic route", <em>IEEE Intell. Transp. Syst. Mag.</em>, vol. 6, no. 2, pp. 8-20, Summer 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6803933"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6803933"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3149KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Making+Bertha+drive%E2%80%94An+autonomous+journey+on+a+historic+route&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref58"><b _ngcontent-eae-c168="">58.</b></span><span _ngcontent-eae-c168=""><em>Apollo Auto</em>, May 2019,  [online]  Available: <a class="vglnk" href="https://github.com/ApolloAuto/apollo" rel="nofollow"><span>https</span><span>://</span><span>github</span><span>.</span><span>com</span><span>/</span><span>ApolloAuto</span><span>/</span><span>apollo</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Apollo+Auto&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref59"><b _ngcontent-eae-c168="">59.</b></span><span _ngcontent-eae-c168="">C. Chen, A. Seff, A. Kornhauser and J. Xiao, "DeepDriving: Learning affordance for direct perception in autonomous driving", <em>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, pp. 2722-2730, Dec. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7410669"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7410669"> Full Text: PDF </a><span _ngcontent-eae-c168="">(773KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=DeepDriving%3A+Learning+affordance+for+direct+perception+in+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref60"><b _ngcontent-eae-c168="">60.</b></span><span _ngcontent-eae-c168="">D. A. Pomerleau, "ALVINN: An autonomous land vehicle in a neural network", <em>Proc. Adv. Neural Inf. Process. Syst.</em>, pp. 305-313, 1989.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=ALVINN%3A+An+autonomous+land+vehicle+in+a+neural+network&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref61"><b _ngcontent-eae-c168="">61.</b></span><span _ngcontent-eae-c168="">U. Muller, J. Ben, E. Cosatto, B. Flepp and Y. L. Cun, "Off-road obstacle avoidance through end-to-end learning", <em>Proc. Adv. Neural Inf. Process. Syst.</em>, pp. 739-746, 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Off-road+obstacle+avoidance+through+end-to-end+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref62"><b _ngcontent-eae-c168="">62.</b></span><span _ngcontent-eae-c168="">M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, et al., "End to end learning for self-driving cars" in arXiv:1604.07316, 2016,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1604.07316" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1604</span><span>.</span><span>07316</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=End+to+end+learning+for+self-driving+cars&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref63"><b _ngcontent-eae-c168="">63.</b></span><span _ngcontent-eae-c168="">H. Xu, Y. Gao, F. Yu and T. Darrell, "End-to-end learning of driving models from large-scale video datasets" in arXiv:1612.01079, 2017,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1612.01079" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1612</span><span>.</span><span>01079</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8099859"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099859"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1756KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=End-to-end+learning+of+driving+models+from+large-scale+video+datasets&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref64"><b _ngcontent-eae-c168="">64.</b></span><span _ngcontent-eae-c168="">A. Sallab, M. Abdou, E. Perot and S. Yogamani, "Deep reinforcement learning framework for autonomous driving", <em>Electron. Imag.</em>, vol. 2017, no. 19, pp. 70-76, Jan. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.2352/ISSN.2470-1173.2017.19.AVM-023"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Deep+reinforcement+learning+framework+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref65"><b _ngcontent-eae-c168="">65.</b></span><span _ngcontent-eae-c168="">A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, et al., "Learning to drive in a day" in arXiv:1807.00412, 2018,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1807.00412" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1807</span><span>.</span><span>00412</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+to+drive+in+a+day&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref66"><b _ngcontent-eae-c168="">66.</b></span><span _ngcontent-eae-c168="">S. Baluja, "Evolution of an artificial neural network based autonomous land vehicle controller", <em>IEEE Trans. Syst. Man Cybern. B Cybern.</em>, vol. 26, no. 3, pp. 450-463, Jun. 1996.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/499795"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=499795"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2369KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Evolution+of+an+artificial+neural+network+based+autonomous+land+vehicle+controller&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref67"><b _ngcontent-eae-c168="">67.</b></span><span _ngcontent-eae-c168="">J. Koutník, G. Cuccu, J. Schmidhuber and F. Gomez, "Evolving large-scale neural networks for vision-based reinforcement learning", <em>Proc. 15th Annu. Conf. Genetic Evol. Comput. Conf. (GECCO)</em>, pp. 1061-1068, 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Evolving+large-scale+neural+networks+for+vision-based+reinforcement+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref68"><b _ngcontent-eae-c168="">68.</b></span><span _ngcontent-eae-c168="">S. Behere and M. Torngren, "A functional architecture for autonomous driving", <em>Proc. 1st Int. Workshop Automot. Softw. Archit. (WASA)</em>, pp. 3-10, May 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7447216"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7447216"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1156KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+functional+architecture+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref69"><b _ngcontent-eae-c168="">69.</b></span><span _ngcontent-eae-c168="">L. Chi and Y. Mu, "Deep steering: Learning end-to-end driving model from spatial and temporal visual cues" in arXiv:1708.03798, 2017,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1708.03798" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1708</span><span>.</span><span>03798</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Deep+steering%3A+Learning+end-to-end+driving+model+from+spatial+and+temporal+visual+cues&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref70"><b _ngcontent-eae-c168="">70.</b></span><span _ngcontent-eae-c168="">J.-P. Laumond, Robot Motion Planning and Control, Berlin, Germany:Springer, vol. 229, 1998.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/BFb0036070"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robot+Motion+Planning+and+Control&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref71"><b _ngcontent-eae-c168="">71.</b></span><span _ngcontent-eae-c168="">R. Jain, R. Kasturi and B. G. Schunck, Machine Vision, New York, NY, USA:McGraw-Hill, vol. 5, 1995.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Machine+Vision&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref72"><b _ngcontent-eae-c168="">72.</b></span><span _ngcontent-eae-c168="">S. J. Anderson, S. B. Karumanchi and K. Iagnemma, "Constraint-based planning and control for safe semi-autonomous operation of vehicles", <em>Proc. IEEE Intell. Vehicles Symp.</em>, pp. 383-388, Jun. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6232153"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6232153"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1703KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Constraint-based+planning+and+control+for+safe%2C+semi-autonomous+operation+of+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref73"><b _ngcontent-eae-c168="">73.</b></span><span _ngcontent-eae-c168="">V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, et al., "Human-level control through deep reinforcement learning", <em>Nature</em>, vol. 518, no. 7540, pp. 529-533, Feb. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1038/nature14236"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Human-level+control+through+deep+reinforcement+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref74"><b _ngcontent-eae-c168="">74.</b></span><span _ngcontent-eae-c168="">D. Floreano, P. Dürr and C. Mattiussi, "Neuroevolution: From architectures to learning", <em>Evol. Intell.</em>, vol. 1, no. 1, pp. 47-62, Mar. 2008.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s12065-007-0002-4"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Neuroevolution%3A+From+architectures+to+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref75"><b _ngcontent-eae-c168="">75.</b></span><span _ngcontent-eae-c168="">H. T. Cheng, H. Shan and W. Zhuang, "Infotainment and road safety service support in vehicular networking: From a communication perspective", <em>Mech. Syst. Signal Process.</em>, vol. 25, no. 6, pp. 2020-2038, Aug. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.ymssp.2010.11.009"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Infotainment+and+road+safety+service+support+in+vehicular+networking%3A+From+a+communication+perspective&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref76"><b _ngcontent-eae-c168="">76.</b></span><span _ngcontent-eae-c168="">J. Wang, Y. Shao, Y. Ge and R. Yu, "A survey of vehicle to everything (V2X) testing", <em>Sensors</em>, vol. 19, no. 2, pp. 334, 2019.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/s19020334"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+survey+of+vehicle+to+everything+%28V2X%29+testing&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref77"><b _ngcontent-eae-c168="">77.</b></span><span _ngcontent-eae-c168="">C. H. Jang, C. S. Kim, K. C. Jo and M. Sunwoo, "Design factor optimization of 3D flash lidar sensor based on geometrical model for automated vehicle and advanced driver assistance system applications", <em>Int. J. Automot. Technol.</em>, vol. 18, no. 1, pp. 147-156, Feb. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s12239-017-0015-7"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Design+factor+optimization+of+3D+flash+lidar+sensor+based+on+geometrical+model+for+automated+vehicle+and+advanced+driver+assistance+system+applications&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref78"><b _ngcontent-eae-c168="">78.</b></span><span _ngcontent-eae-c168="">A. I. Maqueda, A. Loquercio, G. Gallego, N. Garcia and D. Scaramuzza, "Event-based vision meets deep learning on steering prediction for self-driving cars", <em>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</em>, pp. 5419-5427, Jun. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8578666"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8578666"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1920KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Event-based+vision+meets+deep+learning+on+steering+prediction+for+self-driving+cars&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref79"><b _ngcontent-eae-c168="">79.</b></span><span _ngcontent-eae-c168="">C. Fries and H.-J. Wuensche, "Autonomous convoy driving by night: The vehicle tracking system", <em>Proc. IEEE Int. Conf. Technol. Practical Robot Appl. (TePRA)</em>, pp. 1-6, May 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7219675"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7219675"> Full Text: PDF </a><span _ngcontent-eae-c168="">(555KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+convoy+driving+by+night%3A+The+vehicle+tracking+system&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref80"><b _ngcontent-eae-c168="">80.</b></span><span _ngcontent-eae-c168="">Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku and T. Harada, "MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</em>, pp. 5108-5115, Sep. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8206396"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8206396"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1549KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=MFNet%3A+Towards+real-time+semantic+segmentation+for+autonomous+vehicles+with+multi-spectral+scenes&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref81"><b _ngcontent-eae-c168="">81.</b></span><span _ngcontent-eae-c168="">T. B. Lee, How 10 Leading Companies Are Trying To Make Powerful Low-Cost Lidar, May 2019,  [online]  Available: <a class="vglnk" href="https://arstechnica.com/cars/2019/02/the-ars-technica-guide-to-the-lidar-industry/" rel="nofollow"><span>https</span><span>://</span><span>arstechnica</span><span>.</span><span>com</span><span>/</span><span>cars</span><span>/</span><span>2019</span><span>/</span><span>02</span><span>/</span><span>the</span><span>-</span><span>ars</span><span>-</span><span>technica</span><span>-</span><span>guide</span><span>-</span><span>to</span><span>-</span><span>the</span><span>-</span><span>lidar</span><span>-</span><span>industry</span><span>/</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=How+10+Leading+Companies+Are+Trying+To+Make+Powerful%2C+Low-Cost+Lidar&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref82"><b _ngcontent-eae-c168="">82.</b></span><span _ngcontent-eae-c168="">S. Kumar, L. Shi, N. Ahmed, S. Gil, D. Katabi and D. Rus, "CarSpeak: A content-centric network for autonomous driving", <em>Proc. ACM SIGCOMM Conf. Appl. Technol. Archit. Protocols Comput. Commun.</em>, pp. 259-270, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=CarSpeak%3A+A+content-centric+network+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref83"><b _ngcontent-eae-c168="">83.</b></span><span _ngcontent-eae-c168="">E. Yurtsever, S. Yamazaki, C. Miyajima, K. Takeda, M. Mori, K. Hitomi, et al., "Integrating driving behavior and traffic context through signal symbolization for data reduction and risky lane change detection", <em>IEEE Trans. Intell. Vehicles</em>, vol. 3, no. 3, pp. 242-253, Sep. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8370754"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8370754"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3222KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Integrating+driving+behavior+and+traffic+context+through+signal+symbolization+for+data+reduction+and+risky+lane+change+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref84"><b _ngcontent-eae-c168="">84.</b></span><span _ngcontent-eae-c168="">M. Gerla, "Vehicular cloud computing", <em>Proc. 11th Annu. Medit. Ad Hoc Netw. Workshop (Med-Hoc-Net)</em>, pp. 152-155, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6257116"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6257116"> Full Text: PDF </a><span _ngcontent-eae-c168="">(363KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Vehicular+cloud+computing&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref85"><b _ngcontent-eae-c168="">85.</b></span><span _ngcontent-eae-c168="">M. Whaiduzzaman, M. Sookhak, A. Gani and R. Buyya, "A survey on vehicular cloud computing", <em>J. Netw. Comput. Appl.</em>, vol. 40, pp. 325-344, Apr. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.jnca.2013.08.004"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+survey+on+vehicular+cloud+computing&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref86"><b _ngcontent-eae-c168="">86.</b></span><span _ngcontent-eae-c168="">I. Din, B.-S. Kim, S. Hassan, M. Guizani, M. Atiquzzaman and J. Rodrigues, "Information-centric network-based vehicular communications: Overview and research opportunities", <em>Sensors</em>, vol. 18, no. 11, pp. 3957, 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/s18113957"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Information-centric+network-based+vehicular+communications%3A+Overview+and+research+opportunities&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref87"><b _ngcontent-eae-c168="">87.</b></span><span _ngcontent-eae-c168="">A. Saxena, S. H. Chung and A. Y. Ng, "Learning depth from single monocular images", <em>Proc. Adv. Neural Inf. Process. Syst.</em>, pp. 1161-1168, 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+depth+from+single+monocular+images&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref88"><b _ngcontent-eae-c168="">88.</b></span><span _ngcontent-eae-c168="">J. Janai, F. Güney, A. Behl and A. Geiger, "Computer vision for autonomous vehicles: Problems datasets and state of the art" in arXiv:1704.05519, Apr. 2017,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1704.05519" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1704</span><span>.</span><span>05519</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Computer+vision+for+autonomous+vehicles%3A+Problems%2C+datasets+and+state+of+the+art&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref89"><b _ngcontent-eae-c168="">89.</b></span><span _ngcontent-eae-c168="">J. Binas, D. Neil, S.-C. Liu and T. Delbruck, "DDD17: End-to-end DAVIS driving dataset" in arXiv:1711.01458, 2017,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1711.01458" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1711</span><span>.</span><span>01458</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=DDD17%3A+End-to-end+DAVIS+driving+dataset&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref90"><b _ngcontent-eae-c168="">90.</b></span><span _ngcontent-eae-c168="">P. Lichtsteiner, C. Posch and T. Delbruck, "
            A 128 × 128 120 dB 15
            μ
            s latency asynchronous temporal contrast vision sensor
          ", <em>IEEE J. Solid-State Circuits</em>, vol. 43, no. 2, pp. 566-576, Feb. 2008.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4444573"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4444573"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2528KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+128+%C3%97+128+120+dB+15&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref91"><b _ngcontent-eae-c168="">91.</b></span><span _ngcontent-eae-c168="">B. Schoettle, "Sensor fusion: A comparison of sensing capabilities of human drivers and highly automated vehicles", Aug. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sensor+fusion%3A+A+comparison+of+sensing+capabilities+of+human+drivers+and+highly+automated+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref92"><b _ngcontent-eae-c168="">92.</b></span><span _ngcontent-eae-c168=""><em>Autopilot Press Kit</em>, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://www.tesla.com/presskit/autopilot#autopilot" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>tesla</span><span>.</span><span>com</span><span>/</span><span>presskit</span><span>/</span><span>autopilot</span><span>#</span><span>autopilot</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autopilot+Press+Kit&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref93"><b _ngcontent-eae-c168="">93.</b></span><span _ngcontent-eae-c168=""><em>Chris Urmson Explains Google Self-Driving Car Project</em>, Dec. 2016,  [online]  Available: <a class="vglnk" href="https://www.sxsw.com/interactive/2016/chris-urmson-explain-googles-self-driving-car-project/" rel="nofollow"><span>https</span><span>://</span><span>www</span><span>.</span><span>sxsw</span><span>.</span><span>com</span><span>/</span><span>interactive</span><span>/</span><span>2016</span><span>/</span><span>chris</span><span>-</span><span>urmson</span><span>-</span><span>explain</span><span>-</span><span>googles</span><span>-</span><span>self</span><span>-</span><span>driving</span><span>-</span><span>car</span><span>-</span><span>project</span><span>/</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Chris+Urmson+Explains+Google+Self-Driving+Car+Project&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref94"><b _ngcontent-eae-c168="">94.</b></span><span _ngcontent-eae-c168="">M. A. Al-Khedher, "Hybrid GPS-GSM localization of automobile tracking system" in arXiv:1201.2630, 2012,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1201.2630" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1201</span><span>.</span><span>2630</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Hybrid+GPS-GSM+localization+of+automobile+tracking+system&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref95"><b _ngcontent-eae-c168="">95.</b></span><span _ngcontent-eae-c168="">K. S. Chong and L. Kleeman, "Accurate odometry and error modelling for a mobile robot", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, vol. 4, pp. 2783-2788, Apr. 1997.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/606708"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=606708"> Full Text: PDF </a><span _ngcontent-eae-c168="">(721KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Accurate+odometry+and+error+modelling+for+a+mobile+robot&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref96"><b _ngcontent-eae-c168="">96.</b></span><span _ngcontent-eae-c168="">C. Urmson, J. Anhalt, M. Clark, T. Galatali, J. P. Gonzalez, J. Gowdy, et al., "High speed navigation of unrehearsed terrain: Red team technology for grand challenge 2004", 2004.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=High+speed+navigation+of+unrehearsed+terrain%3A+Red+team+technology+for+grand+challenge+2004&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref97"><b _ngcontent-eae-c168="">97.</b></span><span _ngcontent-eae-c168="">T. Bailey and H. Durrant-Whyte, "Simultaneous localization and mapping (SLAM): Part II", <em>IEEE Robot. Autom. Mag.</em>, vol. 13, no. 3, pp. 108-117, Sep. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/1678144"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1678144"> Full Text: PDF </a><span _ngcontent-eae-c168="">(372KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Simultaneous+localization+and+mapping+%28SLAM%29%3A+Part+II&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref98"><b _ngcontent-eae-c168="">98.</b></span><span _ngcontent-eae-c168="">A. Hata and D. Wolf, "Road marking detection using LIDAR reflective intensity data and its application to vehicle localization", <em>Proc. 17th Int. IEEE Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 584-589, Oct. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6957753"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6957753"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2360KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Road+marking+detection+using+LIDAR+reflective+intensity+data+and+its+application+to+vehicle+localization&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref99"><b _ngcontent-eae-c168="">99.</b></span><span _ngcontent-eae-c168="">T. Ort, L. Paull and D. Rus, "Autonomous vehicle navigation in rural environments without detailed prior maps", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, pp. 2040-2047, May 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8460519"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8460519"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1723KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+vehicle+navigation+in+rural+environments+without+detailed+prior+maps&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref100"><b _ngcontent-eae-c168="">100.</b></span><span _ngcontent-eae-c168="">J. Levinson and S. Thrun, "Robust vehicle localization in urban environments using probabilistic maps", <em>Proc. IEEE Int. Conf. Robot. Autom.</em>, pp. 4372-4378, May 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5509700"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5509700"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1960KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robust+vehicle+localization+in+urban+environments+using+probabilistic+maps&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref101"><b _ngcontent-eae-c168="">101.</b></span><span _ngcontent-eae-c168="">E. Takeuchi and T. Tsubouchi, "A 3-D scan matching using improved 3-D normal distributions transform for mobile robotic mapping", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 3068-3073, Oct. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4058864"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4058864"> Full Text: PDF </a><span _ngcontent-eae-c168="">(996KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+3-D+scan+matching+using+improved+3-D+normal+distributions+transform+for+mobile+robotic+mapping&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref102"><b _ngcontent-eae-c168="">102.</b></span><span _ngcontent-eae-c168="">S. E. Shladover, "PATH at 20—History and major milestones", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 8, no. 4, pp. 584-592, Dec. 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4358931"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4358931"> Full Text: PDF </a><span _ngcontent-eae-c168="">(547KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PATH+at+20%E2%80%94History+and+major+milestones&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref103"><b _ngcontent-eae-c168="">103.</b></span><span _ngcontent-eae-c168="">A. Alam, B. Besselink, V. Turri, J. MåRtensson and K. H. Johansson, "Heavy-duty vehicle platooning for sustainable freight transportation: A cooperative method to enhance safety and efficiency", <em>IEEE Control Syst. Mag.</em>, vol. 35, no. 6, pp. 34-56, Dec. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7286902"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7286902"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4682KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Heavy-duty+vehicle+platooning+for+sustainable+freight+transportation%3A+A+cooperative+method+to+enhance+safety+and+efficiency&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref104"><b _ngcontent-eae-c168="">104.</b></span><span _ngcontent-eae-c168="">C. Bergenhem, S. Shladover, E. Coelingh, C. Englund, S. Shladover and S. Tsugawa, "Overview of platooning systems", <em>Proc. 19th ITS World Congr.</em>, pp. 1-8, Oct. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Overview+of+platooning+systems&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref105"><b _ngcontent-eae-c168="">105.</b></span><span _ngcontent-eae-c168="">E. Chan, "Sartre automated platooning vehicles", <em>Towards Innov. Freight Logistics</em>, vol. 2, pp. 137-150, May 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1002/9781119307785.ch10"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sartre+automated+platooning+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref106"><b _ngcontent-eae-c168="">106.</b></span><span _ngcontent-eae-c168="">A. Keymasi Khalaji and S. A. A. Moosavian, "Robust adaptive controller for a tractor-trailer mobile robot", <em>IEEE/ASME Trans. Mechatronics</em>, vol. 19, no. 3, pp. 943-953, Jun. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6524048"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6524048"> Full Text: PDF </a><span _ngcontent-eae-c168="">(806KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robust+adaptive+controller+for+a+tractor-trailer+mobile+robot&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref107"><b _ngcontent-eae-c168="">107.</b></span><span _ngcontent-eae-c168="">J. Cheng, B. Wang and Y. Xu, "Backward path tracking control for mobile robot with three trailers", <em>Proc. Int. Conf. Neural Inf. Process.</em>, pp. 32-41, Nov. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-70136-3_4"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Backward+path+tracking+control+for+mobile+robot+with+three+trailers&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref108"><b _ngcontent-eae-c168="">108.</b></span><span _ngcontent-eae-c168="">M. Hejase, J. Jing, J. M. Maroli, Y. Bin Salamah, L. Fiorentini and U. Ozguner, "Constrained backward path tracking control using a plug-in jackknife prevention system for autonomous tractor-trailers", <em>Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 2012-2017, Nov. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8569262"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8569262"> Full Text: PDF </a><span _ngcontent-eae-c168="">(842KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Constrained+backward+path+tracking+control+using+a+plug-in+jackknife+prevention+system+for+autonomous+tractor-trailers&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref109"><b _ngcontent-eae-c168="">109.</b></span><span _ngcontent-eae-c168="">F. Zhang, H. Stahle, G. Chen, C. C. C. Simon, C. Buckl and A. Knoll, "A sensor fusion approach for localization with cumulative error elimination", <em>Proc. IEEE Int. Conf. Multisensor Fusion Integr. for Intell. Syst. (MFI)</em>, pp. 1-6, Sep. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6343009"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6343009"> Full Text: PDF </a><span _ngcontent-eae-c168="">(253KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+sensor+fusion+approach+for+localization+with+cumulative+error+elimination&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref110"><b _ngcontent-eae-c168="">110.</b></span><span _ngcontent-eae-c168="">W.-W. Kao, "Integration of GPS and dead-reckoning navigation systems", <em>Proc. Vehicle Navigat. Inf. Syst. Conf.</em>, vol. 2, pp. 635-643, Oct. 1991.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/1623672"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1623672"> Full Text: PDF </a><span _ngcontent-eae-c168="">(804KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Integration+of+GPS+and+dead-reckoning+navigation+systems&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref111"><b _ngcontent-eae-c168="">111.</b></span><span _ngcontent-eae-c168="">J. Levinson, M. Montemerlo and S. Thrun, "Map-based precision vehicle localization in urban environments" in Robotics: Science and Systems III, Cambridge, MA, USA:MIT Press, pp. 4372-4378, 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.15607/RSS.2007.III.016"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Map-based+precision+vehicle+localization+in+urban+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref112"><b _ngcontent-eae-c168="">112.</b></span><span _ngcontent-eae-c168="">A. Ranganathan, D. Ilstrup and T. Wu, "Light-weight localization for vehicles using road markings", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 921-927, Nov. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6696460"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6696460"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1602KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Light-weight+localization+for+vehicles+using+road+markings&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref113"><b _ngcontent-eae-c168="">113.</b></span><span _ngcontent-eae-c168="">J. Leonard, J. How, S. Teller, M. Berger, S. Campbell, G. Fiore, et al., "A perception-driven autonomous urban vehicle", <em>J. Field Robot.</em>, vol. 25, no. 10, pp. 727-774, 2008.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1002/rob.20262"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+perception-driven+autonomous+urban+vehicle&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref114"><b _ngcontent-eae-c168="">114.</b></span><span _ngcontent-eae-c168="">N. Akai, L. Y. Morales, E. Takeuchi, Y. Yoshihara and Y. Ninomiya, "Robust localization using 3D NDT scan matching with experimentally determined uncertainty and road marker matching", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 1356-1363, Jun. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7995900"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7995900"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1164KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robust+localization+using+3D+NDT+scan+matching+with+experimentally+determined+uncertainty+and+road+marker+matching&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref115"><b _ngcontent-eae-c168="">115.</b></span><span _ngcontent-eae-c168="">J. K. Suhr, J. Jang, D. Min and H. G. Jung, "Sensor fusion-based low-cost vehicle localization system for complex urban environments", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 18, no. 5, pp. 1078-1086, May 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7547970"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7547970"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1702KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sensor+fusion-based+low-cost+vehicle+localization+system+for+complex+urban+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref116"><b _ngcontent-eae-c168="">116.</b></span><span _ngcontent-eae-c168="">D. Gruyer, R. Belaroussi and M. Revilloud, "Accurate lateral positioning from map data and road marking detection", <em>Expert Syst. Appl.</em>, vol. 43, pp. 1-8, Jan. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.eswa.2015.08.015"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Accurate+lateral+positioning+from+map+data+and+road+marking+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref117"><b _ngcontent-eae-c168="">117.</b></span><span _ngcontent-eae-c168="">X. Qu, B. Soheilian and N. Paparoditis, "Vehicle localization using mono-camera and geo-referenced traffic signs", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 605-610, Jun. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7225751"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7225751"> Full Text: PDF </a><span _ngcontent-eae-c168="">(361KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Vehicle+localization+using+mono-camera+and+geo-referenced+traffic+signs&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref118"><b _ngcontent-eae-c168="">118.</b></span><span _ngcontent-eae-c168="">M. Magnusson, "The three-dimensional normal-distributions transform—An efficient representation for registration surface analysis and loop detection", 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+three-dimensional+normal-distributions+transform%E2%80%94An+efficient+representation+for+registration%2C+surface+analysis%2C+and+loop+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref119"><b _ngcontent-eae-c168="">119.</b></span><span _ngcontent-eae-c168="">R. W. Wolcott and R. M. Eustice, "Fast LIDAR localization using multiresolution Gaussian mixture maps", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, pp. 2814-2821, May 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7139582"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7139582"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2439KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Fast+LIDAR+localization+using+multiresolution+Gaussian+mixture+maps&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref120"><b _ngcontent-eae-c168="">120.</b></span><span _ngcontent-eae-c168="">M. Magnusson, A. Nuchter, C. Lorken, A. J. Lilienthal and J. Hertzberg, "Evaluation of 3D registration reliability and speed—A comparison of ICP and NDT", <em>Proc. IEEE Int. Conf. Robot. Autom.</em>, pp. 3907-3912, May 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5152538"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5152538"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1168KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Evaluation+of+3D+registration+reliability+and+speed%E2%80%94A+comparison+of+ICP+and+NDT&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref121"><b _ngcontent-eae-c168="">121.</b></span><span _ngcontent-eae-c168="">R. Valencia, J. Saarinen, H. Andreasson, J. Vallvé, J. Andrade-Cetto and A. J. Lilienthal, "Localization in highly dynamic environments using dual-timescale NDT-MCL", <em>Proc. IEEE Int. Conf. Robot. Automat. (ICRA)</em>, pp. 3956-3962, May/Jun. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6907433"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6907433"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1638KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Localization+in+highly+dynamic+environments+using+dual-timescale+NDT-MCL&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref122"><b _ngcontent-eae-c168="">122.</b></span><span _ngcontent-eae-c168="">S. Kato, E. Takeuchi, Y. Ishiguro, Y. Ninomiya, K. Takeda and T. Hamada, "An open approach to autonomous vehicles", <em>IEEE Micro</em>, vol. 35, no. 6, pp. 60-68, Dec. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7368032"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7368032"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3565KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=An+open+approach+to+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref123"><b _ngcontent-eae-c168="">123.</b></span><span _ngcontent-eae-c168="">R. W. Wolcott and R. M. Eustice, "Visual localization within LIDAR maps for automated urban driving", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 176-183, Sep. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6942558"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6942558"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4381KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Visual+localization+within+LIDAR+maps+for+automated+urban+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref124"><b _ngcontent-eae-c168="">124.</b></span><span _ngcontent-eae-c168="">C. Mcmanus, W. Churchill, A. Napier, B. Davis and P. Newman, "Distraction suppression for vision-based pose estimation at city scales", <em>Proc. IEEE Int. Conf. Robot. Autom.</em>, pp. 3762-3769, May 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6631106"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6631106"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2422KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Distraction+suppression+for+vision-based+pose+estimation+at+city+scales&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref125"><b _ngcontent-eae-c168="">125.</b></span><span _ngcontent-eae-c168="">C. Szegedy, S. Ioffe, V. Vanhoucke and A. Alemi, "Inception-v4 inception-ResNet and the impact of residual connections on learning" in arXiv:1602.07261, Feb. 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1602.07261" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1602</span><span>.</span><span>07261</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Inception-v4%2C+inception-ResNet+and+the+impact+of+residual+connections+on+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref126"><b _ngcontent-eae-c168="">126.</b></span><span _ngcontent-eae-c168="">K. He, X. Zhang, S. Ren and J. Sun, "Deep residual learning for image recognition", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 770-778, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7780459"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780459"> Full Text: PDF </a><span _ngcontent-eae-c168="">(282KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Deep+residual+learning+for+image+recognition&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref127"><b _ngcontent-eae-c168="">127.</b></span><span _ngcontent-eae-c168="">G. Huang, Z. Liu, L. van der Maaten and K. Q. Weinberger, "Densely connected convolutional networks" in arXiv:1608.06993, Aug. 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1608.06993" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1608</span><span>.</span><span>06993</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Densely+connected+convolutional+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref128"><b _ngcontent-eae-c168="">128.</b></span><span _ngcontent-eae-c168="">J. Redmon and A. Farhadi, "YOLOv3: An incremental improvement" in arXiv:1804.02767, Apr. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1804.02767" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1804</span><span>.</span><span>02767</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=YOLOv3%3A+An+incremental+improvement&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref129"><b _ngcontent-eae-c168="">129.</b></span><span _ngcontent-eae-c168="">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, et al., "Going deeper with convolutions", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 1-9, Jun. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7298594"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7298594"> Full Text: PDF </a><span _ngcontent-eae-c168="">(215KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Going+deeper+with+convolutions&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref130"><b _ngcontent-eae-c168="">130.</b></span><span _ngcontent-eae-c168="">K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition" in arXiv:1409.1556, Apr. 2015,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1409.1556" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1409</span><span>.</span><span>1556</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Very+deep+convolutional+networks+for+large-scale+image+recognition&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref131"><b _ngcontent-eae-c168="">131.</b></span><span _ngcontent-eae-c168="">J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei, "ImageNet: A large-scale hierarchical image database", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, pp. 248-255, Jun. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5206848"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5206848"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3548KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=ImageNet%3A+A+large-scale+hierarchical+image+database&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref132"><b _ngcontent-eae-c168="">132.</b></span><span _ngcontent-eae-c168="">A. Andreopoulos and J. K. Tsotsos, "50 years of object recognition: Directions forward", <em>Comput. Vis. Image Understand.</em>, vol. 117, no. 8, pp. 827-891, Aug. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.cviu.2013.04.005"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=50+years+of+object+recognition%3A+Directions+forward&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref133"><b _ngcontent-eae-c168="">133.</b></span><span _ngcontent-eae-c168="">Z.-Q. Zhao, P. Zheng, S.-T. Xu and X. Wu, "Object detection with deep learning: A review", <em>IEEE Trans. Neural Netw. Learn. Syst.</em>, vol. 30, no. 11, pp. 3212-3232, Nov. 2019.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8627998"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8627998"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4767KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Object+detection+with+deep+learning%3A+A+review&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref134"><b _ngcontent-eae-c168="">134.</b></span><span _ngcontent-eae-c168="">L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, et al., "Deep learning for generic object detection: A survey" in arXiv:1809.02165, Sep. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1809.02165" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1809</span><span>.</span><span>02165</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Deep+learning+for+generic+object+detection%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref135"><b _ngcontent-eae-c168="">135.</b></span><span _ngcontent-eae-c168="">J. Redmon, S. Divvala, R. Girshick and A. Farhadi, "You only look once: Unified real-time object detection", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 779-788, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7780460"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780460"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1742KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=You+only+look+once%3A+Unified%2C+real-time+object+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref136"><b _ngcontent-eae-c168="">136.</b></span><span _ngcontent-eae-c168="">J. Redmon and A. Farhadi, "YOLO9000: Better faster stronger", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 6517-6525, Jul. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8100173"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8100173"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3364KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=YOLO9000%3A+Better%2C+faster%2C+stronger&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref137"><b _ngcontent-eae-c168="">137.</b></span><span _ngcontent-eae-c168="">W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, et al., "SSD: Single shot MultiBox detector" in arXiv:1512.02325, Dec. 2015,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1512.02325" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1512</span><span>.</span><span>02325</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=SSD%3A+Single+shot+MultiBox+detector&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref138"><b _ngcontent-eae-c168="">138.</b></span><span _ngcontent-eae-c168="">K. He, G. Gkioxari, P. Dollár and R. Girshick, "Mask R-CNN", <em>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, pp. 2980-2988, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8237584"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237584"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3278KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Mask+R-CNN&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref139"><b _ngcontent-eae-c168="">139.</b></span><span _ngcontent-eae-c168="">L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff and H. Adam, "Encoder-decoder with atrous separable convolution for semantic image segmentation" in arXiv:1802.02611, Feb. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1802.02611" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1802</span><span>.</span><span>02611</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-030-01234-2_49"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Encoder-decoder+with+atrous+separable+convolution+for+semantic+image+segmentation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref140"><b _ngcontent-eae-c168="">140.</b></span><span _ngcontent-eae-c168="">Y. Yan, Y. Mao and B. Li, "SECOND: Sparsely embedded convolutional detection", <em>Sensors</em>, vol. 18, no. 10, pp. 3337, Oct. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/s18103337"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=SECOND%3A+Sparsely+embedded+convolutional+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref141"><b _ngcontent-eae-c168="">141.</b></span><span _ngcontent-eae-c168="">C. Geyer and K. Daniilidis, "A unifying theory for central panoramic systems and practical implications" in Computer Vision—ECCV, Berlin, Germany:Springer, pp. 445-461, 2000.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/3-540-45053-X_29"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+unifying+theory+for+central+panoramic+systems+and+practical+implications&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref142"><b _ngcontent-eae-c168="">142.</b></span><span _ngcontent-eae-c168="">D. Scaramuzza, A. Martinelli and R. Siegwart, "A toolbox for easily calibrating omnidirectional cameras", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 5695-5701, Oct. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4059340"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4059340"> Full Text: PDF </a><span _ngcontent-eae-c168="">(8296KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+toolbox+for+easily+calibrating+omnidirectional+cameras&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref143"><b _ngcontent-eae-c168="">143.</b></span><span _ngcontent-eae-c168="">D. Scaramuzza and R. Siegwart, "Appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicles", <em>IEEE Trans. Robot.</em>, vol. 24, no. 5, pp. 1015-1026, Oct. 2008.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4625958"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4625958"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1138KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Appearance-guided+monocular+omnidirectional+visual+odometry+for+outdoor+ground+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref144"><b _ngcontent-eae-c168="">144.</b></span><span _ngcontent-eae-c168="">M. Schonbein and A. Geiger, "Omnidirectional 3D reconstruction in augmented manhattan worlds", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 716-723, Sep. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6942637"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6942637"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4532KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Omnidirectional+3D+reconstruction+in+augmented+manhattan+worlds&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref145"><b _ngcontent-eae-c168="">145.</b></span><span _ngcontent-eae-c168="">G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, et al., "Event-based vision: A survey" in arXiv:1904.08405, Apr. 2019,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1904.08405" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1904</span><span>.</span><span>08405</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Event-based+vision%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref146"><b _ngcontent-eae-c168="">146.</b></span><span _ngcontent-eae-c168="">R. H. Rasshofer and K. Gresser, "Automotive radar and lidar systems for next generation driver assistance functions", <em>Adv. Radio Sci.</em>, vol. 3, pp. 205-209, May 2005.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.5194/ars-3-205-2005"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Automotive+radar+and+lidar+systems+for+next+generation+driver+assistance+functions&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref147"><b _ngcontent-eae-c168="">147.</b></span><span _ngcontent-eae-c168="">P. Radecki, M. Campbell and K. Matzen, "All weather perception: Joint data association tracking and classification for autonomous ground vehicles" in arXiv:1605.02196, May 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1605.02196" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1605</span><span>.</span><span>02196</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=All+weather+perception%3A+Joint+data+association%2C+tracking%2C+and+classification+for+autonomous+ground+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref148"><b _ngcontent-eae-c168="">148.</b></span><span _ngcontent-eae-c168="">P. Hurney, F. Morgan, M. Glavin, E. Jones and P. Waldron, "Review of pedestrian detection techniques in automotive far-infrared video", <em>IET Intell. Transp. Syst.</em>, vol. 9, no. 8, pp. 824-832, Oct. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1049/iet-its.2014.0236"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Review+of+pedestrian+detection+techniques+in+automotive+far-infrared+video&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref149"><b _ngcontent-eae-c168="">149.</b></span><span _ngcontent-eae-c168="">N. Carlevaris-Bianco and R. M. Eustice, "Learning visual feature descriptors for dynamic lighting conditions", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.</em>, pp. 2769-2776, Sep. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6942941"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6942941"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2831KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+visual+feature+descriptors+for+dynamic+lighting+conditions&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref150"><b _ngcontent-eae-c168="">150.</b></span><span _ngcontent-eae-c168="">V. Peretroukhin, W. Vega-Brown, N. Roy and J. Kelly, "PROBE-GK: Predictive robust estimation using generalized kernels", <em>Proc. IEEE Int. Conf. Robot. Automat. (ICRA)</em>, pp. 817-824, May 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7487212"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7487212"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2527KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PROBE-GK%3A+Predictive+robust+estimation+using+generalized+kernels&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref151"><b _ngcontent-eae-c168="">151.</b></span><span _ngcontent-eae-c168="">W. Maddern, A. Stewart, C. McManus, B. Upcroft, W. Churchill and P. Newman, "Illumination invariant imaging: Applications in robust vision-based localisation mapping and classification for autonomous vehicles", <em>Proc. Vis. Place Recognit. Changing Environ. Workshop IEEE Int. Conf. Robot. Automat. (ICRA)</em>, vol. 2, pp. 3, May 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Illumination+invariant+imaging%3A+Applications+in+robust+vision-based+localisation%2C+mapping+and+classification+for+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref152"><b _ngcontent-eae-c168="">152.</b></span><span _ngcontent-eae-c168="">T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, et al., "Microsoft COCO: Common Objects in Context" in arXiv:1405.0312, May 2014,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1405.0312" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1405</span><span>.</span><span>0312</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-10602-1_48"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Microsoft+COCO%3A+Common+Objects+in+Context&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref153"><b _ngcontent-eae-c168="">153.</b></span><span _ngcontent-eae-c168="">S. Ren, K. He, R. Girshick and J. Sun, "Faster R-CNN: Towards real-time object detection with region proposal networks" in arXiv:1506.01497, Jun. 2015,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1506.01497" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1506</span><span>.</span><span>01497</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Faster+R-CNN%3A+Towards+real-time+object+detection+with+region+proposal+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref154"><b _ngcontent-eae-c168="">154.</b></span><span _ngcontent-eae-c168="">H. Noh, S. Hong and B. Han, "Learning deconvolution network for semantic segmentation", <em>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, pp. 1520-1528, Dec. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7410535"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7410535"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1017KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+deconvolution+network+for+semantic+segmentation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref155"><b _ngcontent-eae-c168="">155.</b></span><span _ngcontent-eae-c168="">O. Ronneberger, P. Fischer and T. Brox, "U-Net: Convolutional networks for biomedical image segmentation" in arXiv:1505.04597, May 2015,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1505.04597" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1505</span><span>.</span><span>04597</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-24574-4_28"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=U-Net%3A+Convolutional+networks+for+biomedical+image+segmentation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref156"><b _ngcontent-eae-c168="">156.</b></span><span _ngcontent-eae-c168="">H. Zhao, J. Shi, X. Qi, X. Wang and J. Jia, "Pyramid scene parsing network" in arXiv:1612.01105, Dec. 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1612.01105" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1612</span><span>.</span><span>01105</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Pyramid+scene+parsing+network&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref157"><b _ngcontent-eae-c168="">157.</b></span><span _ngcontent-eae-c168="">L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A. L. Yuille, "DeepLab: Semantic image segmentation with deep convolutional nets atrous convolution and fully connected CRFs" in arXiv:1606.00915, Jun. 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1606.00915" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1606</span><span>.</span><span>00915</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=DeepLab%3A+Semantic+image+segmentation+with+deep+convolutional+nets%2C+atrous+convolution%2C+and+fully+connected+CRFs&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref158"><b _ngcontent-eae-c168="">158.</b></span><span _ngcontent-eae-c168="">X. Ma, Z. Wang, H. Li, W. Ouyang, X. Fan and P. Zhang, "Accurate monocular object detection via color-embedded 3D reconstruction for autonomous driving" in arXiv:1903.11444, Mar. 2019,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1903.11444" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1903</span><span>.</span><span>11444</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/9009489"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9009489"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4486KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Accurate+monocular+object+detection+via+color-embedded+3D+reconstruction+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref159"><b _ngcontent-eae-c168="">159.</b></span><span _ngcontent-eae-c168="">X. Cheng, P. Wang and R. Yang, "Learning depth with convolutional spatial propagation network" in arXiv:1810.02695, 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1810.02695" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1810</span><span>.</span><span>02695</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+depth+with+convolutional+spatial+propagation+network&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref160"><b _ngcontent-eae-c168="">160.</b></span><span _ngcontent-eae-c168="">R. B. Rusu, "Semantic 3D object maps for everyday manipulation in human living environments", <em>KI–Künstliche Intelligenz</em>, vol. 24, pp. 345-348, Oct. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s13218-010-0059-6"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Semantic+3D+object+maps+for+everyday+manipulation+in+human+living+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref161"><b _ngcontent-eae-c168="">161.</b></span><span _ngcontent-eae-c168="">W. Wang, K. Sakurada and N. Kawaguchi, "Incremental and enhanced scanline-based segmentation method for surface reconstruction of sparse LiDAR data", <em>Remote Sens.</em>, vol. 8, no. 11, pp. 967, 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/rs8110967"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Incremental+and+enhanced+scanline-based+segmentation+method+for+surface+reconstruction+of+sparse+LiDAR+data&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref162"><b _ngcontent-eae-c168="">162.</b></span><span _ngcontent-eae-c168="">P. Narksri, E. Takeuchi, Y. Ninomiya, Y. Morales, N. Akai and N. Kawaguchi, "A slope-robust cascaded ground segmentation in 3D point cloud for autonomous vehicles", <em>Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 497-504, Nov. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8569534"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8569534"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3074KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+slope-robust+cascaded+ground+segmentation+in+3D+point+cloud+for+autonomous+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref163"><b _ngcontent-eae-c168="">163.</b></span><span _ngcontent-eae-c168="">J. Lambert, L. Liang, Y. Morales, N. Akai, A. Carballo, E. Takeuchi, et al., "Tsukuba challenge 2017 dynamic object tracks dataset for pedestrian behavior analysis", <em>J. Robot. Mechtron.</em>, vol. 30, no. 4, pp. 598-612, Aug. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.20965/jrm.2018.p0598"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Tsukuba+challenge+2017+dynamic+object+tracks+dataset+for+pedestrian+behavior+analysis&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref164"><b _ngcontent-eae-c168="">164.</b></span><span _ngcontent-eae-c168="">A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez and V. Koltun, "CARLA: An open urban driving simulator" in arXiv:1711.03938, 2017,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1711.03938" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1711</span><span>.</span><span>03938</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=CARLA%3A+An+open+urban+driving+simulator&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref165"><b _ngcontent-eae-c168="">165.</b></span><span _ngcontent-eae-c168="">S. Song and J. Xiao, "Sliding shapes for 3D object detection in depth images", <em>Proc. Eur. Conf. Comput. Vis. (ECCV)</em>, pp. 634-651, 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-10599-4_41"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sliding+shapes+for+3D+object+detection+in+depth+images&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref166"><b _ngcontent-eae-c168="">166.</b></span><span _ngcontent-eae-c168="">D. Z. Wang and I. Posner, "Voting for voting in online point cloud object detection", <em>Proc. Robot. Sci. Syst.</em>, pp. 1-9, Jul. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.15607/RSS.2015.XI.035"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Voting+for+voting+in+online+point+cloud+object+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref167"><b _ngcontent-eae-c168="">167.</b></span><span _ngcontent-eae-c168="">Y. Zhou and O. Tuzel, "VoxelNet: End-to-end learning for point cloud based 3D object detection" in arXiv:1711.06396, Nov. 2017,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1711.06396" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1711</span><span>.</span><span>06396</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=VoxelNet%3A+End-to-end+learning+for+point+cloud+based+3D+object+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref168"><b _ngcontent-eae-c168="">168.</b></span><span _ngcontent-eae-c168="">X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, et al., "3D object proposals for accurate object class detection" in Advances in Neural Information Processing Systems 28, New York, NY, USA:Curran Associates, Inc, pp. 424-432, 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=3D+object+proposals+for+accurate+object+class+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref169"><b _ngcontent-eae-c168="">169.</b></span><span _ngcontent-eae-c168="">D. Lin, S. Fidler and R. Urtasun, "Holistic scene understanding for 3D object detection with RGBD cameras", <em>Proc. IEEE Int. Conf. Comput. Vis.</em>, pp. 1417-1424, Dec. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6751286"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6751286"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1727KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Holistic+scene+understanding+for+3D+object+detection+with+RGBD+cameras&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref170"><b _ngcontent-eae-c168="">170.</b></span><span _ngcontent-eae-c168="">B. Li, T. Zhang and T. Xia, "Vehicle detection from 3D lidar using fully convolutional network", <em>Proc. Robot. Sci. Syst.</em>, pp. 1-8, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.15607/RSS.2016.XII.042"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Vehicle+detection+from+3D+lidar+using+fully+convolutional+network&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref171"><b _ngcontent-eae-c168="">171.</b></span><span _ngcontent-eae-c168="">L. Liu, Z. Pan and B. Lei, "Learning a rotation invariant detector with rotatable bounding box" in arXiv:1711.09405, Nov. 2017,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1711.09405" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1711</span><span>.</span><span>09405</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+a+rotation+invariant+detector+with+rotatable+bounding+box&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref172"><b _ngcontent-eae-c168="">172.</b></span><span _ngcontent-eae-c168="">X. Chen, H. Ma, J. Wan, B. Li and T. Xia, "Multi-view 3D object detection network for autonomous driving", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 6526-6534, Jul. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8100174"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8100174"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1447KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Multi-view+3D+object+detection+network+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref173"><b _ngcontent-eae-c168="">173.</b></span><span _ngcontent-eae-c168="">M. Ren, A. Pokrovsky, B. Yang and R. Urtasun, "SBNet: Sparse blocks network for fast inference", <em>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</em>, pp. 8711-8720, Jun. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8579006"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8579006"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1201KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=SBNet%3A+Sparse+blocks+network+for+fast+inference&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref174"><b _ngcontent-eae-c168="">174.</b></span><span _ngcontent-eae-c168="">W. Ali, S. Abdelkarim, M. Zahran, M. Zidan and A. El Sallab, "YOLO3D: End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud" in arXiv:1808.02350, Aug. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1808.02350" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1808</span><span>.</span><span>02350</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=YOLO3D%3A+End-to-end+real-time+3D+oriented+object+bounding+box+detection+from+LiDAR+point+cloud&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref175"><b _ngcontent-eae-c168="">175.</b></span><span _ngcontent-eae-c168="">B. Yang, W. Luo and R. Urtasun, "PIXOR: Real-time 3D object detection from point clouds", <em>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</em>, pp. 7652-7660, Jun. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8578896"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8578896"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1307KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PIXOR%3A+Real-time+3D+object+detection+from+point+clouds&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref176"><b _ngcontent-eae-c168="">176.</b></span><span _ngcontent-eae-c168="">D. Feng, L. Rosenbaum and K. Dietmayer, "Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3D vehicle detection" in arXiv:1804.05132, Apr. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1804.05132" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1804</span><span>.</span><span>05132</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8569814"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8569814"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2314KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Towards+safe+autonomous+driving%3A+Capture+uncertainty+in+the+deep+neural+network+for+lidar+3D+vehicle+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref177"><b _ngcontent-eae-c168="">177.</b></span><span _ngcontent-eae-c168="">A. Geiger, P. Lenz and R. Urtasun, "Are we ready for autonomous driving? The KITTI vision benchmark suite", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, pp. 3354-3361, Jun. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6248074"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6248074"> Full Text: PDF </a><span _ngcontent-eae-c168="">(339KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Are+we+ready+for+autonomous+driving%3F+The+KITTI+vision+benchmark+suite&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref178"><b _ngcontent-eae-c168="">178.</b></span><span _ngcontent-eae-c168="">H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. Erin Liong, Q. Xu, et al., "NuScenes: A multimodal dataset for autonomous driving" in arXiv:1903.11027, 2019,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1903.11027" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1903</span><span>.</span><span>11027</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=NuScenes%3A+A+multimodal+dataset+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref179"><b _ngcontent-eae-c168="">179.</b></span><span _ngcontent-eae-c168="">S. Shi, X. Wang and H. Li, "PointRCNN: 3D object proposal generation and detection from point cloud" in arXiv:1812.04244, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1812.04244" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1812</span><span>.</span><span>04244</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PointRCNN%3A+3D+object+proposal+generation+and+detection+from+point+cloud&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref180"><b _ngcontent-eae-c168="">180.</b></span><span _ngcontent-eae-c168="">A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang and O. Beijbom, "PointPillars: Fast encoders for object detection from point clouds" in arXiv:1812.05784, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1812.05784" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1812</span><span>.</span><span>05784</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PointPillars%3A+Fast+encoders+for+object+detection+from+point+clouds&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref181"><b _ngcontent-eae-c168="">181.</b></span><span _ngcontent-eae-c168="">Z. Yang, Y. Sun, S. Liu, X. Shen and J. Jia, "IPOD: Intensive point-based object detector for point cloud" in arXiv:1812.05276, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1812.05276" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1812</span><span>.</span><span>05276</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=IPOD%3A+Intensive+point-based+object+detector+for+point+cloud&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref182"><b _ngcontent-eae-c168="">182.</b></span><span _ngcontent-eae-c168="">C. R. Qi, W. Liu, C. Wu, H. Su and L. J. Guibas, "Frustum PointNets for 3D object detection from RGB-D data" in arXiv:1711.08488, Nov. 2017,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1711.08488" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1711</span><span>.</span><span>08488</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Frustum+PointNets+for+3D+object+detection+from+RGB-D+data&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref183"><b _ngcontent-eae-c168="">183.</b></span><span _ngcontent-eae-c168="">W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, X. Zhao, et al., "Multiple object tracking: A literature review" in arXiv:1409.7618, Sep. 2014,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1409.7618" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1409</span><span>.</span><span>7618</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Multiple+object+tracking%3A+A+literature+review&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref184"><b _ngcontent-eae-c168="">184.</b></span><span _ngcontent-eae-c168="">A. Azim and O. Aycard, "Detection classification and tracking of moving objects in a 3D environment", <em>Proc. IEEE Intell. Vehicles Symp.</em>, pp. 802-807, Jun. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6232303"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6232303"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1761KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Detection%2C+classification+and+tracking+of+moving+objects+in+a+3D+environment&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref185"><b _ngcontent-eae-c168="">185.</b></span><span _ngcontent-eae-c168="">J. Shi and Tomasi, "Good features to track", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 593-600, Jun. 1994.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Good+features+to+track&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref186"><b _ngcontent-eae-c168="">186.</b></span><span _ngcontent-eae-c168="">M.-P. Dubuisson and A. K. Jain, "A modified Hausdorff distance for object matching", <em>Proc. 12th Int. Conf. Pattern Recognit.</em>, vol. 1, pp. 566-568, Oct. 1994.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/576361"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=576361"> Full Text: PDF </a><span _ngcontent-eae-c168="">(289KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+modified+Hausdorff+distance+for+object+matching&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref187"><b _ngcontent-eae-c168="">187.</b></span><span _ngcontent-eae-c168="">S. Hwang, N. Kim, Y. Choi, S. Lee and I. S. Kweon, "Fast multiple objects detection and tracking fusing color camera and 3D LIDAR for intelligent vehicles", <em>Proc. 13th Int. Conf. Ubiquitous Robots Ambient Intell. (URAI)</em>, pp. 234-239, Aug. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7625744"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7625744"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1406KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Fast+multiple+objects+detection+and+tracking+fusing+color+camera+and+3D+LIDAR+for+intelligent+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref188"><b _ngcontent-eae-c168="">188.</b></span><span _ngcontent-eae-c168="">T.-N. Nguyen, B. Michaelis, A. Al-Hamadi, M. Tornow and M.-M. Meinecke, "Stereo-camera-based urban environment perception using occupancy grid and object tracking", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 13, no. 1, pp. 154-165, Mar. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6021374"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6021374"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1227KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Stereo-camera-based+urban+environment+perception+using+occupancy+grid+and+object+tracking&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref189"><b _ngcontent-eae-c168="">189.</b></span><span _ngcontent-eae-c168="">J. Ziegler, P. Bender, T. Dang and C. Stiller, "Trajectory planning for bertha—A local continuous method", <em>Proc. IEEE Intell. Vehicles Symp. Proc.</em>, pp. 450-457, Jun. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6856581"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6856581"> Full Text: PDF </a><span _ngcontent-eae-c168="">(590KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Trajectory+planning+for+bertha%E2%80%94A+local%2C+continuous+method&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref190"><b _ngcontent-eae-c168="">190.</b></span><span _ngcontent-eae-c168="">A. Ess, K. Schindler, B. Leibe and L. Van Gool, "Object detection and tracking for autonomous navigation in dynamic environments", <em>Int. J. Robot. Res.</em>, vol. 29, no. 14, pp. 1707-1725, Dec. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0278364910365417"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Object+detection+and+tracking+for+autonomous+navigation+in+dynamic+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref191"><b _ngcontent-eae-c168="">191.</b></span><span _ngcontent-eae-c168="">A. Petrovskaya and S. Thrun, "Model based vehicle detection and tracking for autonomous urban driving", <em>Auto. Robots</em>, vol. 26, no. 2, pp. 123-139, Apr. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s10514-009-9115-1"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Model+based+vehicle+detection+and+tracking+for+autonomous+urban+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref192"><b _ngcontent-eae-c168="">192.</b></span><span _ngcontent-eae-c168="">M. He, E. Takeuchi, Y. Ninomiya and S. Kato, "Precise and efficient model-based vehicle tracking method using Rao-Blackwellized and scaling series particle filters", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</em>, pp. 117-124, Oct. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7759043"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7759043"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2536KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Precise+and+efficient+model-based+vehicle+tracking+method+using+Rao-Blackwellized+and+scaling+series+particle+filters&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref193"><b _ngcontent-eae-c168="">193.</b></span><span _ngcontent-eae-c168="">D. Z. Wang, I. Posner and P. Newman, "Model-free detection and tracking of dynamic objects with 2D lidar", <em>Int. J. Robot. Res.</em>, vol. 34, no. 7, pp. 1039-1063, Jun. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0278364914562237"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Model-free+detection+and+tracking+of+dynamic+objects+with+2D+lidar&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref194"><b _ngcontent-eae-c168="">194.</b></span><span _ngcontent-eae-c168="">B. Huval, T. Wang, S. Tandon, J. Kiske, W. Song, J. Pazhayampallil, et al., "An empirical evaluation of deep learning on highway driving" in arXiv:1504.01716, Apr. 2015,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1504.01716" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1504</span><span>.</span><span>01716</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=An+empirical+evaluation+of+deep+learning+on+highway+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref195"><b _ngcontent-eae-c168="">195.</b></span><span _ngcontent-eae-c168="">D. Held, S. Thrun and S. Savarese, "Learning to track at 100 FPS with deep regression networks" in arXiv:1604.01802, Apr. 2016,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1604.01802" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1604</span><span>.</span><span>01802</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-46448-0_45"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+to+track+at+100+FPS+with+deep+regression+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref196"><b _ngcontent-eae-c168="">196.</b></span><span _ngcontent-eae-c168="">S. Chowdhuri, T. Pankaj and K. Zipser, "MultiNet: Multi-modal multi-task learning for autonomous driving" in arXiv:1709.05581, Sep. 2017,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1709.05581" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1709</span><span>.</span><span>05581</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=MultiNet%3A+Multi-modal+multi-task+learning+for+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref197"><b _ngcontent-eae-c168="">197.</b></span><span _ngcontent-eae-c168="">Jun. 2019,  [online]  Available: <a class="vglnk" href="https://github.com/autowarefoundation/autoware" rel="nofollow"><span>https</span><span>://</span><span>github</span><span>.</span><span>com</span><span>/</span><span>autowarefoundation</span><span>/</span><span>autoware</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref198"><b _ngcontent-eae-c168="">198.</b></span><span _ngcontent-eae-c168="">J. C. McCall and M. M. Trivedi, "Video-based lane estimation and tracking for driver assistance: Survey system and evaluation", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 7, no. 1, pp. 20-37, Mar. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/1603550"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1603550"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1281KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Video-based+lane+estimation+and+tracking+for+driver+assistance%3A+Survey%2C+system%2C+and+evaluation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref199"><b _ngcontent-eae-c168="">199.</b></span><span _ngcontent-eae-c168="">A. Bar Hillel, R. Lerner, D. Levi and G. Raz, "Recent progress in road and lane detection: A survey", <em>Mach. Vis. Appl.</em>, vol. 25, no. 3, pp. 727-745, Apr. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s00138-011-0404-2"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Recent+progress+in+road+and+lane+detection%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref200"><b _ngcontent-eae-c168="">200.</b></span><span _ngcontent-eae-c168="">C. Fernández, D. Fernández-Llorca and M. A. Sotelo, "A hybrid vision-map method for urban road detection", <em>J. Adv. Transp.</em>, vol. 2017, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+hybrid+vision-map+method+for+urban+road+detection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref201"><b _ngcontent-eae-c168="">201.</b></span><span _ngcontent-eae-c168="">R. Labayrade, "A reliable and robust lane detection system based on the parallel use of three algorithms for driving safety assistance", <em>IEICE Trans. Inf. Syst.</em>, vol. E89, no. 7, pp. 2092-2100, Jul. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1093/ietisy/e89-d.7.2092"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+reliable+and+robust+lane+detection+system+based+on+the+parallel+use+of+three+algorithms+for+driving+safety+assistance&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref202"><b _ngcontent-eae-c168="">202.</b></span><span _ngcontent-eae-c168="">Y. Jiang, F. Gao and G. Xu, "Computer vision-based multiple-lane detection on straight road and in a curve", <em>Proc. Int. Conf. Image Anal. Signal Process.</em>, pp. 114-117, Apr. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5476151"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5476151"> Full Text: PDF </a><span _ngcontent-eae-c168="">(442KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Computer+vision-based+multiple-lane+detection+on+straight+road+and+in+a+curve&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref203"><b _ngcontent-eae-c168="">203.</b></span><span _ngcontent-eae-c168="">M. Paton, K. MacTavish, C. J. Ostafew and T. D. Barfoot, "It’s not easy seeing green: Lighting-resistant stereo visual teach &amp; repeat using color-constant images", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, pp. 1519-1526, May 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7139391"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7139391"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1952KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=It%E2%80%99s+not+easy+seeing+green%3A+Lighting-resistant+stereo+visual+teach+%26+repeat+using+color-constant+images&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref204"><b _ngcontent-eae-c168="">204.</b></span><span _ngcontent-eae-c168="">A. S. Huang, D. Moore, M. Antone, E. Olson and S. Teller, "Finding multiple lanes in urban road networks with vision and lidar", <em>Auto. Robots</em>, vol. 26, no. 2, pp. 103-122, Apr. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s10514-009-9113-3"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Finding+multiple+lanes+in+urban+road+networks+with+vision+and+lidar&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref205"><b _ngcontent-eae-c168="">205.</b></span><span _ngcontent-eae-c168="">H.-Y. Cheng, B.-S. Jeng, P.-T. Tseng and K.-C. Fan, "Lane detection with moving vehicles in the traffic scenes", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 7, no. 4, pp. 571-582, Dec. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4019429"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4019429"> Full Text: PDF </a><span _ngcontent-eae-c168="">(845KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Lane+detection+with+moving+vehicles+in+the+traffic+scenes&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref206"><b _ngcontent-eae-c168="">206.</b></span><span _ngcontent-eae-c168="">J. M. Álvarez, A. M. López and R. Baldrich, "Shadow resistant road segmentation from a mobile monocular system" in Pattern Recognition and Image Analysis, Berlin, Germany:Springer, pp. 9-16, 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-540-72849-8_2"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Shadow+resistant+road+segmentation+from+a+mobile+monocular+system&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref207"><b _ngcontent-eae-c168="">207.</b></span><span _ngcontent-eae-c168="">R. Danescu and S. Nedevschi, "Probabilistic lane tracking in difficult road scenarios using stereovision", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 10, no. 2, pp. 272-282, Jun. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4895220"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4895220"> Full Text: PDF </a><span _ngcontent-eae-c168="">(988KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Probabilistic+lane+tracking+in+difficult+road+scenarios+using+stereovision&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref208"><b _ngcontent-eae-c168="">208.</b></span><span _ngcontent-eae-c168="">J. Long, E. Shelhamer and T. Darrell, "Fully convolutional networks for semantic segmentation", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 3431-3440, Jun. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7298965"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7298965"> Full Text: PDF </a><span _ngcontent-eae-c168="">(373KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Fully+convolutional+networks+for+semantic+segmentation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref209"><b _ngcontent-eae-c168="">209.</b></span><span _ngcontent-eae-c168="">A. Borkar, M. Hayes and M. T. Smith, "Robust lane detection and tracking with Ransac and Kalman filter", <em>Proc. 16th IEEE Int. Conf. Image Process. (ICIP)</em>, pp. 3261-3264, Nov. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5413980"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5413980"> Full Text: PDF </a><span _ngcontent-eae-c168="">(884KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robust+lane+detection+and+tracking+with+Ransac+and+Kalman+filter&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref210"><b _ngcontent-eae-c168="">210.</b></span><span _ngcontent-eae-c168="">A. V. Nefian and G. R. Bradski, "Detection of drivable corridors for off-road autonomous navigation", <em>Proc. Int. Conf. Image Process.</em>, pp. 3025-3028, Oct. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4107207"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4107207"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3621KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Detection+of+drivable+corridors+for+off-road+autonomous+navigation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref211"><b _ngcontent-eae-c168="">211.</b></span><span _ngcontent-eae-c168="">E. Yurtsever, Y. Liu, J. Lambert, C. Miyajima, E. Takeuchi, K. Takeda, et al., "Risky action recognition in lane change video clips using deep spatiotemporal networks with segmentation mask transfer", <em>Proc. IEEE Intell. Transp. Syst. Conf. (ITSC)</em>, pp. 3100-3107, Oct. 2019.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8917362"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8917362"> Full Text: PDF </a><span _ngcontent-eae-c168="">(973KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Risky+action+recognition+in+lane+change+video+clips+using+deep+spatiotemporal+networks+with+segmentation+mask+transfer&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref212"><b _ngcontent-eae-c168="">212.</b></span><span _ngcontent-eae-c168="">Y. Gal, "Uncertainty in deep learning", 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Uncertainty+in+deep+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref213"><b _ngcontent-eae-c168="">213.</b></span><span _ngcontent-eae-c168="">S. Yamazaki, C. Miyajima, E. Yurtsever, K. Takeda, M. Mori, K. Hitomi, et al., "Integrating driving behavior and traffic context through signal symbolization", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 642-647, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7535455"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7535455"> Full Text: PDF </a><span _ngcontent-eae-c168="">(582KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Integrating+driving+behavior+and+traffic+context+through+signal+symbolization&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref214"><b _ngcontent-eae-c168="">214.</b></span><span _ngcontent-eae-c168="">X. Geng, H. Liang, B. Yu, P. Zhao, L. He and R. Huang, "A scenario-adaptive driving behavior prediction approach to urban autonomous driving", <em>Appl. Sci.</em>, vol. 7, no. 4, pp. 426, 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/app7040426"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+scenario-adaptive+driving+behavior+prediction+approach+to+urban+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref215"><b _ngcontent-eae-c168="">215.</b></span><span _ngcontent-eae-c168="">M. Bahram, C. Hubmann, A. Lawitzky, M. Aeberhard and D. Wollherr, "A combined model-and learning-based framework for interaction-aware maneuver prediction", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 17, no. 6, pp. 1538-1550, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7374724"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7374724"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1905KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+combined+model-and+learning-based+framework+for+interaction-aware+maneuver+prediction&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref216"><b _ngcontent-eae-c168="">216.</b></span><span _ngcontent-eae-c168="">V. Gadepally, A. Krishnamurthy and Ü. Özgüner, "A framework for estimating long term driver behavior", <em>J. Adv. Transp.</em>, vol. 2017, pp. 1-11, Jan. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1155/2017/3080859"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+framework+for+estimating+long+term+driver+behavior&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref217"><b _ngcontent-eae-c168="">217.</b></span><span _ngcontent-eae-c168="">P. Liu, A. Kurt, K. Redmill and U. Ozguner, "Classification of highway lane change behavior to detect dangerous cut-in maneuvers", <em>Proc. 95th Annu. Meeting Transp. Res. Board (TRB)</em>, vol. 2, pp. 1-14, 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Classification+of+highway+lane+change+behavior+to+detect+dangerous+cut-in+maneuvers&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref218"><b _ngcontent-eae-c168="">218.</b></span><span _ngcontent-eae-c168="">P. Kumar, M. Perrollaz, S. Lefevre and C. Laugier, "Learning-based approach for online lane change intention prediction", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 797-802, Jun. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6629564"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6629564"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1921KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning-based+approach+for+online+lane+change+intention+prediction&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref219"><b _ngcontent-eae-c168="">219.</b></span><span _ngcontent-eae-c168="">H. Darweesh, E. Takeuchi, K. Takeda, Y. Ninomiya, A. Sujiwo, L. Y. Morales, et al., "Open source integrated planner for autonomous navigation in highly dynamic environments", <em>J. Robot. Mechatronics</em>, vol. 29, no. 4, pp. 668-684, 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.20965/jrm.2017.p0668"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Open+source+integrated+planner+for+autonomous+navigation+in+highly+dynamic+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref220"><b _ngcontent-eae-c168="">220.</b></span><span _ngcontent-eae-c168="">F. Sagberg, Selpi, G. F. Bianchi Piccinini and J. Engström, "A review of research on driving styles and road safety", <em>Hum. Factors J. Hum. Factors Ergonom. Soc.</em>, vol. 57, no. 7, pp. 1248-1275, Nov. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0018720815591313"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+review+of+research+on+driving+styles+and+road+safety&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref221"><b _ngcontent-eae-c168="">221.</b></span><span _ngcontent-eae-c168="">C. Marina Martinez, M. Heucke, F.-Y. Wang, B. Gao and D. Cao, "Driving style recognition for intelligent vehicle control and advanced driver assistance: A survey", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 19, no. 3, pp. 666-676, Mar. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8002632"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8002632"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1778KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+style+recognition+for+intelligent+vehicle+control+and+advanced+driver+assistance%3A+A+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref222"><b _ngcontent-eae-c168="">222.</b></span><span _ngcontent-eae-c168="">D. A. Johnson and M. M. Trivedi, "Driving style recognition using a smartphone as a sensor platform", <em>Proc. 14th Int. IEEE Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 1609-1615, Oct. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6083078"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6083078"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1941KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+style+recognition+using+a+smartphone+as+a+sensor+platform&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref223"><b _ngcontent-eae-c168="">223.</b></span><span _ngcontent-eae-c168="">M. Fazeen, B. Gozick, R. Dantu, M. Bhukhiya and M. C. González, "Safe driving using mobile phones", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 13, no. 3, pp. 1462-1468, Sep. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6171850"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6171850"> Full Text: PDF </a><span _ngcontent-eae-c168="">(810KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Safe+driving+using+mobile+phones&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref224"><b _ngcontent-eae-c168="">224.</b></span><span _ngcontent-eae-c168="">N. Karginova, S. Byttner and M. Svensson, Data-driven methods for classification of driving styles in buses, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Data-driven+methods+for+classification+of+driving+styles+in+buses&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref225"><b _ngcontent-eae-c168="">225.</b></span><span _ngcontent-eae-c168="">A. Doshi and M. M. Trivedi, "Examining the impact of driving style on the predictability and responsiveness of the driver: Real-world and simulator analysis", <em>Proc. IEEE Intell. Vehicles Symp.</em>, pp. 232-237, Jun. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5547969"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5547969"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1147KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Examining+the+impact+of+driving+style+on+the+predictability+and+responsiveness+of+the+driver%3A+Real-world+and+simulator+analysis&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref226"><b _ngcontent-eae-c168="">226.</b></span><span _ngcontent-eae-c168="">V. Vaitkus, P. Lengvenis and G. Zylius, "Driving style classification using long-term accelerometer information", <em>Proc. 19th Int. Conf. Methods Models Autom. Robot. (MMAR)</em>, pp. 641-644, Sep. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6957429"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6957429"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2192KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+style+classification+using+long-term+accelerometer+information&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref227"><b _ngcontent-eae-c168="">227.</b></span><span _ngcontent-eae-c168="">F. Syed, S. Nallapa, A. Dobryden, C. Grand, R. McGee and D. Filev, Design and analysis of an adaptive real-time advisory system for improving real world fuel economy in a hybrid electric vehicle, 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Design+and+analysis+of+an+adaptive+real-time+advisory+system+for+improving+real+world+fuel+economy+in+a+hybrid+electric+vehicle&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref228"><b _ngcontent-eae-c168="">228.</b></span><span _ngcontent-eae-c168="">A. Corti, C. Ongini, M. Tanelli and S. M. Savaresi, "Quantitative driving style estimation for energy-oriented applications in road vehicles", <em>Proc. IEEE Int. Conf. Syst. Man Cybern.</em>, pp. 3710-3715, Oct. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6722385"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6722385"> Full Text: PDF </a><span _ngcontent-eae-c168="">(834KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Quantitative+driving+style+estimation+for+energy-oriented+applications+in+road+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref229"><b _ngcontent-eae-c168="">229.</b></span><span _ngcontent-eae-c168="">E. Ericsson, "Independent driving pattern factors and their influence on fuel-use and exhaust emission factors", <em>Transp. Res. D Transp. Environ.</em>, vol. 6, no. 5, pp. 325-345, Sep. 2001.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/S1361-9209(01)00003-7"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Independent+driving+pattern+factors+and+their+influence+on+fuel-use+and+exhaust+emission+factors&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref230"><b _ngcontent-eae-c168="">230.</b></span><span _ngcontent-eae-c168="">V. Manzoni, A. Corti, P. De Luca and S. M. Savaresi, "Driving style estimation via inertial measurements", <em>Proc. 13th Int. IEEE Conf. Intell. Transp. Syst.</em>, pp. 777-782, Sep. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5625113"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5625113"> Full Text: PDF </a><span _ngcontent-eae-c168="">(508KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+style+estimation+via+inertial+measurements&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref231"><b _ngcontent-eae-c168="">231.</b></span><span _ngcontent-eae-c168="">J. S. Neubauer and E. Wood, Accounting for the variation of driver aggression in the simulation of conventional and advanced vehicles, 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Accounting+for+the+variation+of+driver+aggression+in+the+simulation+of+conventional+and+advanced+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref232"><b _ngcontent-eae-c168="">232.</b></span><span _ngcontent-eae-c168="">Y. L. Murphey, R. Milton and L. Kiliaris, "Driver’s style classification using jerk analysis", <em>Proc. IEEE Workshop Comput. Intell. Vehicles Veh. Syst.</em>, pp. 23-28, Mar. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driver%E2%80%99s+style+classification+using+jerk+analysis&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref233"><b _ngcontent-eae-c168="">233.</b></span><span _ngcontent-eae-c168="">E. Yurtsever, K. Takeda and C. Miyajima, "Traffic trajectory history and drive path generation using GPS data cloud", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 229-234, Jun. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7225691"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7225691"> Full Text: PDF </a><span _ngcontent-eae-c168="">(552KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Traffic+trajectory+history+and+drive+path+generation+using+GPS+data+cloud&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref234"><b _ngcontent-eae-c168="">234.</b></span><span _ngcontent-eae-c168="">D. Dorr, D. Grabengiesser and F. Gauterin, "Online driving style recognition using fuzzy logic", <em>Proc. 17th Int. IEEE Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 1021-1026, Oct. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6957822"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6957822"> Full Text: PDF </a><span _ngcontent-eae-c168="">(527KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Online+driving+style+recognition+using+fuzzy+logic&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref235"><b _ngcontent-eae-c168="">235.</b></span><span _ngcontent-eae-c168="">L. Xu, J. Hu, H. Jiang and W. Meng, "Establishing style-oriented driver models by imitating human driving behaviors", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 16, no. 5, pp. 2522-2530, Oct. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7078931"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7078931"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1093KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Establishing+style-oriented+driver+models+by+imitating+human+driving+behaviors&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref236"><b _ngcontent-eae-c168="">236.</b></span><span _ngcontent-eae-c168="">B. Rajan, A. McGordon and P. Jennings, "An investigation on the effect of driver style and driving events on energy demand of a PHEV", <em>World Electric Vehicle J.</em>, vol. 5, no. 1, pp. 173-181, 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.3390/wevj5010173"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=An+investigation+on+the+effect+of+driver+style+and+driving+events+on+energy+demand+of+a+PHEV&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref237"><b _ngcontent-eae-c168="">237.</b></span><span _ngcontent-eae-c168="">A. Augustynowicz, "Preliminary classification of driving style with objective rank method", <em>Int. J. Automot. Technol.</em>, vol. 10, no. 5, pp. 607-610, Oct. 2009.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s12239-009-0071-8"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Preliminary+classification+of+driving+style+with+objective+rank+method&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref238"><b _ngcontent-eae-c168="">238.</b></span><span _ngcontent-eae-c168="">Z. Constantinescu, C. Marinoiu and M. Vladoiu, "Driving style analysis using data mining techniques", <em>Int. J. Comput. Commun. Control</em>, vol. 5, no. 5, pp. 654, 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.15837/ijccc.2010.5.2221"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+style+analysis+using+data+mining+techniques&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref239"><b _ngcontent-eae-c168="">239.</b></span><span _ngcontent-eae-c168="">Y. Zhang, W. C. Lin and Y.-K.-S. Chin, "A pattern-recognition approach for driving skill characterization", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 11, no. 4, pp. 905-916, Dec. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/5518413"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5518413"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1135KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+pattern-recognition+approach+for+driving+skill+characterization&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref240"><b _ngcontent-eae-c168="">240.</b></span><span _ngcontent-eae-c168="">E. Yurtsever, C. Miyajima and K. Takeda, "A traffic flow simulation framework for learning driver heterogeneity from naturalistic driving data using autoencoders", <em>Int. J. Automot. Eng.</em>, vol. 10, no. 1, pp. 86-93, 2019.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.20485/jsaeijae.10.1_86"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+traffic+flow+simulation+framework+for+learning+driver+heterogeneity+from+naturalistic+driving+data+using+autoencoders&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref241"><b _ngcontent-eae-c168="">241.</b></span><span _ngcontent-eae-c168="">C. Miyajima, Y. Nishiwaki, K. Ozawa, T. Wakita, K. Itou, K. Takeda, et al., "Driver modeling based on driving behavior and its evaluation in driver identification", <em>Proc. IEEE</em>, vol. 95, no. 2, pp. 427-437, Feb. 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4142931"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4142931"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1481KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driver+modeling+based+on+driving+behavior+and+its+evaluation+in+driver+identification&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref242"><b _ngcontent-eae-c168="">242.</b></span><span _ngcontent-eae-c168="">A. Bolovinou, I. Bakas, A. Amditis, F. Mastrandrea and W. Vinciotti, "Online prediction of an electric vehicle remaining range based on regression analysis", <em>Proc. IEEE Int. Electr. Vehicle Conf. (IEVC)</em>, pp. 1-8, Dec. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7056167"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7056167"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1242KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Online+prediction+of+an+electric+vehicle+remaining+range+based+on+regression+analysis&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref243"><b _ngcontent-eae-c168="">243.</b></span><span _ngcontent-eae-c168="">A. Mudgal, S. Hallmark, A. Carriquiry and K. Gkritza, "Driving behavior at a roundabout: A hierarchical Bayesian regression analysis", <em>Transp. Res. Part D: Transp. Environ.</em>, vol. 26, pp. 20-26, Jan. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.trd.2013.10.003"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+behavior+at+a+roundabout%3A+A+hierarchical+Bayesian+regression+analysis&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref244"><b _ngcontent-eae-c168="">244.</b></span><span _ngcontent-eae-c168="">J. C. McCall and M. M. Trivedi, "Driver behavior and situation aware brake assistance for intelligent vehicles", <em>Proc. IEEE</em>, vol. 95, no. 2, pp. 374-387, Feb. 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4142930"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4142930"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1422KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driver+behavior+and+situation+aware+brake+assistance+for+intelligent+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref245"><b _ngcontent-eae-c168="">245.</b></span><span _ngcontent-eae-c168="">E. Yurtsever, C. Miyajima, S. Selpi and K. Takeda, "Driving signature extraction", <em>Proc. 3rd Int. Symp. Future Act. Saf. Technol. Toward Zero Traffic Accidents (FAST-Zero)</em>, pp. 1-5, 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+signature+extraction&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref246"><b _ngcontent-eae-c168="">246.</b></span><span _ngcontent-eae-c168="">K. Sama, Y. Morales, N. Akai, H. Liu, E. Takeuchi and K. Takeda, "Driving feature extraction and behavior classification using an autoencoder to reproduce the velocity styles of experts", <em>Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 1337-1343, Nov. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8569245"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8569245"> Full Text: PDF </a><span _ngcontent-eae-c168="">(975KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driving+feature+extraction+and+behavior+classification+using+an+autoencoder+to+reproduce+the+velocity+styles+of+experts&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref247"><b _ngcontent-eae-c168="">247.</b></span><span _ngcontent-eae-c168="">H. Liu, T. Taniguchi, Y. Tanaka, K. Takenaka and T. Bando, "Visualization of driving behavior based on hidden feature extraction by using deep learning", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 18, no. 9, pp. 2477-2489, Sep. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7839988"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7839988"> Full Text: PDF </a><span _ngcontent-eae-c168="">(5686KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Visualization+of+driving+behavior+based+on+hidden+feature+extraction+by+using+deep+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref248"><b _ngcontent-eae-c168="">248.</b></span><span _ngcontent-eae-c168="">H. Bast, D. Delling, A. Goldberg, M. Müller-Hannemann, T. Pajor, P. Sanders, et al., "Route planning in transportation networks" in Algorithm Engineering, Cham, Switzerland:Springer, pp. 19-80, 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-319-49487-6_2"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Route+planning+in+transportation+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref249"><b _ngcontent-eae-c168="">249.</b></span><span _ngcontent-eae-c168="">P. Hart, N. Nilsson and B. Raphael, "A formal basis for the heuristic determination of minimum cost paths", <em>IEEE Trans. Syst. Sci. Cybern.</em>, vol. 4, no. 2, pp. 100-107, Jul. 1968.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4082128"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4082128"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2390KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+formal+basis+for+the+heuristic+determination+of+minimum+cost+paths&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref250"><b _ngcontent-eae-c168="">250.</b></span><span _ngcontent-eae-c168="">D. Van Vliet, "Improved shortest path algorithms for transport networks", <em>Transp. Res.</em>, vol. 12, no. 1, pp. 7-20, Feb. 1978.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/0041-1647(78)90102-8"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Improved+shortest+path+algorithms+for+transport+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref251"><b _ngcontent-eae-c168="">251.</b></span><span _ngcontent-eae-c168="">R. Geisberger, P. Sanders, D. Schultes and C. Vetter, "Exact routing in large road networks using contraction hierarchies", <em>Transp. Sci.</em>, vol. 46, no. 3, pp. 388-404, Aug. 2012.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1287/trsc.1110.0401"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Exact+routing+in+large+road+networks+using+contraction+hierarchies&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref252"><b _ngcontent-eae-c168="">252.</b></span><span _ngcontent-eae-c168="">E. Cohen, E. Halperin, H. Kaplan and U. Zwick, "Reachability and distance queries via 2-Hop labels", <em>SIAM J. Comput.</em>, vol. 32, no. 5, pp. 1338-1355, Jan. 2003.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1137/S0097539702403098"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Reachability+and+distance+queries+via+2-Hop+labels&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref253"><b _ngcontent-eae-c168="">253.</b></span><span _ngcontent-eae-c168="">R. Bauer, D. Delling, P. Sanders, D. Schieferdecker, D. Schultes and D. Wagner, "Combining hierarchical and goal-directed speed-up techniques for Dijkstra’s algorithm", <em>J. Experim. Algorithmics</em>, vol. 15, pp. 2-3, Mar. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Combining+hierarchical+and+goal-directed+speed-up+techniques+for+Dijkstra%E2%80%99s+algorithm&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref254"><b _ngcontent-eae-c168="">254.</b></span><span _ngcontent-eae-c168="">D. Delling, A. V. Goldberg, A. Nowatzyk and R. F. Werneck, "PHAST: Hardware-accelerated shortest path trees", <em>J. Parallel Distrib. Comput.</em>, vol. 73, no. 7, pp. 940-952, Jul. 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.jpdc.2012.02.007"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=PHAST%3A+Hardware-accelerated+shortest+path+trees&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref255"><b _ngcontent-eae-c168="">255.</b></span><span _ngcontent-eae-c168="">M. Pivtoraiko and A. Kelly, "Efficient constrained path planning via search in state lattices", <em>Proc. Int. Symp. Artif. Intell. Robot. Automat. Space</em>, pp. 1-7, 2005.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Efficient+constrained+path+planning+via+search+in+state+lattices&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref256"><b _ngcontent-eae-c168="">256.</b></span><span _ngcontent-eae-c168="">J. Barraquand and J.-C. Latombe, "Robot motion planning: A distributed representation approach", <em>Int. J. Robot. Res.</em>, vol. 10, no. 6, pp. 628-649, Dec. 1991.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/027836499101000604"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Robot+motion+planning%3A+A+distributed+representation+approach&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref257"><b _ngcontent-eae-c168="">257.</b></span><span _ngcontent-eae-c168="">S. M. LaValle and J. J. Kuffner, "Randomized kinodynamic planning", <em>Int. J. Robot. Res.</em>, vol. 20, no. 5, pp. 378-400, May 2001.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/02783640122067453"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Randomized+kinodynamic+planning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref258"><b _ngcontent-eae-c168="">258.</b></span><span _ngcontent-eae-c168="">S. Karaman and E. Frazzoli, "Sampling-based algorithms for optimal motion planning", <em>Int. J. Robot. Res.</em>, vol. 30, no. 7, pp. 846-894, Jun. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0278364911406761"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sampling-based+algorithms+for+optimal+motion+planning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref259"><b _ngcontent-eae-c168="">259.</b></span><span _ngcontent-eae-c168="">L. E. Kavraki et al., "Probabilistic roadmaps for path planning in high-dimensional configuration spaces", <em>IEEE Trans. Robot. Automat.</em>, vol. 12, no. 4, pp. 566-580, 1996.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/508439"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=508439"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1960KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Probabilistic+roadmaps+for+path+planning+in+high-dimensional+configuration+spaces&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref260"><b _ngcontent-eae-c168="">260.</b></span><span _ngcontent-eae-c168="">H. Fuji, J. Xiang, Y. Tazaki, B. Levedahl and T. Suzuki, "Trajectory planning for automated parking using multi-resolution state roadmap considering non-holonomic constraints", <em>Proc. IEEE Intell. Vehicles Symp. Proc.</em>, pp. 407-413, Jun. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6856433"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6856433"> Full Text: PDF </a><span _ngcontent-eae-c168="">(3145KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Trajectory+planning+for+automated+parking+using+multi-resolution+state+roadmap+considering+non-holonomic+constraints&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref261"><b _ngcontent-eae-c168="">261.</b></span><span _ngcontent-eae-c168="">P. Petrov and F. Nashashibi, "Modeling and nonlinear adaptive control for autonomous vehicle overtaking", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 15, no. 4, pp. 1643-1656, Aug. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6754174"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6754174"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1333KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Modeling+and+nonlinear+adaptive+control+for+autonomous+vehicle+overtaking&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref262"><b _ngcontent-eae-c168="">262.</b></span><span _ngcontent-eae-c168="">J. P. Rastelli, R. Lattarulo and F. Nashashibi, "Dynamic trajectory generation using continuous-curvature algorithms for door to door assistance vehicles", <em>Proc. IEEE Intell. Vehicles Symp. Proc.</em>, pp. 510-515, Jun. 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6856526"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6856526"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1126KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Dynamic+trajectory+generation+using+continuous-curvature+algorithms+for+door+to+door+assistance+vehicles&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref263"><b _ngcontent-eae-c168="">263.</b></span><span _ngcontent-eae-c168="">D. Dolgov, S. Thrun, M. Montemerlo and J. Diebel, "Path planning for autonomous vehicles in unknown semi-structured environments", <em>Int. J. Robot. Res.</em>, vol. 29, no. 5, pp. 485-501, Apr. 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1177/0278364909359210"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Path+planning+for+autonomous+vehicles+in+unknown+semi-structured+environments&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref264"><b _ngcontent-eae-c168="">264.</b></span><span _ngcontent-eae-c168="">J. Ren, K. A. McIsaac and R. V. Patel, "Modified Newton’s method applied to potential field-based navigation for mobile robots", <em>IEEE Trans. Robot.</em>, vol. 22, no. 2, pp. 384-391, Apr. 2006.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/1618532"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1618532"> Full Text: PDF </a><span _ngcontent-eae-c168="">(663KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Modified+Newton%E2%80%99s+method+applied+to+potential+field-based+navigation+for+mobile+robots&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref265"><b _ngcontent-eae-c168="">265.</b></span><span _ngcontent-eae-c168="">L. Caltagirone, M. Bellone, L. Svensson and M. Wahde, "LIDAR-based driving path generation using fully convolutional neural networks", <em>Proc. IEEE 20th Int. Conf. Intell. Transp. Syst. (ITSC)</em>, pp. 1-6, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8317618"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8317618"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1101KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=LIDAR-based+driving+path+generation+using+fully+convolutional+neural+networks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref266"><b _ngcontent-eae-c168="">266.</b></span><span _ngcontent-eae-c168="">D. Barnes, W. Maddern and I. Posner, "Find your own way: Weakly-supervised segmentation of path proposals for urban autonomy", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, pp. 203-210, May 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7989025"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7989025"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2529KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Find+your+own+way%3A+Weakly-supervised+segmentation+of+path+proposals+for+urban+autonomy&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref267"><b _ngcontent-eae-c168="">267.</b></span><span _ngcontent-eae-c168="">M. Elbanhawi and M. Simic, "Sampling-based robot motion planning: A review", <em>IEEE Access</em>, vol. 2, pp. 56-77, 2014.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6722915"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6722915"> Full Text: PDF </a><span _ngcontent-eae-c168="">(7166KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Sampling-based+robot+motion+planning%3A+A+review&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref268"><b _ngcontent-eae-c168="">268.</b></span><span _ngcontent-eae-c168="">D. Isele, R. Rahimi, A. Cosgun, K. Subramanian and K. Fujimura, "Navigating occluded intersections with autonomous vehicles using deep reinforcement learning", <em>Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</em>, pp. 2034-2039, May 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8461233"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8461233"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1415KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Navigating+occluded+intersections+with+autonomous+vehicles+using+deep+reinforcement+learning&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref269"><b _ngcontent-eae-c168="">269.</b></span><span _ngcontent-eae-c168="">C. A. Pickering, K. J. Burnham and M. J. Richardson, "A review of automotive human machine interface technologies and techniques to reduce driver distraction", <em>Proc. 2nd IET Int. Conf. Syst. Saf.</em>, pp. 223-228, 2007.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/4399937"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4399937"> Full Text: PDF </a><span _ngcontent-eae-c168="">(4084KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+review+of+automotive+human+machine+interface+technologies+and+techniques+to+reduce+driver+distraction&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref270"><b _ngcontent-eae-c168="">270.</b></span><span _ngcontent-eae-c168="">O. Carsten and M. H. Martens, "How can humans understand their automated cars? HMI principles problems and solutions", <em>Cognition Technol. Work</em>, vol. 21, no. 1, pp. 3-20, Feb. 2019.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/s10111-018-0484-0"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=How+can+humans+understand+their+automated+cars%3F+HMI+principles%2C+problems+and+solutions&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref271"><b _ngcontent-eae-c168="">271.</b></span><span _ngcontent-eae-c168="">P. Bazilinskyy and J. de Winter, "Auditory interfaces in automated driving: An international survey", <em>PeerJ Comput. Sci.</em>, vol. 1, pp. e13, Aug. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.7717/peerj-cs.13"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Auditory+interfaces+in+automated+driving%3A+An+international+survey&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref272"><b _ngcontent-eae-c168="">272.</b></span><span _ngcontent-eae-c168="">M. Peden, R. Scurfield, D. Sleet, D. Mohan, A. A. Hyder, E. Jarawan, et al., <em>World Report on Road Traffic Injury Prevention</em>, 2004.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=World+Report+on+Road+Traffic+Injury+Prevention&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref273"><b _ngcontent-eae-c168="">273.</b></span><span _ngcontent-eae-c168="">D. R. Large, L. Clark, A. Quandt, G. Burnett and L. Skrypchuk, "Steering the conversation: A linguistic exploration of natural language interactions with a digital assistant during simulated driving", <em>Appl. Ergonom.</em>, vol. 63, pp. 53-61, Sep. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1016/j.apergo.2017.04.003"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Steering+the+conversation%3A+A+linguistic+exploration+of+natural+language+interactions+with+a+digital+assistant+during+simulated+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref274"><b _ngcontent-eae-c168="">274.</b></span><span _ngcontent-eae-c168="">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, et al., "The cityscapes dataset for semantic urban scene understanding", <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, pp. 3213-3223, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7780719"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780719"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1145KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+cityscapes+dataset+for+semantic+urban+scene+understanding&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref275"><b _ngcontent-eae-c168="">275.</b></span><span _ngcontent-eae-c168="">F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, et al., "BDD100K: A diverse driving video database with scalable annotation tooling" in arXiv:1805.04687, 2018,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1805.04687" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1805</span><span>.</span><span>04687</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=BDD100K%3A+A+diverse+driving+video+database+with+scalable+annotation+tooling&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref276"><b _ngcontent-eae-c168="">276.</b></span><span _ngcontent-eae-c168="">G. Neuhold, T. Ollmann, S. R. Bulo and P. Kontschieder, "The mapillary vistas dataset for semantic understanding of street scenes", <em>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, pp. 5000-5009, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8237796"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237796"> Full Text: PDF </a><span _ngcontent-eae-c168="">(10604KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+mapillary+vistas+dataset+for+semantic+understanding+of+street+scenes&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref277"><b _ngcontent-eae-c168="">277.</b></span><span _ngcontent-eae-c168="">A. Patil, S. Malla, H. Gang and Y.-T. Chen, "The H3D dataset for full-surround 3D multi-object detection and tracking in crowded urban scenes" in arXiv:1903.01568, 2019,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1903.01568" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1903</span><span>.</span><span>01568</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8793925"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8793925"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1814KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+H3D+dataset+for+full-surround+3D+multi-object+detection+and+tracking+in+crowded+urban+scenes&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref278"><b _ngcontent-eae-c168="">278.</b></span><span _ngcontent-eae-c168="">X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng and R. Yang, "The ApolloScape open dataset for autonomous driving and its application" in arXiv:1803.06184, 2018,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1803.06184" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1803</span><span>.</span><span>06184</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+ApolloScape+open+dataset+for+autonomous+driving+and+its+application&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref279"><b _ngcontent-eae-c168="">279.</b></span><span _ngcontent-eae-c168=""><em>Udacity Dataset</em>, Apr. 2019,  [online]  Available: <a class="vglnk" href="https://github.com/udacity/self-driving-car/tree/master/datasets" rel="nofollow"><span>https</span><span>://</span><span>github</span><span>.</span><span>com</span><span>/</span><span>udacity</span><span>/</span><span>self</span><span>-</span><span>driving</span><span>-</span><span>car</span><span>/</span><span>tree</span><span>/</span><span>master</span><span>/</span><span>datasets</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Udacity+Dataset&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref280"><b _ngcontent-eae-c168="">280.</b></span><span _ngcontent-eae-c168="">H. Schafer, E. Santana, A. Haden and R. Biasini, "A commute in data: The Comma2k19 dataset" in arXiv:1812.05752, 2018,  [online]  Available: <a class="vglnk" href="https://arxiv.org/abs/1812.05752" rel="nofollow"><span>https</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1812</span><span>.</span><span>05752</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=A+commute+in+data%3A+The+Comma2k19+dataset&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref281"><b _ngcontent-eae-c168="">281.</b></span><span _ngcontent-eae-c168="">Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, et al., "LiDAR-video driving dataset: Learning driving policies effectively", <em>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</em>, pp. 5870-5878, Jun. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8578713"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8578713"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2390KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=LiDAR-video+driving+dataset%3A+Learning+driving+policies+effectively&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref282"><b _ngcontent-eae-c168="">282.</b></span><span _ngcontent-eae-c168="">K. Takeda, J. H. L. Hansen, P. Boyraz, L. Malta, C. Miyajima and H. Abut, "International large-scale vehicle corpora for research on driver behavior on the road", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 12, no. 4, pp. 1609-1623, Dec. 2011.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/6046133"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6046133"> Full Text: PDF </a><span _ngcontent-eae-c168="">(1810KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=International+large-scale+vehicle+corpora+for+research+on+driver+behavior+on+the+road&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref283"><b _ngcontent-eae-c168="">283.</b></span><span _ngcontent-eae-c168="">A. Blatt, J. Pierowicz, M. Flanigan, P. S. Lin, A. Kourtellis, C. Lee, et al., "Naturalistic driving study: Field data collection", 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Naturalistic+driving+study%3A+Field+data+collection&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref284"><b _ngcontent-eae-c168="">284.</b></span><span _ngcontent-eae-c168="">S. G. Klauer, F. Guo, J. Sudweeks and T. A. Dingus, "An analysis of driver inattention using a case-crossover approach on 100-car data", 2010.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=An+analysis+of+driver+inattention+using+a+case-crossover+approach+on+100-car+data&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref285"><b _ngcontent-eae-c168="">285.</b></span><span _ngcontent-eae-c168="">M. Benmimoun et al., "euroFOT: Field operational test and impact assessment of advanced driver assistance systems: Final results", <em>Proc. FISITA World Automot. Congr.</em>, pp. 537-547, 2013.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-crossRef" href="https://doi.org/10.1007/978-3-642-33805-2_43"> CrossRef <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=euroFOT%3A+Field+operational+test+and+impact+assessment+of+advanced+driver+assistance+systems%3A+Final+results&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref286"><b _ngcontent-eae-c168="">286.</b></span><span _ngcontent-eae-c168="">S. Wang, M. Bai, G. Mattyus, H. Chu, W. Luo, B. Yang, et al., "TorontoCity: Seeing the world with a million eyes", <em>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, pp. 3028-3036, Oct. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8237589"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237589"> Full Text: PDF </a><span _ngcontent-eae-c168="">(36557KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=TorontoCity%3A+Seeing+the+world+with+a+million+eyes&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref287"><b _ngcontent-eae-c168="">287.</b></span><span _ngcontent-eae-c168="">Y. Choi, N. Kim, S. Hwang, K. Park, J. S. Yoon, K. An, et al., "KAIST multi-spectral Day/Night data set for autonomous and assisted driving", <em>IEEE Trans. Intell. Transp. Syst.</em>, vol. 19, no. 3, pp. 934-948, Mar. 2018.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8293689"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8293689"> Full Text: PDF </a><span _ngcontent-eae-c168="">(6340KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=KAIST+multi-spectral+Day%2FNight+data+set+for+autonomous+and+assisted+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref288"><b _ngcontent-eae-c168="">288.</b></span><span _ngcontent-eae-c168="">S. Sibi, H. Ayaz, D. P. Kuhns, D. M. Sirkin and W. Ju, "Monitoring driver cognitive load using functional near infrared spectroscopy in partially autonomous cars", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 419-425, Jun. 2016.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7535420"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7535420"> Full Text: PDF </a><span _ngcontent-eae-c168="">(671KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Monitoring+driver+cognitive+load+using+functional+near+infrared+spectroscopy+in+partially+autonomous+cars&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref289"><b _ngcontent-eae-c168="">289.</b></span><span _ngcontent-eae-c168="">C. Braunagel, E. Kasneci, W. Stolzmann and W. Rosenstiel, "Driver-activity recognition in the context of conditionally autonomous driving", <em>Proc. IEEE 18th Int. Conf. Intell. Transp. Syst.</em>, pp. 1652-1657, Sep. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7313360"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7313360"> Full Text: PDF </a><span _ngcontent-eae-c168="">(718KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driver-activity+recognition+in+the+context+of+conditionally+autonomous+driving&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref290"><b _ngcontent-eae-c168="">290.</b></span><span _ngcontent-eae-c168="">M. Walch, K. Lange, M. Baumann and M. Weber, "Autonomous driving: Investigating the feasibility of car-driver handover assistance", <em>Proc. 7th Int. Conf. Automot. User Interfaces Interact. Veh. Appl.</em>, pp. 11-18, 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Autonomous+driving%3A+Investigating+the+feasibility+of+car-driver+handover+assistance&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref291"><b _ngcontent-eae-c168="">291.</b></span><span _ngcontent-eae-c168="">J. H. L. Hansen, C. Busso, Y. Zheng and A. Sathyanarayana, "Driver modeling for detection and assessment of driver distraction: Examples from the UTDrive test bed", <em>IEEE Signal Process. Mag.</em>, vol. 34, no. 4, pp. 130-142, Jul. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7974867"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7974867"> Full Text: PDF </a><span _ngcontent-eae-c168="">(2294KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driver+modeling+for+detection+and+assessment+of+driver+distraction%3A+Examples+from+the+UTDrive+test+bed&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref292"><b _ngcontent-eae-c168="">292.</b></span><span _ngcontent-eae-c168="">M. Everingham et al., "The 2005 PASCAL visual object classes challenge", <em>Proc. Mach. Learn. Challenges Workshop</em>, 2005.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=The+2005+PASCAL+visual+object+classes+challenge&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref293"><b _ngcontent-eae-c168="">293.</b></span><span _ngcontent-eae-c168="">E. Santana and G. Hotz, "Learning a driving simulator" in arXiv:1608.01230, 2016,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1608.01230" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1608</span><span>.</span><span>01230</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Learning+a+driving+simulator&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref294"><b _ngcontent-eae-c168="">294.</b></span><span _ngcontent-eae-c168="">M. Althoff, M. Koschi and S. Manzinger, "CommonRoad: Composable benchmarks for motion planning on roads", <em>Proc. IEEE Intell. Vehicles Symp. (IV)</em>, pp. 719-726, Jun. 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7995802"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7995802"> Full Text: PDF </a><span _ngcontent-eae-c168="">(281KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=CommonRoad%3A+Composable+benchmarks+for+motion+planning+on+roads&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref295"><b _ngcontent-eae-c168="">295.</b></span><span _ngcontent-eae-c168="">H. Fan, F. Zhu, C. Liu, L. Zhang, L. Zhuang, D. Li, et al., "Baidu Apollo EM motion planner" in arXiv:1807.08048, 2018,  [online]  Available: <a class="vglnk" href="http://arxiv.org/abs/1807.08048" rel="nofollow"><span>http</span><span>://</span><span>arxiv</span><span>.</span><span>org</span><span>/</span><span>abs</span><span>/</span><span>1807</span><span>.</span><span>08048</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Baidu+Apollo+EM+motion+planner&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref296"><b _ngcontent-eae-c168="">296.</b></span><span _ngcontent-eae-c168=""><em>Driveworks SDK</em>, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://developer.nvidia.com/driveworks" rel="nofollow"><span>https</span><span>://</span><span>developer</span><span>.</span><span>nvidia</span><span>.</span><span>com</span><span>/</span><span>driveworks</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Driveworks+SDK&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref297"><b _ngcontent-eae-c168="">297.</b></span><span _ngcontent-eae-c168=""><em>OpenPilot</em>, Dec. 2018,  [online]  Available: <a class="vglnk" href="https://github.com/commaai/openpilot" rel="nofollow"><span>https</span><span>://</span><span>github</span><span>.</span><span>com</span><span>/</span><span>commaai</span><span>/</span><span>openpilot</span></a>.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=OpenPilot&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref298"><b _ngcontent-eae-c168="">298.</b></span><span _ngcontent-eae-c168="">B. Wymann, C. Dimitrakakis, A. Sumner, E. Espié and C. Guionneau, TORCS: The Open Racing Car Simulator, May 2015,  [online]  Available: <a class="vglnk" href="http://www.cse.chalmers.se/chrdimi/" rel="nofollow"><span>http</span><span>://</span><span>www</span><span>.</span><span>cse</span><span>.</span><span>chalmers</span><span>.</span><span>se</span><span>/</span><span>chrdimi</span><span>/</span></a> papers/torcs.pdf.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=TORCS%3A+The+Open+Racing+Car+Simulator&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref299"><b _ngcontent-eae-c168="">299.</b></span><span _ngcontent-eae-c168="">S. R. Richter, Z. Hayder and V. Koltun, "Playing for benchmarks", <em>Proc. Int. Conf. Comput. Vis. (ICCV)</em>, vol. 2, pp. 2213-2222, 2017.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/8237505"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8237505"> Full Text: PDF </a><span _ngcontent-eae-c168="">(374KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Playing+for+benchmarks&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref300"><b _ngcontent-eae-c168="">300.</b></span><span _ngcontent-eae-c168="">N. Koenig and A. Howard, "Design and use paradigms for Gazebo an open-source multi-robot simulator", <em>Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</em>, vol. 4, pp. 2149-2154, Sep./Oct. 2004.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/1389727"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1389727"> Full Text: PDF </a><span _ngcontent-eae-c168="">(512KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Design+and+use+paradigms+for+Gazebo%2C+an+open-source+multi-robot+simulator&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref301"><b _ngcontent-eae-c168="">301.</b></span><span _ngcontent-eae-c168="">D. Krajzewicz, G. Hertkorn, C. Rössel and P. Wagner, "SUMO (Simulation of Urban MObility)—An open-source traffic simulation", <em>Proc. 4th Middle East Symp. Simulation Modelling (MESM)</em>, pp. 183-187, 2002.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=SUMO+%28Simulation+of+Urban+MObility%29%E2%80%94An+open-source+traffic+simulation&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><div _ngcontent-eae-c188="" class="reference-container"><xpl-reference-item-migr _ngcontent-eae-c188="" displaytype="all" _nghost-eae-c168=""><!----><!----><div _ngcontent-eae-c168="" class="row"><div _ngcontent-eae-c168="" class="col-12 u-overflow-wrap-break-word"><span _ngcontent-eae-c168="" class="number" id="ref302"><b _ngcontent-eae-c168="">302.</b></span><span _ngcontent-eae-c168="">J. E. Stellet, M. R. Zofka, J. Schumacher, T. Schamm, F. Niewels and J. M. Zollner, "Testing of advanced driver assistance towards automated driving: A survey and taxonomy on existing approaches and open questions", <em>Proc. IEEE 18th Int. Conf. Intell. Transp. Syst.</em>, pp. 1455-1462, Sep. 2015.</span><div _ngcontent-eae-c168="" class="ref-links-container stats-reference-links-container"><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-showContext"><i _ngcontent-eae-c168="" class="icon-caret-abstract"></i><span _ngcontent-eae-c168="">Show in Context</span></a></span><!----><!----><!----><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_self" class="stats-reference-link-viewArticle" href="https://ieeexplore.ieee.org/document/7313330"> View Article </a></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" class="stats-reference-link-fullTextPdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7313330"> Full Text: PDF </a><span _ngcontent-eae-c168="">(256KB)</span><!----></span><!----><span _ngcontent-eae-c168="" class="ref-link"><a _ngcontent-eae-c168="" target="_blank" class="stats-reference-link-googleScholar" href="https://scholar.google.com/scholar?as_q=Testing+of+advanced+driver+assistance+towards+automated+driving%3A+A+survey+and+taxonomy+on+existing+approaches+and+open+questions&amp;as_occt=title&amp;hl=en&amp;as_sdt=0%2C31"> Google Scholar <i _ngcontent-eae-c168="" class="icon ref-external-icon"></i></a></span><!----></div><!----></div></div><!----></xpl-reference-item-migr></div><!----></div><!----><!----><!----></div></section></xpl-reference-panel><!----></div><!----></xpl-document-details><!----></div><xpl-footer _ngcontent-eae-c458="" _nghost-eae-c457=""><footer _ngcontent-eae-c457="" id="xplore-footer" class="stats-footer footer-new"><div _ngcontent-eae-c457="" class="footer-wrapper"><div _ngcontent-eae-c457="" class="flexible-row-col"><div _ngcontent-eae-c457="" class="footer-col"><h3 _ngcontent-eae-c457="" class="text-base-md-lh">IEEE Personal Account</h3><ul _ngcontent-eae-c457="" class="text-sm-md-lh"><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Change username/password</a></li></ul></div><div _ngcontent-eae-c457="" class="footer-col"><h3 _ngcontent-eae-c457="" class="text-base-md-lh">Purchase Details</h3><ul _ngcontent-eae-c457="" class="text-sm-md-lh"><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp">View Purchased Documents</a></li></ul></div><div _ngcontent-eae-c457="" class="footer-col"><h3 _ngcontent-eae-c457="" class="text-base-md-lh">Profile Information</h3><ul _ngcontent-eae-c457="" class="text-sm-md-lh"><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Technical interests</a></li></ul></div><div _ngcontent-eae-c457="" class="footer-col need-help"><h3 _ngcontent-eae-c457="" class="text-base-md-lh">Need Help?</h3><ul _ngcontent-eae-c457="" class="text-sm-md-lh"><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" href="tel:+1-800-678-4333"> US &amp; Canada: +1 800 678 4333 </a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" href="tel:+1-732-981-0060"> Worldwide: +1 732 981 0060 </a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/xpl/contact"> Contact &amp; Support </a></li></ul></div><div _ngcontent-eae-c457="" class="footer-col follow"><h3 _ngcontent-eae-c457="" class="text-base-md-lh">Follow</h3><ul _ngcontent-eae-c457="" class="icon-size-md"><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.facebook.com/IEEEXploreDigitalLibrary/"><i _ngcontent-eae-c457="" aria-hidden="true" class="fab fa-facebook-f"></i></a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://www.linkedin.com/showcase/ieee-xplore"><i _ngcontent-eae-c457="" aria-hidden="true" class="fab fa-linkedin-in"></i></a></li><li _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_blank" href="https://twitter.com/IEEEXplore?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor"><i _ngcontent-eae-c457="" aria-hidden="true" class="fab fa-twitter"></i></a></li></ul></div></div><div _ngcontent-eae-c457="" class="footer-bottom-section"><p _ngcontent-eae-c457="" class="text-sm-md-lh"><span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/Xplorehelp/about-ieee-xplore.html">About IEEE <em _ngcontent-eae-c457="">Xplore</em></a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/xpl/contact">Contact Us</a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/Xplorehelp/Help_start.html">Help</a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/Xplorehelp/accessibility-statement.html">Accessibility</a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/Xplorehelp/Help_Terms_of_Use.html">Terms of Use</a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html">Nondiscrimination Policy</a></span> | <span _ngcontent-eae-c457="" class="ethics-reporting-link"><a _ngcontent-eae-c457="" target="_blank" href="http://www.ieee-ethics-reporting.org/">IEEE Ethics Reporting<i _ngcontent-eae-c457="" class="fa fa-external-link-alt"></i></a></span> | <span _ngcontent-eae-c457=""><a _ngcontent-eae-c457="" target="_self" href="https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/ieee-xplore-sitemap">Sitemap</a></span> | <span _ngcontent-eae-c457="" class="nowrap"><a _ngcontent-eae-c457="" target="_self" href="http://www.ieee.org/about/help/security_privacy.html">Privacy &amp; Opting Out of Cookies</a></span></p><p _ngcontent-eae-c457=""> A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. </p><p _ngcontent-eae-c457=""> © Copyright 2022 IEEE - All rights reserved. </p></div></div></footer></xpl-footer><!----></xpl-root>
	</div>
	
	<!-- START: Angular 2+ Migration: Due to Angualr2+ migration AngualrJs router place holder commented -->
<!-- 	<div ng-show="stateIsLoading" class="Spinner"></div>
	<div ui-view class="{{stateIsLoading ? 'loading': ''}}"
		autoscroll="false"></div>
 -->	<!-- END: Angular 2+ Migration-->
</div>

						




<section id="xploreFooter" class="hide-desktop">
	
	<div class="Footer stats-footer hide-mobile">
		<div class="pure-g Footer-sections">
			<div class="pure-u-1-4">
				<h3 class="Footer-header">IEEE Account</h3>
				<ul class="Footer-list">
					<li><a href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
					<li><a href="https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
				</ul>
			</div>
			<div class="pure-u-1-4">
				<h3 class="Footer-header">Purchase Details</h3>
				<ul class="Footer-list">
					<li><a href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
					<li><a href="https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
					<li><a href="https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp">View Purchased Documents</a></li>
				</ul>
			</div>
			<div class="pure-u-1-4">
				<h3 class="Footer-header">Profile Information</h3>
				<ul class="Footer-list">
					<li><a href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
					<li><a href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
					<li><a href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
				</ul>
			</div>
			<div class="pure-u-1-4">
				<h3 class="Footer-header">Need Help?</h3>
				<ul class="Footer-list">
					<li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
					<li><strong>Worldwide: </strong> +1 732 981 0060<br>
					</li>
					<li><a href="https://ieeexplore.ieee.org/xpl/contact">Contact &amp; Support</a></li>
				</ul>
			</div>
		</div>
		<div class="row">
			<div class="col-12 Footer-bottom">
				<div class="Footer-bottom-inner-div row">
					<div class="col">
						<ul class="Menu Menu--horizontal Menu--dividers u-mb-1">
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/Xplorehelp/about-ieee-xplore.html">About IEEE <em>Xplore</em></a></li>
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/xpl/contact">Contact Us</a></li>
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/Xplorehelp/Help_start.html" target="blank">Help</a></li>
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/Xplorehelp/accessibility-statement.html" target="blank">Accessibility</a></li> 
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/Xplorehelp/Help_Terms_of_Use.html" target="_blank">Terms of Use</a></li>
							<li class="Menu-item"><a href="http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html">Nondiscrimination Policy</a></li>
							<li class="Menu-item"><a href="https://ieeexplore.ieee.org/xpl/sitemap.jsp">Sitemap</a></li>
							<li class="Menu-item"><a href="http://www.ieee.org/about/help/security_privacy.html" target="blank">Privacy &amp; Opting Out of Cookies</a></li>
						</ul>
						<p class="Footer-bottom-terms">
							A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
						</p>
					</div>

					<div><i class="logo-ieee-white"></i></div>
				</div>
				
			</div>
		</div>
	</div>

	
	
</section>

						<!-- BEGIN: tealium in v2/common/template.jsp. We need to include tealiumAnalytics.js here since Angular 2+ app load if you load after commnon.js then tealium value will not be available in angular 2+ app  -->
						






		<!-- BEGIN: TealiumAnalytics.jsp -->
		
		
		
		
		
		
		
		
		
		
		
		
			
				
			
			
			
			
		
		
		
		
		
		

			<script type="text/javascript">
 				// tealium config vars
				var TEALIUM_CONFIG_TAGGING_ENABLED = true;		
				var TEALIUM_CONFIG_CDN_URL = '//tags.tiqcdn.com/utag/';
				var TEALIUM_CONFIG_ACCOUNT_PROFILE_ENV = 'ieeexplore/main/prod';
				
				// tealium utag_data values for user 
				var TEALIUM_userType = 'Anonymous';
				var TEALIUM_userInstitutionId = '';
				var TEALIUM_userId = '';
				var TEALIUM_user_third_party = '';
				
				var TEALIUM_products = '';
			</script>


			<script type="text/javascript">
			// asynchronously load tealium's utag.js , which declares tealium JS variables like; utag_data, utag
			(function(a,b,c,d){
			
				a=TEALIUM_CONFIG_CDN_URL + TEALIUM_CONFIG_ACCOUNT_PROFILE_ENV + '/utag.js';
				b=document;c='script';d=b.createElement(c);d.src=a;
				d.type='text/java'+c;d.async=true;
				a=b.getElementsByTagName(c)[0];a.parentNode.insertBefore(d,a);
			})();
			</script>

			<script type="text/javascript" src="./全景摄像头_files/tealiumTagsData.js"></script>
			<script type="text/javascript" src="./全景摄像头_files/tealiumAnalytics.js"></script>


		
 		
		<!-- END: TealiumAnalytics.jsp -->
			 

						<!-- END: tealium in v2/common/template.jsp -->
						







<script type="text/javascript" src="./全景摄像头_files/MathJax.js"></script>












<link rel="stylesheet" href="./全景摄像头_files/ie7Sniffer.css">
<script type="text/javascript" src="./全景摄像头_files/js.cookie.js"></script>
<script type="text/javascript" src="./全景摄像头_files/fingerprint2.js"></script>
<script type="text/javascript" src="./全景摄像头_files/fingerprint.js"></script>

<!-- START OF Angular bundle assets -->
<script type="text/javascript" src="./全景摄像头_files/runtime.js" defer="" charset="utf-8"></script>
<script type="text/javascript" src="./全景摄像头_files/polyfills-es5.js" nomodule="" defer="" charset="utf-8"></script>
<script type="text/javascript" src="./全景摄像头_files/polyfills.js" defer="" charset="utf-8"></script>
<script type="text/javascript" src="./全景摄像头_files/styles.js" defer="" charset="utf-8"></script>

<script type="text/javascript" src="./全景摄像头_files/main.js" defer="" charset="utf-8"></script>
<!-- END OF Angular bundle assets -->

<!-- Usabilla Combicode for IEEE-->
<!-- Begin Usabilla for Websites embed code -->
<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t) { f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)}; a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({}); if(!navigator.userAgent.match(/Android|BlackBerry|BB10|iPhone|iPad|iPod|Opera Mini|IEMobile/i)) {window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/e9930a118e08.js"); } else {window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/118ca38ae742.js"); }/*]]>{/literal}*/</script>
<!-- end usabilla live embed code -->


	

		
		

		
		
		

		
			
		
		
		

		

		
		


		

		
		
		
		
		
		
		
		
		
		
	
	






<div style="width: 1263px;" id="popup_overlay"></div>

<g:compress>








		
		
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/modernizr.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/vendor.js" charset="utf-8"></script>
					
								
			
				
					
						








 

<script>
	var $j = jQuery.noConflict();
	var j$ = jQuery.noConflict();
	var membershipIncomplete;
    var IS_INDIVIDUAL_USER=false;

	var searchPropertiesParamQueryText = 'queryText';
	var searchPropertiesParamNewSearch = 'newsearch';
	var searchPropertiesParamMatchBoolean = 'matchBoolean';
	var searchPropertiesParamSearchWithin = 'searchWithin';
	var searchInterfaceArticleIndexTermReference = 'Search_Index_Terms';
	var searchPropertiesParamRecordsPerPage = 'rowsPerPage';
	var searchPropertiesParamPageNumber = 'pageNumber';
	var searchPropertiesParamRemoveRefinement = 'removeRefinement';	
	var searchPropertiesParamSearchField = 'searchField';
	var searchPropertiesParamArticleNumber = 'arnumber';
	
	var authorsGetDisplay = 'Authors';
	var authorsFirstNameProperty = 'First Name';
	var authorsLastNameProperty = 'Last Name';
	var authorsMiddleNameProperty = 'Middle Name';
	var pubTitleDispNameProperty = 'Publication Title';
	var volumeDispNameProperty = 'Volume';
	var issueDispNameProperty = 'Issue';
	var startPageDispNameProperty = 'Start Page';
	var endPageDispNameProperty = 'Start Page';

	var searchIcsTermProperty = 'Standards ICS Terms';
	var SearchMapperParamSearchField = 'searchField';
	
	var SearchMapperParamNewSearch = 'newsearch';
	var SearchMapperParamArticleNumber = 'arnumber';	
		
	
	
	var NTPT_IMAGE_LOCATION = '';
	var XPLORE_SSL_HOST = 'https://ieeexplore.ieee.org';	
	
	var SSL_YES_NO = 'yes';
	if (SSL_YES_NO.toUpperCase() == "NO"){
		var XPLORE_SSL_YES_NO = false;
	}else{
		var XPLORE_SSL_YES_NO = true;
	}
	var WEBSERVICES_SSL_YES_NO = 'yes';
	if (WEBSERVICES_SSL_YES_NO.toUpperCase() == "NO"){
		var XPLORE_WEBSERV_YES_NO = false;
	}else{
		var XPLORE_WEBSERV_YES_NO = true;
	}
	
	var AUTHOR_PROFILE = 'ON';
	if (AUTHOR_PROFILE.toUpperCase() == "OFF"){
		var AUTHOR_PROFILE_ENABLED = false;
	}else{
		var AUTHOR_PROFILE_ENABLED = true;
	}

	var IMAGE_SEARCH_FLAG = 'OFF';
	if (IMAGE_SEARCH_FLAG.toUpperCase() == "ON"){
		var IMAGE_SEARCH_ENABLED = true;
	} else {
		var IMAGE_SEARCH_ENABLED = false;
	}
	
	var ILN_ENABLED = true;
	
	
	var IBP_MEMBEER_SIGNIN_TIME_WAIT_IN_MILLIES = '800';
	var ASSETS_RELATIVE_PATH = '/assets'; // NOTE: AngularJS code relies on this
	var ASSETS_RELATIVE_PATH_NO_SERVER = '/assets';
	var ASSETS_VERSION = '20220125_000001'; // NOTE: AngularJS code relies on this
	var IBP_WS_ASSETS='https://www.ieee.org';
	var IBP_WS_ENABLED_FLAG = true;
	var ENTERPRISE_CART_URL = 'https://www.ieee.org/cart/public/myCart/page.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE%20Xplore';
	var IEEE_USER_INFO_COOKIE = 'ieeeUserInfoCookie';
	
	var ACC_MGMT_NEW = true;
	if (! XPLORE_WEBSERV_YES_NO) {
		var ACC_MGMT_NEW = 'false';
	}
	var ACC_MGMT_CREATE_URL = 'https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=xplore&car=IEEE-Xplore&autoSignin=Y';
	var ACC_MGMT_FORGOT_PASSWD_URL = 'https://www.ieee.org/profile/public/forgotpassword/forgotUsernamePassword.html?sourceCode=xplore';

	var SPECIAL_CHARACTER_MAPS = '&:.AND.,=:.EQ.,+:.PLS.,#:.HSH.';
	var SPECIAL_CHARACTERS = new Array();
	var SPECIAL_CHARACTER_REPLACEMENTS = new Array();
	var characterMaps = SPECIAL_CHARACTER_MAPS.split(",");
	for (var i = 0; i < characterMaps.length; i++) {
		parts = characterMaps[i].split(":");
		SPECIAL_CHARACTERS[i] = parts[0];
		SPECIAL_CHARACTER_REPLACEMENTS[i] = parts[1];
	}

	var MAX_WILDCARDS = '7';
	var MAX_SEARCH_TERMS = '20';
	var ALL_SEARCH_FIELDS = 'Search_Index_Terms:Index Terms,Search_All_Text:Full Text & Metadata,Search_All:All Metadata,fullText:Full Text Only,Search_Publication_Title:Search Publication Title,Search_Related_Terms:Search Related Terms,Search_Authors:Author Name,Search_Standard_Ics_Title:ICE Terms,Search_Unsilo:Search Unsilo,Search_Mai:Search MAI,promoTopic:Custom Search,id:Article Number,moduleNumber:Module Number,part_num:Part_Num,issn:ISSN,isbn:ISBN,eisbn:EISBN,issueId:IS Number,pubIssueId:Pub Issue Id,accessionNumber:Accession Number,copyrightHolder:Copyright Holder,copyrightYear:Copyright Year,license:License,documentAbstract:Abstract,publicationId:Publication Number,parentPublicationId:Parent Publication Number,parentId:Parent Id,standardArticleId:Standard Article Id,title:Document Title,parentTitle:Parent Publication Title,parentDisplayTitle:Parent Display Title,publicationTitle:Publication Title,publicationDisplayTitle:Publication Display Title,volume:Volume,issue:Issue,paddedIssueNumber:Padded Issue Number,part:Part,startPage:Start Page,endPage:End Page,filePath:File Path,publicationDate:Publication Date,PublicationYear:Publication Year,onlineDate:Online Date,month:Month,Author:Author Pref Names,authorNormNames:Author Norm Names,author:Authors,authorIds:Author Ids,firstName:First Name,middleName:Middle Name,lastName:Last Name,authorAffiliations:Author Affiliations,authorBio:Author Bio,authorImg:Author Img,referenceCount:Reference Count,citationCount:Citation Count,downloadCount:Download Count,patentCount:Patent Count,multimediaFlag:Multimedia Flag,biomedicalEngFlag:Biomedical Eng Flag,nonIeee:Non IEEE,stdsProductNumber:STDS Product Number,bmsProductNumber:Bms Product Number,status:Status,doi:DOI,articleDoi:Article DOI,publicationDoi:Publication DOI,pdfPath:Pdf Path,pdfSize:Pdf Size,contentSubtype:Content Subtype,Publisher:Publisher,ControlledTerms:INSPEC Controlled Terms,freeTerms:INSPEC Non-Controlled Terms,ieeeTerms:IEEE Terms,authorTerms:Author Keywords,maiTerms:MAI Terms,meshTerms:Mesh_Terms,pacsTerms:PACS Terms,insertDate:Insert Date,ConferenceLocation:Conference Location,indexContent:Index Content,coden:CODEN,documentText:Document Text,standardNumber:Standard Number,preprintFlag:Preprint Flag,rapidPostFlag:Rapid Post Flag,lastUpdate:Last Update,newFlag:New Flag,openAccessFlag:Open Access Flag,publicationOpenAccess:Publication Open Access,promoFlag:Promo Flag,pubmedId:Pubmed Id,duration:Duration,society:Society,conference:Conference,ConferenceCountry:ConferenceCountry,conferenceStartDate:Conference Start Date,conferenceEndDate:Conference End Date,societyUrl:Society URL,idSubject:Id Subject,bookNumber:Book Number,pages:Pages,editionNumber:Edition Number,sequence:Sequence,relatedInfoType:Related Info Type,relatedInfo:Related Info,formatIsbn:Format ISBN,meetingDate:Meeting Date,courseLevel:Course level,courseParts:Course Parts,courseId:Course ID,aboutUrl:About Url,additionalUrl:Additional Url,authorsUrl:Authors Url,openAccessUrl:Open Access Url,openAccessFlag:Open Access flag,partnumVendorurlMediatype:Partnum VendorURL MediaType,brandingImageFile:Branding Image File,coverImageFile:Cover Image File,frequency:Frequency,fieldOfInterest:Field Of Interest,gParentPublicationNumber:G Parent Publication Number,msUrl:Ms Url,publicationRelationship:Relationship,societyImage:Society Image,visitUrl:Visit Url,visitWebsite:Visit Website,startYear:Start Year,endYear:End Year,publicationInactive:Publication Inactive,recordType:Record Type,epub:Epub,picCodeDescription:Pic Code Description,picCode:Pic Code,sponsors:Sponsors,issueNotes:Notes,conferenceDate:Conference Date,publicationContact:Publication Contact,isbuyable:isBuyable,standardRelationships:Standard Relationships,unavailableForSale:Unavailable for Sale,availableForSale:Available for Sale,standardFamily:Standard Family,standardGroup:Standard Group,productUrl:Product Url,isbnMediatype:ISBN MediaType,htmlFlag:Html Flag,rightslinkFlag:Rightslink Flag,pageCount:Page Count,name:Name,tableOfContents:Table of Contents,timeStamp:Time Stamp,subTitle:Sub Title,relatedItem:Related Item,referenceMaterial:Reference Material,latestIssueTime:Latest Issue Time,startYear:First Published Year,insertDate:Search Latest Date,pbdFlag:Pbd Flag,lmsUrl:Lms Url,currentVolume:Current Volume,graphicalFile:Graphical File,graphicalCoverImage:Graphical Cover Image,graphicalSummary:Graphical Summary,graphicalType:Graphical Type,authorNativeNames:Native Name,externalId:Article Page Number,standardBundles:Standard Bundles,standardBundleParts:Standard Bundle Parts,virtualTitle:Virtual Title,seriesName:Series Name,seriesId:Series Id,mlHtmlFlag:ML Html flag,promoDates:Promo Dates,promoStartDate:Promo Start Date,promoEndDate:Promo End Date,issueCompleteFlag:Issue Complete Flag,scope:Scope,purpose:Purpose,standardFamilyTitle:Standard Family Title,thumbnailImg:Thumbnail Img,supplementFile:Supplement File,courseFile:Course File,supplement:Supplement,courseAuthor:Course Author,pdhs:Pdhs,ceus:Ceus,courseFirstFrame:FirstFrame Img,idSubTopic:Id Sub Topic,certificateUrl:Certificate Url,standardRoot:Standard Root,icsTerms:Standards ICS Terms,impactStatement:Impact Statement,plagiarizedFlag:Plagiarized Flag,affirmedDate:Affirmed Date,sourcePdf:Source Pdf,orcid:Author ORCID,algorithmFlag:Algorithm Flag,fundingAgency:Funding Agency,funder:Funder,pricingKey:Map Pricing Key,publicationRollup:Rollup Key,collection:Collection,previewImage:Preview Image,regularDate:Regular Date,ContentType:Content Type,ringgoldIds:Ringgold ID,figures:Figures,mediaPath:Media Path,publicationVolumeOnly:Publication Volume Only,Search_All:All Metadata,Search_All_Text:Full Text & Metadata,fullText:Full Text Only,unsiloTerm:Unsilo Term,hasUnsilo:Has Unsilo,TypeAheadTerms:Type Ahead Terms,IeeeTerm:IEEE Term,ContentType:Content Type,Author:Author,Affiliation:Affiliation,Topic:Topic,PublicationTitle:Publication Title,PublicationYear:Year,Publisher:Publisher,ConferenceCountry:Conference Country,ConferenceLocation:Conference Location,StandardStatus:Standard Status,ConferenceYear:Conference Year,PublicationPackage:Subscribed Content,StandardPackage:Standard Package,ReadingRoom:Reading Room,StandardTitle:Standard Title,StdDictionaryTerms:Standards Dictionary Terms,TitleRange:Publication Range,StandardRange:Standard Range,RecordType:Record Type,MediaType:Media Type,BookType:Book Type,CourseType:Course Type,PublicationStandardRange:Publication Standard Range,PerpetualYear:Perpetual Year,OpacTitleRange:Opac Title Range,BookSeries:Book Series,SubjectCategory:Subject Category,Sessions:Sessions,TypeAheadPublication:Type Ahead Publication,SubTopic:Sub Topic,CourseDuration:Course Duration,CourseLevel:Course Level,StandardType:Standard Type,StandardSubtype:Standard SubType,StandardModifier:Standard Modifier,IcsTerms1:Ics Terms 1,IcsTerms2:Ics Terms 2,IcsTerms3:Ics Terms 3,SupplementalItem:Supplemental Items,CourseDuration:Course Duration,SpecialSection:Topics,SocietySection:Society Sections,ControlledTerms:Publication Topics';
	var SEARCH_FIELD_REFERENCES = new Array();
	var SEARCH_FIELD_DISPLAYS = new Array();
	var searchFields = ALL_SEARCH_FIELDS.split(",");
	for (var j = 0; j < searchFields.length; j++) {
		parts = searchFields[j].split(":");
		SEARCH_FIELD_REFERENCES[j] = parts[0];
		SEARCH_FIELD_DISPLAYS[j] = parts[1];
	}
	

	var refSite='https://ieeexplore.ieee.org';
	var refSiteName="IEEE Xplore";
	var applicationName = 'Xplore';
	var MC_OPERATION_DELAY_TIMEOUT='5000';
	var MC_ADDING_DELAY_MSG='Please wait.The selected item(s) is being added to the cart.';
	var MC_TIMEOUT='60000';
	var MC_OPERATION_DELAY_MSG_FLAG='true';

	var MEMBER_PROFILE_CHANGE_USERNAMEPASS_LINK = 'https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_MY_PROFILE ='https://www.ieee.org/profile/myprofile/myprofile.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_PROFILE_ADDRESSINFO_LINK = 'https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_PROFILE_PAYMENTINFO_LINK = 'https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_PROFILE_ORDER_HISTORY_LINK = 'https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_USER_COMMUNICATION_LINK = 'https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_EDUCATIONAL_PROFILE_LINK = 'https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var MEMBER_TECHNICAL_INTERESTS_LINK = 'https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org&refSiteName=IEEE Xplore';
	var HELPFILE_RELATIVE_PATH = '/Xplorehelp';
	
	//content types
	var CONTENT_TYPE_PARAM = 'contentType';
	var CONTENT_TYPE_BOOKS = 'Books';
	var CONTENT_TYPE_COURSES = 'Courses';
	var CONTENT_TYPE_STANDARDS = 'Standards';
	var CONTENT_TYPE_CONFERENCES = 'Conferences';
	var CONTENT_TYPE_JOURNALS = 'Journals';
	var CONTENT_TYPE_EARLY_ACCESS = 'Early Access Articles';
	
	//User Preferences
	var citFormat = "";
	var dlFormat = "";
	var myProjectLimit = 15;
	var myProjectDocumentLimit = 1000;
	var myProjectDocumentTagLimit = 50;
	
	
	//Google ReCaptcha public Key 
	var RECAPTCHA_PUBLIC_KEY = '6Ld6GEUUAAAAALdaAmeGUhZyz1KFFHnd5oCaTW-t';
</script>

					
					
								
			
				
					
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/history.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/cartScroll.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/minicart.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/core.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/ads.js" charset="utf-8"></script>
					
								
			
				
					
					
								<script type="text/javascript" src="./全景摄像头_files/master.js" charset="utf-8"></script>
					
								
			
		
	






<script>

j$('document').ready(function(){
	if (Cookies.get('legacyUserName')) {
		if (IBP_WS_ENABLED_FLAG){
			Modal.refreshLegacyAccountTransition('/xpl/mwLegacyAccountTransition.jsp');
		}
	}
});

</script>












</g:compress>








		
			
					
			
					
			
					
			
				
					







<script>
	// Get/Set XPL namespace.
	window.xpl = window.xpl || {};
	
	// Set constants/application properties.
	xpl.properties = { 
			collabratec: { 
					url: 'https://ieee-collabratec.ieee.org/app/library'
			}
	};
	
	xpl.properties.details = {
			oqs: ''
	};
</script>
					
			
					
			
					
			
					
			
					
			
					
			
					
			
		
		
	





	<!--Begin Optional Configuration-->
	<script type="text/javascript" src="./全景摄像头_files/jquery.json-2.2.min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/postmessage-min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/jquery.cookie-min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/ieee-auth-constants-min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/ieee-auth-include-min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/ieee-mini-cart-constants-min.js"></script>


	<script type="text/javascript" src="./全景摄像头_files/ieee-mashup-util-min.js"></script>
	<script type="text/javascript" src="./全景摄像头_files/jquery.ba-jqmq-min.js"></script>
	<script type="text/javaScript" src="./全景摄像头_files/ieee-minicart-include-min.js"> </script>

	<!--End Optional Configuration-->




<!-- Removed due to network issues when loading in China -->
<!-- <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid=ra-5005a435228f9245" async="async"></script>-->

<!-- Load Mathjax and process the document for Mathjax characters -->


<!-- <script type="text/javascript" src="/xploreAssets/MathJax-274/MathJax.js?config=default"></script> -->


<script defer="">
	MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.body]);
</script>



					</div>
				</div>
			</div>

			
				<script>
					// set $ alias back to jQuery because noConflict mode is used in legacy pages
					window.$ = jQuery;
				</script>
			
		</div><!-- /#LayoutWrapper -->
	
<div id="gs-casa-r" style="top: 18440.5px;"><div id="gs-casa-c" class="gs-casa-mid"><div id="gs-casa-b"><a id="gs-casa-f" href="https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel7/6287639/8948470/09046805.pdf&amp;hl=en&amp;sa=T&amp;oi=ucasa&amp;ct=ufr&amp;ei=Ci_yYYmrBIqL6rQP0-6_iAw&amp;scisig=AAGBfm0bH-8Vk4pjyBq41-PO0Srjh0imPg">PDF</a><a id="gs-casa-h" target="_blank" href="https://scholar.google.com/scholar/help.html#access">Help</a></div></div></div><iframe sandbox="allow-scripts allow-same-origin" title="Adobe ID Syncing iFrame" id="destination_publishing_iframe_ieeexplore_undefined" name="destination_publishing_iframe_ieeexplore_undefined_name" src="./全景摄像头_files/dest5.html" style="display: none; width: 0px; height: 0px;" class="aamIframeLoaded"></iframe><div id="cboxOverlay" style="display: none;"></div><div id="colorbox" class="" role="dialog" tabindex="-1" style="display: none;"><div id="cboxWrapper"><div><div id="cboxTopLeft" style="float: left;"></div><div id="cboxTopCenter" style="float: left;"></div><div id="cboxTopRight" style="float: left;"></div></div><div style="clear: left;"><div id="cboxMiddleLeft" style="float: left;"></div><div id="cboxContent" style="float: left;"><div id="cboxTitle" style="float: left;"></div><div id="cboxCurrent" style="float: left;"></div><button type="button" id="cboxPrevious"></button><button type="button" id="cboxNext"></button><button id="cboxSlideshow"></button><div id="cboxLoadingOverlay" style="float: left;"></div><div id="cboxLoadingGraphic" style="float: left;"></div></div><div id="cboxMiddleRight" style="float: left;"></div></div><div style="clear: left;"><div id="cboxBottomLeft" style="float: left;"></div><div id="cboxBottomCenter" style="float: left;"></div><div id="cboxBottomRight" style="float: left;"></div></div></div><div style="position: absolute; width: 9999px; visibility: hidden; display: none; max-width: none;"></div></div><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Math-italic, sans-serif;"></div></div><iframe style="display: none;" src="./全景摄像头_files/saved_resource(3).html"></iframe><iframe style="display: none;" src="./全景摄像头_files/saved_resource(4).html"></iframe><div class="usabilla_live_button_container" id="usabilla_live_button_container_429212215" role="button" tabindex="0" aria-label="Usabilla Feedback Button"><style type="text/css" nonce="">div.usabilla_live_button_container#usabilla_live_button_container_429212215[role="button"] {bottom:0;left:95%;margin-left:-62.5px;width:125px;height:50px;position:fixed;z-index:999}</style><iframe src="./全景摄像头_files/saved_resource(5).html" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" data-tags="bottom" title="Usabilla Feedback Button" class="usabilla-live-button" id="usabilla_live_button_container_iframe661210976"></iframe></div></body></html>
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "## 神经元模型\n",
    "\n",
    "神经网络（neural networks）方面的研究很早就出现了，今天“神经网络”已经是一个相当大的、多学科交叉的学科领域。\n",
    "\n",
    "各种相关学科对神经网络的定义多种多样，我们采用目前使用的最广泛的一种：\n",
    "\n",
    "**神经网络是由具有适应性的、简单单元组成的、广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反映。**[Koheonen，1988]\n",
    "\n",
    "我们在机器学习中谈论神经网络时，指的是“神经网络学习”，或者说是，机器学习于神经网络这两个学科领域的交叉部分。\n",
    "\n",
    "**神经网络中，最基本的成分是神经元（neuron）模型**，即上述定义中的“简单单元”。\n",
    "\n",
    "![生物神经元](images/neuralnetwork/生物神经元.jpg)\n",
    "\n",
    "在生物神经网络中，每个神经元与其它神经元相连，当他兴奋时，就会像相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”（threshold）也称为bias，那么他就会被激活，即“兴奋”起来，像其它神经元发送化学物质。\n",
    "\n",
    "![兴奋的生物神经元](images/neuralnetwork/兴奋的生物神经元.jpg)\n",
    "\n",
    "1943年，[McCulloch and Pitts，1943]将上述情景抽象为下图所示的简单模型。\n",
    "\n",
    "![mp神经元模型](images/neuralnetwork/mp神经元模型.png)\n",
    "\n",
    "\n",
    "这就是沿用至今的“M-P神经元模型”。在这个模型中，神经元接收到来自n个其它神经元传递过来的输入信号，这些输入信号通过带权重的链接（connection）进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过“激活函数”（activation function)处理以产生神经元的输出。\n",
    "\n",
    "理想中的激活函数是下图5.2a所示的阶跃函数，它将输入值映射为输出值“0”或“1”，显然：\n",
    "\n",
    "- “1”对应于神经元兴奋\n",
    "- “0”对应于神经元抑制\n",
    "\n",
    "然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用Sigmoid函数如图5.2b所示，他把可能在较大范围内变化的输入值挤压到（0，1）输出值范围内，因此有时也成为“挤压函数”（squashing function）。\n",
    "\n",
    "![典型神经元激活函数](images/neuralnetwork/典型神经元激活函数.png)\n",
    "\n",
    "**把许多的神经元按一定的层次结构连接起来，就得到了神经网络。**\n",
    "\n",
    "事实上，从计算机科学角度看，我们可以先不考虑神经网络是否真的模拟了生物神经网络，只需**将一个神经网络视为包含了许多参数的数学模型，这个模型是若干个函数**，例如: $y_i = f(\\sum_i w_ix_i -\\theta_j)$ 相互（嵌套）代入而得。\n",
    "\n",
    "例如：10个神经元两两连接，则有100个参数：90个连接权重和10个阈值。\n",
    "\n",
    "有效的神经网络学习算法大多以数学证明为支撑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机与多层网络\n",
    "\n",
    "### 感知机（两层网络）\n",
    "\n",
    "感知机（Perceptron）由两层神经元组成，如下图所示：\n",
    "\n",
    "输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”（threshold logic unit）。\n",
    "\n",
    "感知机能容易地实现逻辑与、或、非运算。注意到 $y_i = f(\\sum_i w_ix_i -\\theta_j)$ ，假定f是图5.2中的阶跃函数，有：\n",
    "\n",
    "- “与”（$x_1 \\cap x_2 $）: 令 $w_1 = w_2 = 1,\\theta = 2$, $则 y = f(1 \\centerdot x_1 + 1 \\centerdot x_2 - 2)$，仅在$x_1 = x_2 = 1时，y=1；$\n",
    "- “或”（$x_1 \\cup x_2$）：令$w_1 = w_2 = 1, \\theta = 0.5$，$则 y = f(1 \\centerdot x_1 + 1 \\centerdot x_2 - 0.5), 当 x_1 = 1 或 x_2 = 1时，y=1$；\n",
    "- “非”（$-x_1$）：令$w_1 = -0.6，w_2 = 0，\\theta = - 0.5$，$则 y = f( -0.6 \\centerdot x_1 + 0 \\centerdot x_2 + 0.5)$,$当 x_1 = 1，y = 0 ; 当 x_1 = 0，y = 1 。$\n",
    "\n",
    "\n",
    "![两个输入神经元的感知机网络结构示意图](images/neuralnetwork/两个输入神经元的感知机网络结构示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们直接指定了连接权重$w_{i}$，而更一般地，给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\\theta$可通过学习得到。\n",
    "\n",
    "阈值$\\theta$可看作一个固定输入为 -1.0 的**哑节点（dummy node）**所对应的连接权重$w_{n+1}$，这样，权重和阈值的学习就可以统一为权重的学习。\n",
    "\n",
    "感知机学习规则非常简单，对训练样例（x,y），若当前感知机的输出为$\\hat y$，则感知机权重将这样调整：\n",
    "\n",
    "$w_i \\gets w_i  +  \\Delta w_i$  ——式（1）\n",
    "\n",
    "$\\Delta w_i = \\eta(y - \\hat y)x_i$ ——式（2）\n",
    "\n",
    "其中，$\\eta \\in (0,1)$称为学习率（learning rate）。从式（1）可以看出，若感知机对训练样例（x,y）预测正确，即$\\hat y = y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。\n",
    "\n",
    "需注意的是，感知机只有输出层神经元进行激活函数处理，即只有一层功能神经元（functional neuron），其学习能力非常有限。\n",
    "\n",
    "事实上，上述与、或、非问题都是线性可分（linear separable）的问题。可以证明，若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如5.4（a)-(c)所示，则感知机的学习过程一定会收敛（converge）而求得适当的权向量 $w = (w_1;w_2;...;w_{n+1})$；否则感知机学习过程将会发生振荡（fluctuation），$w$难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图5.4(d)所示的异或这样简单的非线性可分问题。\n",
    "\n",
    "![线性可分或不可分示意图](images/neuralnetwork/线性可分或不可分示意图.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn中的Perception模型\n",
    "\n",
    "感知机模型作为单层的人工神经网络，实际上是一个线性模型，感知机可说是最简单的前向人工神经网络形式。\n",
    "\n",
    "\n",
    "Sklearn中有其实现。使用它时不需要预设学习率$\\eta$，连接权重$w_i,i = 1,2,...,n $、阈值$\\theta $ 可以通过coef_和intercept_属性得出。\n",
    "\n",
    "连接权重与阈值的选择是通过训练集得出的，其值的更新依赖于预测错误的反馈，即上式（1）和（2）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 1 1 1 2 0 2 0 0 1 2 2 2 2 1 2\n",
      " 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0 1 0 2 1 0 1 2 1 0 2 2 2 2 0 0\n",
      " 2 2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0\n",
      " 2 1 1 1 2 2 1 1 0 1 2 2 0 1 1 1 1 0 0 0 2 1 2 0]\n",
      "训练后模型对象： Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n",
      "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
      "      validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "连接权重： [[  0.3   4.4  -7.8  -3.4]\n",
      " [ -0.4 -29.4   8.2 -22.8]\n",
      " [-38.1 -35.7  57.7  54.1]]\n",
      "阈值： [  1.  11. -24.]\n",
      "预测输出： [2 0 0 2 0 2 0 0 0 0 2 0 2 2 2]\n",
      "真实结果： [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1]\n",
      "测试得分： 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "# 使用留出法划分训练集与测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, \n",
    "                                                    test_size=0.1, random_state=0)\n",
    "clf = Perceptron(tol=1e-3, random_state=0)#tol停止标准,程序迭代将在（loss> previous_loss - tol）时停止\n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "print(\"训练后模型对象：\",clf)\n",
    "print(\"连接权重：\",clf.coef_)\n",
    "print(\"阈值：\",clf.intercept_)\n",
    "print(\"预测输出：\",clf.predict(X_test))\n",
    "print(\"真实结果：\",y_test)\n",
    "print(\"测试得分：\",clf.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多层网络\n",
    "\n",
    "要解决非线性可分问题，需要使用多层功能神经元。例如图5.5重这个简单的两层感知机就能解决异或问题。\n",
    "\n",
    "在图5.5a中，输出层与输入层之间的一元神经元，被称为**隐层或隐含层（hidden layer）**，隐含层和输出层都是拥有激活函数的功能神经元。\n",
    "\n",
    "![能解决异或问题的两层感知机](images/neuralnetwork/能解决异或问题的两层感知机.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更一般的，常见的神经网络是形如图5.6所示的层级结构，每层神经元与下一层神经元全连接，神经元间不存在同层连接，也不存在跨层连接。\n",
    "\n",
    "这样的神经网络结构通常称为“多层前馈神经网络”（multi-layer feedforward neural networks），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。\n",
    "\n",
    "![多层前馈神经网络结构示意图](images/neuralnetwork/多层前馈神经网络结构示意图.png)\n",
    "\n",
    "换言之，**输入神经元，仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元。**\n",
    "\n",
    "因此，图5.6a通常被称为“两层网络”。为避免歧义，我们可称其为“单隐层网络”。只需包含隐层，就可成为多层网络。\n",
    "\n",
    "神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight）以及每个功能神经元的阈值；换言之，神经网络“学到”的东西，蕴含在连接权与阈值中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用sklearn中的多层感知器进行分类和回归\n",
    "\n",
    "多层感知器MLP是一种监督学习算法，通过在数据集上训练来学习函数$f:X \\to y$,其中 X 是n维的输入，y是输出。\n",
    "\n",
    "中间隐藏层可能是多层。激活函数可以是双曲正切函数tanh。输出层接收到最后一个隐藏层的输出，经过变换得到y。\n",
    "\n",
    "sklearn中MLP模型，包含权重矩阵coefs_和阈值向量intercepts_。\n",
    "\n",
    "它的优点：\n",
    "\n",
    "- 可以学习到非线性模型，既可用于分类，也可用于回归；\n",
    "\n",
    "- 使用“partial_fit”可以学习得到实时模型（在线学习）\n",
    "\n",
    "它的缺点：\n",
    "\n",
    "- 具有隐藏层的 MLP 具有非凸的损失函数，它有不止一个的局部最小值。 因此不同的随机权重初始化会导致不同的验证集准确率。\n",
    "- MLP 需要调试一些超参数，例如隐藏层神经元的数量、层数和迭代轮数。\n",
    "- MLP 对特征归一化很敏感.\n",
    "\n",
    "#### 分类任务\n",
    "\n",
    "MLPClassifier 类实现了通过 Backpropagation 进行训练的多层感知器（MLP）算法。\n",
    "\n",
    "下面举例说明：\n",
    "\n",
    "MLP 在两个 array 上进行训练:尺寸为 (n_samples, n_features) 的 array X 储存表示训练样本的浮点型特征向量; 尺寸为(n_samples,) 的 array y 储存训练样本的目标值（类别标签）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', \n",
    "                    alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拟合（训练）后，该模型可以预测新样本的标签:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.], [-1., -2.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 可以为训练数据拟合一个非线性模型。clf.coefs_ 包含了构建模型的权值矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后模型对象： MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5, 4), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "连接权重： [array([[-0.13547367, -1.27651027, -0.81613612, -2.09678145, -0.57672248],\n",
      "       [-0.66556664, -3.77078763, -0.25214462, -0.91783163,  0.06337397],\n",
      "       [-0.13192673,  5.01761865, -0.48252477, -0.91884437, -0.77160862],\n",
      "       [ 0.27831305, 10.44489359,  0.0958197 , -1.0907232 , -0.49289331]]), array([[ 0.64425287, -0.67747579, -0.75256023, -0.53904995],\n",
      "       [ 1.36942072, -1.8496962 , -0.1288033 ,  2.68887202],\n",
      "       [ 0.05414716,  0.31326735, -0.30119761,  0.30448994],\n",
      "       [ 0.75497471, -0.78646458,  0.40839704,  0.61060245],\n",
      "       [ 0.40516659, -0.35845717,  0.47229065, -0.64779136]]), array([[ -9.1782875 ,   2.80387053,   5.13080726],\n",
      "       [  2.92658208,  -2.96169265,  -0.94836781],\n",
      "       [ -0.8268359 ,   0.13720994,  -0.65399239],\n",
      "       [-10.61876643,   5.47316518,   4.94455281]])]\n",
      "阈值： [array([ 0.49111382, -1.00972315, -0.30467704,  0.0533463 ,  0.61464091]), array([-13.15162989,   0.74910556,  -0.33702668,  -1.09909913]), array([ 20.05959773,  -5.17426176, -14.84342332])]\n",
      "预测输出： [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1]\n",
      "真实结果： [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1]\n",
      "测试得分： 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "# 使用留出法划分训练集与测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, \n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, \n",
    "                    hidden_layer_sizes=(5, 4), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)  \n",
    "\n",
    "print(\"训练后模型对象：\",clf)\n",
    "print(\"连接权重：\",clf.coefs_)\n",
    "print(\"阈值：\",clf.intercepts_)\n",
    "print(\"预测输出：\",clf.predict(X_test))\n",
    "print(\"真实结果：\",y_test)\n",
    "print(\"测试得分：\",clf.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前， MLPClassifier 只支持交叉熵损失函数，通过运行 predict_proba 方法进行概率估计。\n",
    "MLP 算法使用的是反向传播的方式。 更准确地说，它使用了通过反向传播计算得到的梯度和某种形式的梯度下降来进行训练。 对于分类来说，它最小化交叉熵损失函数，为每个样本 X 给出一个向量形式的概率估计 $P(y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归\n",
    "\n",
    "sklearn中提供了一个MLPRegressor实现神经网络回归，该模型使用LBFGS或随机梯度下降来优化平方损失。\n",
    "\n",
    "MLPRegressor 类实现了一个多层感知器（MLP），它在使用反向传播进行训练时的输出层没有使用激活函数，也可以看作是使用身份函数作为激活函数。 因此，它使用平方误差作为损失函数，输出是一组连续值。\n",
    "\n",
    "MLPRegressor 还支持多输出回归，其中样本可以有多个目标。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "属性名称 ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Training MLPRegressor...\n",
      "真实结果：[1.12  1.072 1.156 0.983 1.168 0.781 0.771 0.923 0.847 0.894]\n",
      "预测结果： [1.30164338 1.26350374 1.11960655 0.91358225 1.13071638 0.75000001\n",
      " 0.86915773 0.89814056 0.98371827 1.05348554]\n",
      "测试评分： 0.3543519520684655\n",
      "测试MSE： 0.013254282799169691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "X, y = cal_housing.data, cal_housing.target\n",
    "\n",
    "names = cal_housing.feature_names\n",
    "print(\"属性名称\",names)\n",
    "\n",
    "print(\"Training MLPRegressor...\")\n",
    "est = MLPRegressor(activation='logistic')\n",
    "est.fit(X[:-10], y[:-10])\n",
    "print(\"真实结果：{}\".format(y[-10:]))\n",
    "print(\"预测结果：\",est.predict(X[-10:]))\n",
    "#print(\"测试评分：\",est.score(X[-10:],y[-10:]))\n",
    "print(\"测试MSE：\",mean_squared_error(est.predict(X[-10:]), y[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差逆传播算法（BP神经网络）\n",
    "\n",
    "多层网络的学习能力比单层感知机强得多。\n",
    "\n",
    "欲训练多层网络，式（1）的简单感知机学习规则显然不够了，需要更强大的学习算法。\n",
    "\n",
    "误差逆传播（Error BackPropagation，简称BP）算法就是其中最杰出的代表，它是迄今最成功的神经网络学习算法。\n",
    "\n",
    "现实任务中使用的神经网络时，大多是在使用BP算法进行训练。\n",
    "\n",
    "值得指出的是，BP算法不仅可用于多层前馈神经网络，还可用于其它类型的神经网络，一般是指用BP算法训练的多层前馈神经网络。\n",
    "\n",
    "下面我们来看看BP算法究竟是什么样的。\n",
    "\n",
    "给定训练集$D = {(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}, x_i \\in R^d ,y_i \\in R^l$，即输入示例由$d$个属性描述，输出$l$维实值向量。\n",
    "\n",
    "为了方便讨论，图5.7给出了一个拥有 d 个输入神经元，$l$个输出神经元、$q$个隐层神经元的多层前馈网络结构。其中，输出层第 j 个神经元的阈值用$\\theta_j$表示，隐层第 h 个神经元的阈值用 $\\gamma_h$表示。\n",
    "\n",
    "![BP网络及算法中的变量符号](images/neuralnetwork/BP网络及算法中的变量符号.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入层第 i 个神经元与隐层第 h 个神经元之间的连接权为 $v_{ih}$，隐层第 h 个神经元与输出层第 j 个神经元之间的连接权为 $w_{hj}$。\n",
    "\n",
    "记隐层第 h 个神经元接收到的输入为 $\\alpha_h = \\sum_{i=1}^d v_{ih}x_i$，输出层第 j 个神经元接收到的输入为 $\\beta_j = \\sum_{h=1}^q w_{hj}b_h$，其中$b_h$为隐层第h个神经的输出。假设隐层和输出层神经元都是用图5.2b中的Sigmoid函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对前向传播和反向传播的解释\n",
    "\n",
    "前向传播与后向传播都是为了减小误差（Error）而进行的处理方式，目的是得到最优的全局参数矩阵。\n",
    "\n",
    "- **前向传播，是指从输入信号直至输出神经元，产生了误差；**\n",
    "- **后向传播，是指至少误差之后调整连接权重（权重矩阵），使模型更加优化。**\n",
    "\n",
    "为了便于理解，下面举一个猜数字的例子：\n",
    "\n",
    "1. 两个人猜数字\n",
    "\n",
    "这代表了简单的两层神经网络，此时没有隐藏层，只有输入层、输出层。\n",
    "\n",
    "其中小黄帽代表输出层节点，左侧接受输入信号，右侧产生输出结果，小蓝猫则代表了误差，指导参数往更优的方向调整。由于小蓝猫可以直接将误差反馈给小黄帽，同时只有一个参数矩阵和小黄帽直接相连，所以可以直接通过误差进行参数优化（实纵线），迭代几轮，误差会降低到最小。\n",
    "\n",
    "![两人猜数字](images/neuralnetwork/两人猜数字.png)\n",
    "\n",
    "\n",
    "2. 三个人猜数字\n",
    "\n",
    "这代表了简单的三层神经网络，此时有输入层、隐藏层、输出层。\n",
    "\n",
    "其中小女孩代表隐藏层节点，小黄帽依然代表输出层节点，小女孩左侧接受输入信号，经过隐层节点产生输出结果，小蓝猫代表了误差，指导参数往更优的方向调整。由于小蓝猫可以直接将误差反馈给小黄帽，所以与小黄帽直接相连的左侧参数矩阵可以直接通过误差进行参数优化（实纵线）；而与小女孩直接相连的左侧参数矩阵由于不能得到小蓝猫的直接反馈而不能直接被优化（虚棕线）。但由于反向传播算法使得小蓝猫的反馈可以被传递到小女孩那进而产生间接误差，所以与小女孩直接相连的左侧权重矩阵可以通过间接误差得到权重更新，迭代几轮，误差会降低到最小。\n",
    "\n",
    "![三人猜数字](images/neuralnetwork/三人猜数字.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练样例$(x_k,y_k)$,假定神经网络的输出为$\\hat y_k = (\\hat y_1^k,\\hat y_2^k,...,\\hat y_l^k)$，即：\n",
    "\n",
    "$\\hat y_j^k = f(\\beta_j - \\theta_j)$  ——式（3）\n",
    "\n",
    "则网络在$(x_k,y_k)$上的均方误差为：\n",
    "\n",
    "$ E_k = \\frac{1}{2 l}\\sum_{j=1}^{l}(\\hat y_j^k - y_j^k)^2$  ——式（4）\n",
    "\n",
    "注：1/2 是为了后续求导方便。\n",
    "\n",
    "图5.7的网络中有 $(d+l+1)q+l$ 个参数需确定：输入层到隐层的$d \\times q$个权值、隐层到输出层的$q \\times l$个权重、q 个隐层神经元的阈值、l 个输出层神经元的阈值。\n",
    "\n",
    "BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，即与式（1）类似，任意参数 $v$ 的更新估计式为：\n",
    "\n",
    "$v \\gets  v  +  \\Delta v$  ——式（5）\n",
    "\n",
    "下面，我们可以图5.7中隐层到输出层的连接权$w_{hj}$为例进行推导。\n",
    "\n",
    "**BP算法基于梯度下降（gradient descent）策略，以目标的负梯度方向对参数进行调整**。对式（4）的误差$E_k$，给定学习率$\\eta$，有：\n",
    "\n",
    "$\\Delta w_{hj} = - \\eta \\frac{\\partial E_k}{\\partial w_{hj}}$  ——式（6）\n",
    "\n",
    "注意到$w_{hj}$先影响到第 j 个输出层神经元的输入值 $\\beta_j$，再影响到其输出值$\\hat y_j^k$，然后影响到$E_k$，有：\n",
    "\n",
    "$\\frac{\\partial E_k}{\\partial w_{hj}} = \\frac{\\partial E_k}{\\partial \\hat y_j^k} \\centerdot \\frac{\\partial \\hat y_j^k}{\\partial \\beta _j} \\centerdot \\frac{\\partial \\beta _j}{\\partial w_{hj}}$  ——式（7）\n",
    "\n",
    "根据$\\beta_j$的定义，显然有：\n",
    "\n",
    "$\\frac{\\partial \\beta _j}{\\partial w_{hj}} = b_h$  ——式（8）\n",
    "\n",
    "图5.2中的Sigmoid函数有一个很好的性质：\n",
    "\n",
    "$f^{'}(x) = f(x)(1-f(x))$ ——式（9）\n",
    "\n",
    "于是根据式（4）和（3），有：\n",
    "\n",
    "$g_j = - \\frac{\\partial E_k}{\\partial \\hat y_j^k} \\centerdot \\frac{\\partial \\hat y_j^k}{\\partial \\beta _j}  = -(\\hat y_j^k - y_j^k)f^{'}(\\beta_j -\\theta_j) = \\hat y_j^k(1-\\hat y_j^k)(y_j^k- \\hat y_j^k)$ ——式（10）\n",
    "\n",
    "将式（10）、式（8）代入式（7），再代入式（6），就得到了BP算法中关于$w_{hj}$的更新公式：\n",
    "\n",
    "$\\Delta w_{hj} = \\eta g_j b_h$ ——式（11）\n",
    "\n",
    "类似可得：\n",
    "\n",
    "$\\Delta \\theta_j = - \\eta g_j $ ——式（12）\n",
    "\n",
    "$\\Delta v_{jh} = \\eta e_h x_i $ ——式（13）\n",
    "\n",
    "$\\Delta \\gamma_h = \\eta e_h $ ——式（14）\n",
    "\n",
    "式（13）和（14）中，\n",
    "\n",
    "$e_h = - \\frac{\\partial E_k}{\\partial b_h} \\centerdot \\frac{\\partial b_h}{\\partial \\alpha_h} = - \\sum_{j=1}^l \\frac{\\partial E_k}{\\partial \\beta_j} \\centerdot \\frac{\\partial \\beta_j}{\\partial b_h} f^{'}(\\alpha_h - \\gamma_h) = \\sum_{j=1}^{l}w_{hj}g_jf^{'}(\\alpha_h - \\gamma_h) = b_h(1-b_h)\\sum_{j=1}^{l}w_{hj}g_j$ ——式（15）\n",
    "\n",
    "学习率$\\eta \\in (0,1)$控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢。\n",
    "\n",
    "有时为了做精细调节，可令式（11）与（12）使用$\\eta_1$，式（13）与（14）使用$\\eta_2$，两者未必相等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP算法的工作流程\n",
    "\n",
    "对于每个训练样例，BP算法执行以下操作：\n",
    "\n",
    "![标准BP算法](images/neuralnetwork/标准BP算法.png)\n",
    "\n",
    "- 先将输入示例提供给输入层神经元；\n",
    "- 然后逐层将信号前传，直到产生输出层的结果；\n",
    "- 然后计算输出层的误差，再将误差逆向传播至隐层神经元；\n",
    "- 最后根据隐层神经元的误差来对连接权和阈值进行调整；\n",
    "- 该迭代过程循环进行，直到达到某些停止条件为止。例如训练误差已达到一个很小的值。\n",
    "\n",
    "下图，给出了在2个属性、5个样本的西瓜数据集上，随着训练轮数的增加，网络参数和分类边界的变化情况：\n",
    "\n",
    "![2个属性5个样本BP网络示例](images/neuralnetwork/2个属性5个样本BP网络示例.png)\n",
    "\n",
    "需要注意的是，BP算法的目标是要最小化训练集D上的累积误差：\n",
    "\n",
    "$E = \\frac{1}{m}\\sum_{k=1}^m E_k$  ——式（16）\n",
    "\n",
    "但我们介绍的“标准BP算法”每次仅针对一个训练样例更新连接权和阈值，也就是说，上述算法的更新规则是基于单个的$E_k$推导而得。\n",
    "\n",
    "如果类似地推导出基于累积误差最小化地更新规则，就得到了累积误差逆传播（accumulated error backpropagation）算法。累积BP算法与标准BP算法都很常用。\n",
    "\n",
    "一般说来，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新得效果可能出现“抵消”现象。\n",
    "\n",
    "因此，为了达到累积误差极小点，标准BP算法往往需要进行更多次数得迭代。\n",
    "\n",
    "累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后，才对参数进行更新，其参数更新的频率地得到。\n",
    "\n",
    "在很多任务中，累积误差下降到一定程度后，进一步下降会非常缓慢，这是标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Hornik et al. 1989]证明，只需要一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数**。\n",
    "\n",
    "然而，**如何设置隐层神经元的个数，仍是一个未决的问题，实际应用中通常靠“试错法”（trial-by-error）调整。**\n",
    "\n",
    "正是由于其强大的表示能力，BP神经网络经常遭遇过拟合。其训练误差持续降低，但测试误差却仍可能上升。\n",
    "\n",
    "有两种策略，常用来缓解BP网络的过拟合：\n",
    "\n",
    "- 第一种策略是“早停（early stopping）”，即将数据分为训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差减低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "\n",
    "- 第二种策略是“正则化（regularization）”，其最基本思想是误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。仍令$E_k$表示第k个训练样例上的误差，$w_i$表示连接权和阈值，则误差目标函数（16）改变为：\n",
    "\n",
    "$E = \\lambda \\frac{1}{m} \\sum_{k=1}^m E_k +(1-\\lambda)\\sum_i w_i^2$ ——式（17）\n",
    "\n",
    "其中$\\lambda \\in (0,1)$用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他常见神经网络\n",
    "\n",
    "神经网络模型、算法繁多，常见的有：\n",
    "\n",
    "### RBF网络\n",
    "\n",
    "RBF（Radial Basis Function，径向基函数）网络是一种单隐层前馈神经网络，它使用径向基函数(径向基函数能显示“我们距离目标有多远”,而不是logistic函数回答的[0,1]之间的值)作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。\n",
    "\n",
    "使用径向基激活函数为：\n",
    "\n",
    "$\\varphi(x) = \\sum_{i=1}^q w_i \\rho (x,c_i)$ ——式（18）\n",
    "\n",
    "其中q为隐层神经元个数，$c_i 和 w_i$分别是第i个隐层神经元所对应的中心和权重，$\\rho(x,c_i)$是径向基函数，这是某种沿径向对称的标量函数，通常定义为样本x到数据中心$c_i$之间欧式距离的单调函数。\n",
    "\n",
    "常用的高斯径向基函数形式为：\n",
    "\n",
    "$\\rho(x,c_i) = e^{-\\beta_i ||x - c_i||^2}$  ——式（18）\n",
    "\n",
    "[Park and Sandberg,1991]证明，具有足够多隐层神经的RBF网络能以任意精度逼近任意连续函数。\n",
    "\n",
    "通常采用两步过程来训练RBF网络：\n",
    "\n",
    "- 第一步，确定神经元中心$c_i$，常用的方式包含随机采样、聚类等；\n",
    "- 第二步，利用BP算法等来确定参数$w_i和\\beta_i$.\n",
    "\n",
    "逻辑激活函数的BP适用于分类决策系统，但不适用于连续变量。\n",
    "\n",
    "径向基函数能显示“我们距离目标有多远”，适用于函数逼近。\n",
    "\n",
    "### ART网络\n",
    "\n",
    "竞争型学习（competitive learning）是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其它神经元的状态被抑制。这种机制亦称“胜者通吃”（winner take all）原则。\n",
    "\n",
    "ART（Adaptive Resonance Theory，自适应谐振理论）网络是竞争型学习的重要代表。\n",
    "\n",
    "该网络由比较层、识别层、识别阈值和重叠模块构成。\n",
    "\n",
    "其中，比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类。\n",
    "\n",
    "### Boltzmann机\n",
    "\n",
    "神经网络中有一类模型是为网络状态定义一个“能量”（energy），能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。\n",
    "\n",
    "Boltzmann机就是一种“基于能量的模型”（energy-based model），常见结构如下图所示，器神经元分为两层：显层与隐层。\n",
    "\n",
    "![Boltzmann机与受限Boltzmann机](images/neuralnetwork/Boltzmann机与受限Boltzmann机.png)\n",
    "\n",
    "显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。\n",
    "\n",
    "Boltzmann机种的神经元都是布尔型的，即只能取0、1两种状态，状态1表示激活，状态0表示抑制。\n",
    "\n",
    "令向量$s \\in {0,1}^n$表示 n 个神经元的状态，$w_{ij}$表示神经元 i 与 j 之间的连接权，$\\theta_i$表示神经元 i 的阈值，则状态向量 s 所对应的 Boltzmann机能量定义为：\n",
    "\n",
    "$E(s) = - \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}s_is_j - \\sum_{i=1}^{n}\\theta_i s_i$  ——式（20）\n",
    "\n",
    "若网络中的神经元一任意不依赖于输入值的顺序进行更新，则网络最终将达到Boltzmann分布，此时状态向量 s 出现的概率将仅由其能量与所有可能状态向量的能量确定：\n",
    "\n",
    "$P(s) = \\frac{e^{-E(s)}}{\\sum_{t}e^{-E(t)}}$ ——式（21）\n",
    "\n",
    "Boltzmann机的训练过程就是将每个训练样本视为一个状态向量，使其出现的概率尽可能答。\n",
    "\n",
    "标准Boltzmann机是一个全连接图，训练网络的复杂度很高，这使其难以用于解决现实任务。\n",
    "\n",
    "现实中常用Boltzmann机（Restricted Boltzmann Machine,简称RBM）。\n",
    "\n",
    "如上图中，RBM机仅保留显层与隐层之间的连接，从而将Boltzmann机结构由完全图简为二部图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络（CNN）\n",
    "\n",
    "卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。\n",
    "卷积神经网络由三部分构成。第一部分是输入层。第二部分由n个卷积层和池化层的组合组成。第三部分由一个全连结的多层感知机分类器构成。\n",
    "\n",
    "![CNN示意图](images/neuralnetwork/cnn示意图.webp)\n",
    "\n",
    "·输入：224×224大小的图片，3通道\n",
    "·第一层卷积：11×11大小的卷积核96个，每个GPU上48个。\n",
    "·第一层max-pooling：2×2的核。\n",
    "·第二层卷积：5×5卷积核256个，每个GPU上128个。\n",
    "·第二层max-pooling：2×2的核。\n",
    "·第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。\n",
    "·第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。\n",
    "·第五层卷积：3×3的卷积核256个，两个GPU上个128个。\n",
    "·第五层max-pooling：2×2的核。\n",
    "·第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。\n",
    "·第二层全连接：4096维\n",
    "·Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。\n",
    "\n",
    "卷积神经网络在模式识别领域有着重要应用，当然这里只是对卷积神经网络做了最简单的讲解，卷积神经网络中仍然有很多知识，比如局部感受野，权值共享，多卷积核等内容，后续有机会再进行讲解。\n",
    "\n",
    "\n",
    "### 循环神经网络（递归神经网络） RNN\n",
    "\n",
    "传统的神经网络对于很多问题难以处理，比如你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。\n",
    "\n",
    "RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNN能够对任何长度的序列数据进行处理。\n",
    "\n",
    "这是一个简单的RNN的结构，可以看到隐藏层自己是可以跟自己进行连接的。\n",
    "\n",
    "![RNN示意图](images/neuralnetwork/RNN示意图.webp)\n",
    "\n",
    "那么RNN为什么隐藏层能够看到上一刻的隐藏层的输出呢，其实我们把这个网络展开来开就很清晰了。\n",
    "\n",
    "![RNN展开示意图](images/neuralnetwork/RNN展开示意图.webp)\n",
    "\n",
    "### 生成对抗网络（ GAN）\n",
    "\n",
    "生成对抗网络的目标在于生成，我们传统的网络结构往往都是判别模型，即判断一个样本的真实性。而生成模型能够根据所提供的样本生成类似的新样本，注意这些样本是由计算机学习而来的。\n",
    "GAN一般由两个网络组成，生成模型网络，判别模型网络。\n",
    "生成模型 G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好；判别模型 D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。\n",
    "举个例子：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。\n",
    "传统的判别网络：\n",
    "\n",
    "![传统判别网络](images/neuralnetwork/传统判别网络.webp)\n",
    "\n",
    "生成对抗网络：\n",
    "\n",
    "![GAN示意图](images/neuralnetwork/GAN示意图.webp)\n",
    "\n",
    "在训练的过程中固定一方，更新另一方的网络权重，交替迭代，在这个过程中，双方都极力优化自己的网络，从而形成竞争对抗，直到双方达到一个动态的平衡（纳什均衡），此时生成模型 G 恢复了训练数据的分布（造出了和真实数据一模一样的样本），判别模型再也判别不出来结果，准确率为 50%。\n",
    "\n",
    "生成网络\n",
    "\n",
    "![GAN生成网络](images/neuralnetwork/GAN生成网络.webp)\n",
    "\n",
    "判别网络\n",
    "\n",
    "![GAN判别网络](images/neuralnetwork/GAN判别网络.webp)\n",
    "\n",
    "最终结果，使用MNIST作为初始样本，通过学习后生成的数字，可以看到学习的效果还是不错的。\n",
    "\n",
    "![GAN学习的手写数字结果](images/neuralnetwork/GAN学习的手写数字结果.webp)\n",
    "\n",
    "### 深度信念网络 DBN\n",
    "\n",
    "在讲DBN之前，我们需要对DBN的基本组成单位有一定的了解，那就是RBM，受限玻尔兹曼机。\n",
    "\n",
    "![RBM示意图](images/neuralnetwork/RBM示意图.png)\n",
    "\n",
    "如图所示为一个玻尔兹曼机，其粉色节点为隐层，蓝色节点为输入层。\n",
    "\n",
    "玻尔兹曼机和递归神经网络相比，区别体现在以下几点：\n",
    "\n",
    "- 递归神经网络本质是学习一个函数，因此有输入和输出层的概念；\n",
    "- 玻尔兹曼机的用处在于学习一组数据的“内在表示”，因此其没有输出层的概念。\n",
    "\n",
    "\n",
    "- 递归神经网络各节点链接为有向环；\n",
    "- 玻尔兹曼机各节点连接成无向完全图。\n",
    "\n",
    "受限玻尔兹曼机，最简单的来说就是加入了限制，这个限制就是将完全图变成了二分图。即由一个显层和一个隐层构成，显层与隐层的神经元之间为双向全连接。\n",
    "\n",
    "![RBM抽象示意图](images/neuralnetwork/RBM抽象示意图.webp)\n",
    "\n",
    "h表示隐藏层，v表示显层\n",
    "在RBM中，任意两个相连的神经元之间有一个权值w表示其连接强度，每个神经元自身有一个偏置系数b（对显层神经元）和c（对隐层神经元）来表示其自身权重。\n",
    "具体的公式推导在这里就不展示了\n",
    "\n",
    "DBN是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对P(Observation|Label)和 P(Label|Observation)都做了评估，而判别模型仅仅而已评估了后者，也就是P(Label|Observation)。\n",
    "DBN由多个限制玻尔兹曼机（Restricted Boltzmann Machines）层组成，一个典型的神经网络类型如图所示。这些网络被“限制”为一个可视层和一个隐层，层间存在连接，但层内的单元间不存在连接。隐层单元被训练去捕捉在可视层表现出来的高阶数据的相关性。\n",
    "\n",
    "![DBN示意图](images/neuralnetwork/DBN示意图.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

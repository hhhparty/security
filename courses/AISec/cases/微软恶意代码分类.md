# 微软恶意代码分类

微软在2015年推出了一项竞赛，旨在发现对恶意代码进行更好的分类方法。

微软将每个恶意代码文件的 hex dump 和 IDA 反汇编内容作为数据集的主要部分，要求选手根据12个属性值判断一个恶意代码的类型（9种常见类型中的一种）。

## 特征提取 （Features extraction）

这个比赛给出的数据，与kaggle比赛的大多情况不同，它给出的数据是恶意代码的二进制元数据（但没有PE头），需要自己从中提取特征。

从原始数据中提取特征成为最大挑战。

最简单的提取方法：
- 字节数
- 字节 bi-gram
- 汇编指令数
- section 数
- 文件长度
- 文件大小

令人惊奇的是，仅仅使用 '00'频数、 'FF'频数 '??'频数 这三个特征，就能够使分类准确率达到0.96.

除此之外，没有办法再提高评分了。所以有研究人员开始构造新特性：
- 利用可执行代码得逻辑，构造指令码的 n-grams 特征。研究人员提取了4-grams、甚至10-grams。
- 导入表函数名
- 导入库名
- 进程数
- 通过计算.asm文件中的“loc_*”引用数，得到可执行文件的“conditionality”
- 由于恶意代码通常是加密的，所以引入了一些加密相关的特性
- 按照某个半字节序列滑动窗口，计算熵值，提取了一些分布统计值：20分位数、20百分比、均值、中位数、标准、最大值、最大值、最大值-最小值等，以及一阶差分分布、部分的分熵序列统计。
- 压缩率（作为柯式复杂度的近似值，(as an approximation ofKolmogorov complexity）
- 可打印字符串（最后没有使用）
- 每个文件的可打印字符串的分布特征

## 特征选择和降维

所有我们提取的特征可以大致分为两类：
- sparse，例如导入表函数名频数
- dense，例如分布统计值

稀疏特征的总体维数远高于稠密特征的维数，因此在随机森林中直接使用稀疏特征时更容易考虑稀疏特征。这会降低性能，因为稀疏特性中有更多的非信息特性。

我们使用了两种方法进行降维：
- 转换 transformation
- 选择 selection

第一项技术是transformation：
我们观察到大量的、超过100维的特征可以简单的降维到10维，且保留绝大部分信息。令人惊讶的是，在我们的特征中，Non-Negative Matrix Factorisation(NMF)比PCA方法或ICA方法更好用，可以解释为这是由于属性的特性——统计值都是非负的。我们使用NMF对原始数据降维表现与该数据经过log(1+x)转换后的结果一样好。

我们使用的另一项技术是选择属性：
- 我们使用L1规则化的SVM模型粗略地分离出一些无关特征，删去了少量特征。
- 然后我们使用随机森林特征重要性进一步选择。

使用上述pipeline，我们从$(256+1)^4$种可能中选出 131 个 4-grams ，得到0.01473交叉验证评分。下面是几个最重要的 4-grams：
```
??????????
0400000000
5dc3000000
f0f0f0f001
00100000
00f0f000
0d2f0600
5dc38bff
8bff558b
840d2f06

```

最终，我们甚至对 10-grams 进行特征选择。使用上面的方法，尝试选出10-grams，它将有助于有最大损失下精确分类对象。我们对训练集进行了折叠预测，并根据真实分类概率存储对象，标记为‘1’的前100个坏，而运行随机森林对 500个预选出的10-grams。我们选出了最重要的特征，证明在这个二进制分类任务中，使用14个 10-grams可以达到 1.0的精确率。

### 小结

我们最终使用的特征集：
#### feature set 1c
- .asm文件中section 的行数 ，例如.bss .data .rsrc等等，以及行数分布的熵 
- 30种最频繁的asm指令的出现次数
- .asm文件和.bytes文件大小和它们的比值
- sys 调用（grep查找__stdcall ） + TF-IDF+NMF（n_components=10）
- 函数调用（grep查找FUNCTION)  + TF-IDF+NMF（n_components=10）
- 131 个 选中的 4-grams
- 14 个选择的 10-grams

#### feature set 2
- 单字节计数，使用NMF转换，且NMF(log(x+1))
- 字节对计数，使用NMF转换，且NMF(log(x+1))
- 98个汇编指令计数，一些寄存器计数
- 各种bags：dll names, imported functions, extern functions,std calls
- keywords 计数和注册类字符串（例如'proc','cookie','loadlibrary','installdir','HKEY_LOCAL_MACHINE','HKEY_CURRENT_USER','spanalysis failed','__security_cookie'等）
- 每个节的行数
- 反汇编子程序的数量
- 熵分布的统计值
- 字符串长度的统计值
## 学习

我们使用了两种模型，是为了结合他们的后向有效性。结合两个模型比仅使用一个要好。在比赛中，我们使用10折交叉验证评估我们的性能。

### modles
主要使用的模型是 GBDT 分类器的 xgboost实现（严格说，xgboost并没使用损失函数的梯度下降法求极值，而是用牛顿法求极值）。Extremal bagging was very helpful。没有使用一个训练集，取而代之的是我们使用了8倍对象的训练集。这个8倍训练集的构成方法是：我们取所有的 $L$ 训练对象，然后采样 $\alpha L$ 个更多的对象进行替换。Alpha 可以通过 grid search找到，被设为7。

### 半监督技巧

因为分类精确性非常高（大于0.998），我们可以从测试集中拿更多的数据用于训练。测试集可切分为 K 折。我们使用训练集和 k-1 折测试集同时训练，然后用剩下的1折进行测试。我们基于在公开榜上我们最好预测结果来标记测试集中对象的标记。当使用bagging时，我们就不再采样测试集对象了。

### per-class 权重
当比较两个模型时，发现他们对不同的类犯了不同的错误，这就是为什么对每个分类要分别的结合两种方法进行预测了。对每个类，我们建立一个模型的线性组合进行预测。我们也在基本预测中增加第2序和第3序交互。

### 预测剪枝
我们相当大胆的的将对1，3，和 第7 分类的预测减到0和1。幸运地是我们将私分从0.0041到0.0039，尽管我们指导LogLoss 根本不能消除误差。

## 病毒家族

### ramnit
Ramnit于2010年开始活动。它主要通过感染可执行文件并附加到HTML文件进行传播。一年后，发布了一个更危险的版本-作者使用了Zeus恶意软件代码，该代码于2011年泄漏。使用Zeus代码元素可以为Ramnit添加新功能，最终使他成为了成熟的银行木马。


当前，Ramnit具有更多功能：
- 进行浏览器人攻击
从浏览器中窃取FTP密码和cookie
使用DGA（域生成算法）查找C＆C（命令和控制）服务器
在防病毒程序中手动添加例外
使用CVE-2013-3660和CVE-2014-4113漏洞的特权升级（特权升级）
截屏
尽管Europol在2015年关闭了300台C＆C服务器，但Ramnit的表现仍然不错。最近使用带有无缝门的RIG EK漏洞利用工具包进行了分发。

行动
主要的可执行文件以某种形式的Matryoshka包装：第一层（顶层）是原始包装，第二层是普通的UPX。

https://www.cert.pl/news/single/ramnit-doglebna-analiza/
https://www.virusbulletin.com/virusbulletin/2012/11/ramnit-bot
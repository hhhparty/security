# N-Gram 模型

## What is N-Gram model？
N-Gram 是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照单词（或字符、字节）进行大小为N的滑动窗口操作，形成了长度为N的字节片段序列。

N-gram 概念要点：
- 将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度为N的字节片段序列；
- 每个字节片段称为 gram；
- 对所有 gram 的出现频度进行统计；
- 按设定阈值对统计结果进行过滤，形成关键 gram 列表，该表即为文本的向量特征空间。
- 列表中每一种 gram 就是一个特征向量维度。

基本假设：
- 第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关。
- 整句的概率就是各个词出现概率的乘积。
- 概率可以通过直接从语料库中统计N个词同时出现的次数。

最常用的是二元Bi-Gram 和 三元 Tri-Gram。


如果我们有一个由 m 个词组成的序列（或说是一个句子），我们希望算得概率 $p(w_1,w_2,...,w_m)$，根据链式规则，可得：

$p(w_1,w_2,...,w_m) = p(w_1)*p(w_2|w_1)*p(w_3|w_1,w_2)...p(w_n|w_1,w_2,...,w_{n-1})$

这个概率显然并不好算，可以利用马尔科夫链的假设，**即当前这个词仅仅跟前面几个有限的词相关，因此不必追溯到最开始的那个词，这样就可以大幅缩减上述算式**。

$p(w_1,w_2,...,w_m) = p(w_i|w_{i-n+1},...,w_{i-1})$

下面给出一元、二元、三元模型的定义：

- 当 n=1, 一个一元模型（unigram model)即为 ：

$P(w_1,w_2,...,w_m) = \prod_{i=1}^m{P(w_i)}$

- 当 n=2, 一个二元模型（bigram model)即为 ：

$P(w_1,w_2,...,w_m) = \prod_{i=1}^m{P(w_i|w_{i-1})} $

- 当 n=3, 一个三元模型（trigram model)即为 ：

$P(w_1,w_2,...,w_m) = \prod_{i=1}^m{P(w_i|w_{i-1}，w_{i-2})}$

利用贝叶斯定理，条件概率可以计算出来（通常，后面的概率值是词频统计值的比）。
$\prod_{i=1}^m{P(w_i|w_{i-1})} = \frac{P(w_{i-1}w_i)}{P(w_{i-1})} = \frac{Count(w_{i-1},w_i)}{Count(w_{i-1})}$




## 应用
N-gram模型的一个常见应用
搜索引擎（Google或者Baidu）、或者输入法的猜想或者提示。你在用谷歌时，输入一个或几个词，搜索框通常会以下拉菜单的形式给出几个像下图一样的备选，这些备选其实是在猜想你想要搜索的那个词串。

再者，当你用输入法输入一个汉字的时候，输入法通常可以联系出一个完整的词，例如我输入一个“刘”字，通常输入法会提示我是否要输入的是“刘备”。通过上面的介绍，你应该能够很敏锐的发觉，这其实是以N-Gram模型为基础来实现的。

那么原理是什么呢？也就是我打入“我们”的时候，后面的“不一样”，”的爱“这些是怎么出来的，怎么排序的？

实际上是根据语言模型得出。假如使用的是二元语言模型预测下一个单词：

排序的过程就是：

p(”不一样“|"我们")>p(”的爱“|"我们")>p(”相爱吧“|"我们")>.......>p("这一家"|”我们“)，这些概率值的求法和上面提到的完全一样，数据的来源可以是用户搜索的log。


## n-gram的n大小对性能的影响
### n更大的时候
- 对下一个词出现的约束性信息更多
- 更大的辨别力
- 但是更稀疏
- 并且n-gram的总数也更多，为$V^n$个（V为词汇表大小或字节的大小，即1 gram 表的大小）

### n更小的时候
- 在训练语料库中出现的次数更多
- 更可靠的统计结果
- 更高的可靠性 
- 但是约束信息更少
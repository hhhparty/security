{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树（Decision Tree）\n",
    "\n",
    "决策树是一种常见的机器学习方法。常用于分类任务。\n",
    "\n",
    "决策树，顾名思义，是基于树结构来进行决策的，这是人类在面临决策问题时的一种很自然的处理机制。\n",
    "\n",
    "例如下图所示：\n",
    "\n",
    "- 决策是否见约会对象\n",
    "\n",
    "![决策树示例1](images\\dt\\dating.png)\n",
    "\n",
    "- 决策是否外出\n",
    "\n",
    "![决策树示例2](images\\dt\\outdoor.png)\n",
    "\n",
    "## 基本流程\n",
    "\n",
    "决策树的主要优势在于决策过程很容易理解。利用决策树形成的判断过程，同富有经验的领域专家几乎实现相同的。\n",
    "\n",
    "下面我们分析一下决策树的基本处理流程。\n",
    "\n",
    "以二分类任务为例，我们希望从给定训练数据集学得一个模型，用于对新示例进行分类，这个对样本进行分类的任务，可以看作对“当前样本属于正类么？”这个问题的“决策”或“判定”过程。\n",
    "\n",
    "![决策树示例3](images\\dt\\西瓜问题决策树.png)\n",
    "\n",
    "- 决策过程的最终结论（树的叶子），对应了我们希望的判定结果；\n",
    "\n",
    "- 决策过程中提出的每个判定问题，都是对某个属性的“测试”；\n",
    "\n",
    "- 每个测试的结果，要么导出最终结论，要么导出进一步的判定问题，其考虑范围是在上一次决策结果的限定范围之内；\n",
    "\n",
    "一般地，一颗决策树，包含一个根节点，若干个内部结点，和若干个叶节点。\n",
    "\n",
    "- 叶节点，对应决策结果；\n",
    "\n",
    "- 其它节点，对应一个属性测试。\n",
    "\n",
    "- 每个节点，包含的样本集合，根据属性测试的结果被划分到子节点中；\n",
    "\n",
    "- 根节点，包含样本全集；\n",
    "\n",
    "- 从根节点到每个叶节点的路径，对应了一个判定测试序列。\n",
    "\n",
    "**决策树学习的目的，是为了产生一棵泛化能力强的决策树，基本流程遵循简单而且直观的“分而治之（divide-and-conquer）”策略。**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的构造\n",
    "\n",
    "如何根据训练集数据，建立决策树呢？\n",
    "\n",
    "结合上面的基本过程，我们知道，决策树的每个节点对应了一个属性测试，在数据集中往往有一些属性是“好属性”，使用它们做测试，能够有“正确”的分类结果。\n",
    "\n",
    "\n",
    "### 构造决策树的伪代码\n",
    "\n",
    "输入1：训练集$D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$   \n",
    "输入2：属性集$A = {a_1,a_2,...,a_d}$  \n",
    "过程：函数 TreeGenerate(D,A)\n",
    "\n",
    "\n",
    "生成结点；  \n",
    "\n",
    "if D 中样本全属于同一类别C :  \n",
    "&ensp;&ensp;&ensp;&ensp;将node标记为C类叶结点；  \n",
    "\n",
    "if A = 空集 or D 中样本在 A 上取值相同 :    \n",
    "&ensp;&ensp;&ensp;&ensp; 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；  \n",
    "&ensp;&ensp;&ensp;&ensp; return；\n",
    "    \n",
    "从 A 中选择最优划分属性 $a_*$;  \n",
    "for $a_*$ 的每一个值 $a_*^v$ :    \n",
    "&ensp;&ensp;&ensp;&ensp;为node生成一个分支；  \n",
    "&ensp;&ensp;&ensp;&ensp;令$D_v$表示D中在$a_*$上取值为$a_*^v$的样本子集；  \n",
    "&ensp;&ensp;&ensp;&ensp;if $D_v$ 为空：  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将分支结点标记为叶节点，其类别标记为D中样本最多的类；  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;return    \n",
    "&ensp;&ensp;&ensp;&ensp;else：  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;以$TreeGenerate(D_v,A \\ {a_*})$ 为分支结点；  \n",
    "   \n",
    "输出： 以node为根节点的一棵决策树。\n",
    "\n",
    "此时，我们的问题聚焦到了“数据集上哪些特征在划分数据分类时，起决定性作用？”，换句话说，**“如何选择最优划分属性？”**\n",
    "\n",
    "### 划分选择\n",
    "\n",
    "为了找到最好的决策属性，我们需要评估每个属性。那么以何种标准来评估？\n",
    "\n",
    "一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本，尽可能属于同一类别，即结点的“纯度”（purity）越来越高。\n",
    "\n",
    "如果可以度量，那么我们可以根据纯度来评估每个属性，找到最好的决策属性。\n",
    "\n",
    "如何度量样本集合的纯度呢？\n",
    "\n",
    "### 信息熵\n",
    "\n",
    "“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。\n",
    "\n",
    "假定当前样本集合D中第k类样本所占的比例为$p_k(k = 1,2,...,|y|)$，则$D$的信息熵定义为：\n",
    "\n",
    "$Ent(D) = - \\sum_{k=1}^{|y|}p_k \\log_{2}p_k$    ——式（1）\n",
    "\n",
    "Ent(D)的值越小，则D的纯度越高。熵在信息论中代表随机变量不确定度的度量，熵值越大，不确定性越高。\n",
    "\n",
    "> 对熵的直观解释，可以参考https://blog.csdn.net/qq_39521554/article/details/79078917\n",
    "\n",
    "假定离散属性a有V个可能的取值${a^1,a^2,...,a^v}$，若使用a来对样本集D进行划分，则会产生V个分支结点。\n",
    "\n",
    "其中，第$v$个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。\n",
    "\n",
    "根据式（1），计算出$D^v$的信息熵。\n",
    "\n",
    "即当前集合$D^v$，设第k类样本所占的比例为$p_k^v(k = 1,2,...,|y^v|)$\n",
    "\n",
    "$Ent(D^v) = - \\sum_{k=1}^{|y^v|}p_k^v \\log_{2}p_k^v$ \n",
    "\n",
    "\n",
    "下面给出计算$D^v$的香农信息熵的python程序代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据集的香农信息熵：1.584702\n"
     ]
    }
   ],
   "source": [
    "\"\"\"计算香农信息熵的python程序代码\n",
    "\"\"\"\n",
    "from math import log\n",
    "from sklearn import neighbors,datasets\n",
    "import pandas as pd\n",
    "\n",
    "def calcShannonEnt(dataFrame):\n",
    "    \"\"\"\n",
    "    功能：根据分类标记，计算某数据集的信息熵。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列\n",
    "    输出：shannonEnt，数据集按当前标记分类结果的信息熵值\n",
    "    \n",
    "    \"\"\"  \n",
    "    numEntries = dataFrame.shape[0] #s数据集示例数\n",
    "    \n",
    "    labelCounts = {} #定义字典，键为分类标记名，值为标记的计数值\n",
    "    labelCounts.update(dataFrame.iloc[:,-1].value_counts())#为字典赋值,认为数据集最后一列为label\n",
    "    \n",
    "    shannonEnt = 0.0  # 设置香农信息熵初值为0.0\n",
    "    for key in labelCounts:\n",
    "        # 按公式求信息熵值\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        \n",
    "        shannonEnt -= prob * log(prob,2)    # 求以2为底的对数。\n",
    "    return shannonEnt\n",
    "\n",
    "def calcDatingDataSetEnt():\n",
    "    dataframe = pd.read_csv(\"data\\dating\\datingTestSet.txt\",header=None,sep='\\t',names=['年飞机里程','周冰淇淋升数','游戏耗时比','心仪程度'])\n",
    "    entropy = calcShannonEnt(dataframe)\n",
    "    print(\"当前数据集的香农信息熵：%f\"  % entropy)\n",
    "\n",
    "calcDatingDataSetEnt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中计算得到的信息熵差别不大，我们再看一个例子。\n",
    "\n",
    "这个例子是美国大选投票数据集，基本情况如下图所示：\n",
    "\n",
    "![选举数据示例](images/dt/voteexample.png)\n",
    "\n",
    "这个例子将计算一个投票数据集的熵值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息熵：0.996566\n"
     ]
    }
   ],
   "source": [
    "def calcAllVoteFeaturesEnt():\n",
    "    dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "    \n",
    "    entropy = calcShannonEnt(dataframe)\n",
    "    print(\"信息熵：%f\"  % entropy)\n",
    "\n",
    "calcAllVoteFeaturesEnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益\n",
    "\n",
    "假定离散属性a有V个可能的取值${a^1,a^2,...,a^v}$，若使用a对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。\n",
    "\n",
    "我们可以根据式（1）计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$\\frac{|D^v|}{|D|}$，即样本数越多的分支终点的影响越大，于是可计算出用属性a对样本集D进行划分所获得的“信息增益”（information gain)。\n",
    "\n",
    "$Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)$  ——式（2）\n",
    "\n",
    "一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。\n",
    "\n",
    "因此，我们可用信息增益来进行决策树的划分属性选择，即在选择“最好的”属性进行决策。\n",
    "\n",
    "$a_* = argmax_{a \\in A}Gain(D,a)$\n",
    "\n",
    "著名的ID3决策树学习算法【Quinlan，1986】就是以信息增益为准则来划分属性。\n",
    "\n",
    "下面程序的例子是以美国大选的投票数据样本集为例，选择最好划分属性的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('纹理', 0.3805918973682686)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcInforGain(df,aFeature):\n",
    "    \"\"\"\n",
    "    功能：按照上述公式计算用属性aFeature对样本集dataframe进行划分的信息增益。\n",
    "    输入：数据集dataFrame；\n",
    "          划分属性名aFeature；\n",
    "    输出：（最好划分属性名称，最大信息增益值）\n",
    "    \"\"\"\n",
    "    # 统计数据集的样本数量\n",
    "    totalsampleCount = df.shape[0]\n",
    "    # 统计属性a各值对应的样本数量\n",
    "    sampleCounts = {} \n",
    "    sampleCounts.update(df[aFeature].value_counts())\n",
    "    #print(sampleCounts)\n",
    "    infogain = calcShannonEnt(df)\n",
    "    for key,value in sampleCounts.items():\n",
    "        subdf = df[df[aFeature] == key]\n",
    "        infogain -= subdf.shape[0]/ totalsampleCount * calcShannonEnt(subdf)\n",
    "    return infogain\n",
    "\n",
    "def getBestDivideFeature(df):    \n",
    "    featureInfoGains = {}    \n",
    "    for colname in df.columns[:-1]:\n",
    "        # 对非标记属性，计算其信息增益，标记属性为dataframe中最后一列\n",
    "        infogain = calcInforGain(df,colname)\n",
    "        featureInfoGains[colname] = infogain\n",
    "    # 对已计算增益的结果进行排序\n",
    "    bestFeature = sorted(featureInfoGains.items(),key =lambda item:item[1],reverse=True)[0]\n",
    "    #print(featureInfoGains)\n",
    "    return bestFeature\n",
    "\n",
    "#dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')        \n",
    "#getBestDivideFeature(dataframe)    \n",
    "dataframe = pd.read_csv(\"data/maloon/maloon2.txt\",header=0,sep=',')        \n",
    "getBestDivideFeature(dataframe.iloc[:,1:])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据计算信息增益，发现在投票数据集中，“医师费用冻结”属性对数据集进行划分的信息增益最大，于是我们将选择它作为划分属性。\n",
    "\n",
    "划分结果如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按\"医师费用冻结\"进行划分:\n",
      "    n类的节点数为119\n",
      "    y类的节点数为113\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "dict = {}\n",
    "dict.update(dataframe[\"医师费用冻结\"].value_counts())\n",
    "subDList = {}\n",
    "for colValue in dict.keys():\n",
    "    subDList[colValue] = dataframe[dataframe[\"医师费用冻结\"]==colValue]\n",
    "\n",
    "print(\"按\\\"医师费用冻结\\\"进行划分:\")\n",
    "for i in subDList.items():\n",
    "    print(\"    {}类的节点数为{}\".format(i[0],i[1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，决策树学习算法将对每个分支结点做进一步划分。\n",
    "\n",
    "如上例中，通过“医师费用冻结”进行划分后，需要再次计算“最好划分属性”，对各分支结点所含数据集进行划分。\n",
    "\n",
    "### 构建决策树的程序\n",
    "\n",
    "选择最佳属性的过程一般需要多次，对每个分支结点进行上述操作，直至将所有属性都使用完毕。这样一棵决策树就建立起来了。\n",
    "\n",
    "但是，大多数用户不需要这样的决策树，而是需要那种只通过3~5个属性值就能够进行分类的决策树。\n",
    "\n",
    "这时需要由用户事先指定“树的深度”这个超参来构建决策树。\n",
    "\n",
    "下面，根据上文中构造决策树的伪代码，我们编写决策树构建函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Node:\n",
    "    \"\"\"决策树结点类\"\"\"\n",
    "    def __init__(self):\n",
    "        self._type = None\n",
    "        self._label = None\n",
    "        self._samples = pd.DataFrame()\n",
    "        self._children = {}        \n",
    "    \n",
    "    @property    \n",
    "    def type(self):\n",
    "        return self._type\n",
    "    \n",
    "    @type.setter\n",
    "    def setType(self,type):\n",
    "        self._type = type\n",
    "    \n",
    "    @property    \n",
    "    def label(self):\n",
    "        return self._label\n",
    "    \n",
    "    @label.setter\n",
    "    def setLabel(self,label):\n",
    "        self._label = label\n",
    "        \n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples\n",
    "    \n",
    "    @samples.setter\n",
    "    def setSamples(self,samples):\n",
    "        if isinstance(samples, pd.DataFrame):\n",
    "            self._samples = samples\n",
    "    \n",
    "    @property    \n",
    "    def children(self):\n",
    "        return self._children\n",
    "    \n",
    "\n",
    "    def addChildren(self,key,children):\n",
    "        self._children[key] = children\n",
    "\n",
    "    def getChildrenTypeList(self):\n",
    "        cl = []\n",
    "        for k,v in self._children.items():\n",
    "            cl.append(v.type + '['+k+']')\n",
    "        return cl\n",
    "    \n",
    "    def __str__(self):        \n",
    "        return \"Type:{},Label:{},Samples:{},Children:{} \".format(\n",
    "                self._type,self._label,\n",
    "                self._samples.index.tolist(),\n",
    "                self.getChildrenTypeList())\n",
    "def bfs(rootNode,depth = 0):\n",
    "    print(\"{}{}\".format('\\t'*depth,rootNode))\n",
    "    #print(\"Type:{},Label:{},Samples:{}\".format(rootNode.type,rootNode.label,rootNode.samples.index.tolist()))\n",
    "    for k,v in rootNode.children.items():\n",
    "        bfs(v,depth + 1)\n",
    "    return depth\n",
    "        \n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码用于测试上述类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:root,Label:None,Samples:[],Children:['middle[a=1]', 'middle[a=2]'] \n",
      "Type:root,Label:None,Samples:[],Children:['middle[a=1]', 'middle[a=2]'] \n",
      "\tType:middle,Label:None,Samples:[],Children:['leaf[b=1]', 'leaf[b=2]'] \n",
      "\t\tType:leaf,Label:None,Samples:[],Children:[] \n",
      "\t\tType:leaf,Label:None,Samples:[],Children:[] \n",
      "\tType:middle,Label:None,Samples:[],Children:[] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = Node()\n",
    "node.setType = 'root' \n",
    "child = Node()\n",
    "node.addChildren('a=1',child)\n",
    "child.setType = 'middle'\n",
    "child2 = Node()\n",
    "node.addChildren('a=2',child2)\n",
    "child2.setType = 'middle'\n",
    "ss = Node()\n",
    "ss.setType = 'leaf'\n",
    "ss1 = Node()\n",
    "ss1.setType = 'leaf'\n",
    "child.addChildren('b=1',ss)\n",
    "child.addChildren('b=2',ss1 )\n",
    "print(node)\n",
    "bfs(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def createDT(dataFrame,depth):\n",
    "    \"\"\"\n",
    "    功能：根据有标记数据集dataFrame，构建深度为depth的决策树\n",
    "    输入：训练集 𝐷 = dataFrame ，属性名为dataframe的第0行\n",
    "          树的深度为depth，默认值为3\n",
    "    输出：一棵以嵌套字典表示的决策树  \n",
    "\n",
    "    \"\"\"\n",
    "    # 生成结点\n",
    "    node = Node()\n",
    "         \n",
    "    if len(dataFrame.iloc[:,-1].value_counts()) == 1:\n",
    "        #若D 中样本全属于同一类别，则将node标记为C类叶结点\n",
    "        node.setType = 'Leaf'\n",
    "        node.setLabel = dataFrame.iloc[0,-1]\n",
    "        node.setSamples = dataFrame    \n",
    "        return node\n",
    "\n",
    "    if len(dataFrame.iloc[:,:-1]) == 0 :\n",
    "        # A = 空集 :\n",
    "        # 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；\n",
    "        # 注意，因为可能存在未知因素，会存在A上取值相同，但分类不同的示例。        \n",
    "        tempdict = {}\n",
    "        tempdict.update(dataFrame.iloc[:,-1].value_counts())  \n",
    "        if tempdict:\n",
    "            label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]  \n",
    "        else:\n",
    "            label = ''\n",
    "        node.setType = 'Leaf' \n",
    "        node.setLabel = label\n",
    "        node.setSamples = dataFrame\n",
    "        return node\n",
    "    \n",
    "    valueIsUnique = False\n",
    "   \n",
    "    for feature in dataFrame.columns[:-1]:    \n",
    "        if len(dataFrame[feature].value_counts()) > 1:\n",
    "            # 属性集任一个属性的取值不唯一，就跳出\n",
    "            break\n",
    "        valueIsUnique = True\n",
    "    if valueIsUnique :\n",
    "        # D 中样本在 A 上取值相同 :\n",
    "        # 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；\n",
    "        # 注意，因为可能存在未知因素，会存在A上取值相同，但分类不同的示例。        \n",
    "        tempdict = {}\n",
    "        tempdict.update(dataFrame.iloc[:,-1].value_counts())        \n",
    "        label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]        \n",
    "        node.setType = 'Leaf' \n",
    "        node.setLabel = label\n",
    "        node.setSamples = dataFrame      \n",
    "        return node\n",
    "  \n",
    "    #从 A 中选择最优划分属性  𝑎∗ \n",
    "    \n",
    "    aFeature = getBestDivideFeature(dataFrame)[0]    \n",
    "    #print(\"  使用{}作为划分依据\".format(aFeature))\n",
    "    node.setType = \"据\\\"{}\\\"划分\".format(aFeature)\n",
    "    node.setLabel = '未定'\n",
    "    node.setSamples = dataFrame\n",
    "    \n",
    "    dict = {}\n",
    "    dict.update(dataFrame[aFeature].value_counts())\n",
    "    #print(\"dict内容：{}\".format(dict))\n",
    "    for colValue in dict.keys():\n",
    "        keyname = aFeature+'='+colValue\n",
    "        #对𝑎∗ 的每一个值  𝑎𝑣∗ ，为node生成一个分支 \n",
    "        aBranchNode = Node()                      \n",
    "        #令 𝐷𝑣 表示D中在 𝑎∗ 上取值为 𝑎𝑣∗ 的样本子集；\n",
    "        dv = dataFrame[dataFrame[aFeature]==colValue]         \n",
    "        if  dv.empty:\n",
    "            # 若dv为空,将分支结点标记为叶节点，其类别标记为D中样本最多的类；\n",
    "            tempdict = {}\n",
    "            tempdict.update(dataFrame.iloc[:,-1].value_counts())        \n",
    "            label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]              \n",
    "            aBranchNode.setType = 'Leaf' \n",
    "            aBranchNode.setLabel = label\n",
    "            aBranchNode.setSamples = dv\n",
    "            # 将aBranchNode列为node的子节点           \n",
    "            node.addChildren(keyname,aBranchNode)              \n",
    "        else:    \n",
    "            # 去除数据集Dv 中aFeature列后的样本\n",
    "            subDv = dv.drop([aFeature],axis =1)            \n",
    "            if depth == 0:\n",
    "                #若当前树的深度已经达到要求，则将当前分支节点的其类别标记为D中样本最多的类\n",
    "                tempdict = {}\n",
    "                tempdict.update(subDv.iloc[:,-1].value_counts())        \n",
    "                label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]              \n",
    "                aBranchNode.setType = 'Leaf' \n",
    "                aBranchNode.setLabel = label    \n",
    "                aBranchNode.setSamples = subDv\n",
    "                # 将aBranchNode列为node的子节点                \n",
    "                node.addChildren(keyname,aBranchNode)     \n",
    "                continue\n",
    "            # 以 createDT(subDv)  为分支结点；\n",
    "            node.addChildren(keyname,createDT(subDv,depth-1))\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们以西瓜数据集进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6a07381c491d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#dataframe = pd.read_csv(\"data/vote/votesimple.txt\",header=0,sep=',')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data1/maloon/maloon2.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'决策树：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#dataframe = pd.read_csv(\"data/vote/votesimple.txt\",header=0,sep=',')    \n",
    "df = pd.read_csv(\"data1/maloon/maloon2.txt\",header=0,sep=',')\n",
    "d = df.iloc[:,1:]\n",
    "dtree = createDT(d,depth = 5)\n",
    "print('决策树：')\n",
    "bfs(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据“投票训练数据”，建立决策树 *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树：\n",
      "Type:据\"医师费用冻结\"划分,Label:未定,Samples:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231],Children:['据\"通过预算决议\"划分[医师费用冻结=n]', '据\"合成燃料公司削减\"划分[医师费用冻结=y]'] \n",
      "\tType:据\"通过预算决议\"划分,Label:未定,Samples:[1, 38, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 83, 85, 87, 88, 89, 90, 91, 93, 94, 115, 124, 145, 146, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 230],Children:['Leaf[通过预算决议=y]', '据\"学校中的宗教团体\"划分[通过预算决议=n]'] \n",
      "\t\tType:Leaf,Label:democrat,Samples:[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 73, 74, 76, 83, 85, 87, 88, 89, 90, 91, 93, 124, 146, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 213, 214, 217, 218, 219, 220, 221, 222, 223],Children:[] \n",
      "\t\tType:据\"学校中的宗教团体\"划分,Label:未定,Samples:[1, 38, 40, 41, 72, 75, 94, 115, 145, 165, 178, 189, 207, 216, 227, 230],Children:['Leaf[学校中的宗教团体=y]', '据\"免税出口\"划分[学校中的宗教团体=n]'] \n",
      "\t\t\tType:Leaf,Label:democrat,Samples:[1, 40, 41, 75, 94, 115, 145, 165, 207, 216, 227],Children:[] \n",
      "\t\t\tType:据\"免税出口\"划分,Label:未定,Samples:[38, 72, 178, 189, 230],Children:['Leaf[免税出口=y]', 'Leaf[免税出口=n]'] \n",
      "\t\t\t\tType:Leaf,Label:democrat,Samples:[38, 178, 189, 230],Children:[] \n",
      "\t\t\t\tType:Leaf,Label:republican,Samples:[72],Children:[] \n",
      "\tType:据\"合成燃料公司削减\"划分,Label:未定,Samples:[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 42, 68, 69, 70, 77, 78, 79, 80, 81, 82, 84, 86, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 147, 148, 149, 150, 210, 211, 212, 215, 224, 225, 226, 228, 229, 231],Children:['Leaf[合成燃料公司削减=n]', '据\"MX导弹\"划分[合成燃料公司削减=y]'] \n",
      "\t\tType:Leaf,Label:republican,Samples:[3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 39, 42, 69, 70, 78, 79, 81, 82, 84, 86, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 210, 211, 212, 215, 224, 225, 229, 231],Children:[] \n",
      "\t\tType:据\"MX导弹\"划分,Label:未定,Samples:[0, 2, 4, 20, 21, 22, 36, 37, 68, 77, 80, 92, 99, 121, 122, 123, 125, 147, 148, 149, 150, 226, 228],Children:['据\"南非出口管理法\"划分[MX导弹=n]', '据\"残疾婴儿\"划分[MX导弹=y]'] \n",
      "\t\t\tType:据\"南非出口管理法\"划分,Label:未定,Samples:[0, 2, 4, 20, 21, 22, 36, 37, 80, 92, 99, 121, 122, 123, 125, 147, 148, 149, 226, 228],Children:['据\"通过预算决议\"划分[南非出口管理法=y]', '据\"残疾婴儿\"划分[南非出口管理法=n]'] \n",
      "\t\t\t\tType:据\"通过预算决议\"划分,Label:未定,Samples:[80, 92, 99, 121, 122, 123, 125, 147, 148, 149, 226, 228],Children:['Leaf[通过预算决议=n]', '据\"水利工程费用分摊\"划分[通过预算决议=y]'] \n",
      "\t\t\t\t\tType:Leaf,Label:republican,Samples:[80, 92, 99, 121, 122, 125, 147, 148, 228],Children:[] \n",
      "\t\t\t\t\tType:据\"水利工程费用分摊\"划分,Label:未定,Samples:[123, 149, 226],Children:['Leaf[水利工程费用分摊=n]', 'Leaf[水利工程费用分摊=y]'] \n",
      "\t\t\t\t\t\tType:Leaf,Label:republican,Samples:[149, 226],Children:[] \n",
      "\t\t\t\t\t\tType:Leaf,Label:democrat,Samples:[123],Children:[] \n",
      "\t\t\t\tType:据\"残疾婴儿\"划分,Label:未定,Samples:[0, 2, 4, 20, 21, 22, 36, 37],Children:['据\"水利工程费用分摊\"划分[残疾婴儿=n]', 'Leaf[残疾婴儿=y]'] \n",
      "\t\t\t\t\tType:据\"水利工程费用分摊\"划分,Label:未定,Samples:[2, 4, 20, 22, 36, 37],Children:['据\"通过预算决议\"划分[水利工程费用分摊=y]', 'Leaf[水利工程费用分摊=n]'] \n",
      "\t\t\t\t\t\tType:据\"通过预算决议\"划分,Label:未定,Samples:[2, 4, 22, 36, 37],Children:['Leaf[通过预算决议=n]', 'Leaf[通过预算决议=y]'] \n",
      "\t\t\t\t\t\t\tType:Leaf,Label:republican,Samples:[4, 22, 36, 37],Children:[] \n",
      "\t\t\t\t\t\t\tType:Leaf,Label:democrat,Samples:[2],Children:[] \n",
      "\t\t\t\t\t\tType:Leaf,Label:democrat,Samples:[20],Children:[] \n",
      "\t\t\t\t\tType:Leaf,Label:republican,Samples:[0, 21],Children:[] \n",
      "\t\t\tType:据\"残疾婴儿\"划分,Label:未定,Samples:[68, 77, 150],Children:['Leaf[残疾婴儿=y]', 'Leaf[残疾婴儿=n]'] \n",
      "\t\t\t\tType:Leaf,Label:republican,Samples:[68, 77],Children:[] \n",
      "\t\t\t\tType:Leaf,Label:democrat,Samples:[150],Children:[] \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')  \n",
    "\n",
    "dt = createDT(df,depth = 10)\n",
    "print('决策树：')\n",
    "bfs(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增益率*\n",
    "\n",
    "上面的操作中，我们有意忽略了“编号”这一属性。如果把“编号”作为一个候选划分属性，计算它的信息增益会远大于其它候选划分属性。原因是“编号”产生的分支中，每个分支一个样本，纯度是最大的。然而，这样的决策树显然不具备泛化能力，无法对新样本进行有效预测。\n",
    "\n",
    "以“信息增益”作为划分属性的选择标准，是有偏好的，即**信息增益准则对可取值数据较多的属性有所偏好（形成的分支越多，信息增益越大）**。\n",
    "\n",
    "为了减少这种偏好带来的不利影响，著名的C4.5决策树算法[Quilan,1993]不直接使用信息增益，而是使用“增益率”（gain ratio）来选择最优划分属性。增益率定义为：\n",
    "\n",
    "$Gain_ratio(D,a) = \\frac{Gain(D,a}{IV(a)}$  ——式（3）\n",
    "\n",
    "其中：$IV(a) = - \\sum_{v=1}^{V} \\frac{|D^v|}{|D|}\\log_2 \\frac{|D_v|}{|D|}$   ——式（4）\n",
    "\n",
    "IV称为属性a的“固有值”（intrinsic value)。属性a的可能取值数目越多（即V越大），则IV(a)的值通常也会越大。\n",
    "\n",
    "下面的程序用于计算属性a的IV值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06843956584615818"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"计算昆兰增益率的程序代码\n",
    "\"\"\"\n",
    "from math import log\n",
    "from sklearn import neighbors,datasets\n",
    "import pandas as pd\n",
    "\n",
    "def calcShannonEnt(dataFrame):\n",
    "    \"\"\"\n",
    "    功能：根据分类标记，计算某数据集的信息熵。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列\n",
    "    输出：shannonEnt，数据集按当前标记分类结果的信息熵值\n",
    "    \n",
    "    \"\"\"  \n",
    "    numEntries = dataFrame.shape[0] #s数据集示例数\n",
    "    \n",
    "    labelCounts = {} #定义字典，键为分类标记名，值为标记的计数值\n",
    "    labelCounts.update(dataFrame.iloc[:,-1].value_counts())#为字典赋值,认为数据集最后一列为label\n",
    "    \n",
    "    shannonEnt = 0.0  # 设置香农信息熵初值为0.0\n",
    "    for key in labelCounts:\n",
    "        # 按公式求信息熵值\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        \n",
    "        shannonEnt -= prob * log(prob,2)    # 求以2为底的对数。\n",
    "    return shannonEnt\n",
    "\n",
    "def calcInforGain(df,aFeature):\n",
    "    \"\"\"\n",
    "    功能：按照上述公式计算用属性aFeature对样本集dataframe进行划分的信息增益。\n",
    "    输入：数据集dataFrame；\n",
    "          划分属性名aFeature；\n",
    "    输出：（最好划分属性名称，最大信息增益值）\n",
    "    \"\"\"\n",
    "    # 统计数据集的样本数量\n",
    "    totalsampleCount = df.shape[0]\n",
    "    # 统计属性a各值对应的样本数量\n",
    "    sampleCounts = {} \n",
    "    sampleCounts.update(df[aFeature].value_counts())\n",
    "    #print(sampleCounts)\n",
    "    infogain = calcShannonEnt(df)\n",
    "    for key,value in sampleCounts.items():\n",
    "        subdf = df[df[aFeature] == key]\n",
    "        infogain -= subdf.shape[0]/ totalsampleCount * calcShannonEnt(subdf)\n",
    "    return infogain\n",
    "\n",
    "def calcQuinLanIV(dataFrame,aFeature):\n",
    "    \"\"\"\n",
    "    功能：计算某数据集中某个列属性的Intrinsic Value。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列\n",
    "          aFeature，某个列属性\n",
    "    输出：intrinsicvalue，数据集对某个属性计算出的属性固有值（IV)\n",
    "    \n",
    "    \"\"\"  \n",
    "    numEntries = dataFrame.shape[0] #s数据集示例数\n",
    "    \n",
    "    labelCounts = {} #定义字典，键为分类标记名，值为标记的计数值\n",
    "    labelCounts.update(dataFrame[aFeature].value_counts())#为字典赋值,认为数据集最后一列为label\n",
    "    #print(labelCounts)\n",
    "    intrinsicvalue = 0.0\n",
    "    for key in labelCounts:\n",
    "        # 按公式求增益率iv\n",
    "        prior = float(labelCounts[key])/numEntries        \n",
    "        intrinsicvalue -= prior * log(prior,2)    # 求以2为底的对数。\n",
    "        \n",
    "    return intrinsicvalue\n",
    "   \n",
    "def calcGainRatio(dataFrame,aFeature):\n",
    "    \"\"\"\n",
    "    功能：计算某数据集中某个列属性的昆兰增益率Gain ratio。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列\n",
    "          aFeature，某个列属性\n",
    "    输出：gainratio，数据集dataFrame，对属性aFeature计算增益率\n",
    "    \n",
    "    \"\"\"  \n",
    "    gainratio = calcInforGain(dataFrame,aFeature) / calcQuinLanIV(dataFrame,aFeature)\n",
    "    return gainratio\n",
    "\n",
    "\n",
    "#dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')        \n",
    "#getBestDivideFeature(dataframe)    \n",
    "dataframe = pd.read_csv(\"data/maloon/maloon2.txt\",header=0,sep=',')        \n",
    "dataframe.columns[1]\n",
    "calcGainRatio(dataframe.iloc[:,1:],dataframe.columns[1])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是**增益率准则对可取值数目较少的属性有所偏好**，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：\n",
    "\n",
    "- 先从候选划分属性中找出信息增益高于平均水平的属性\n",
    "\n",
    "- 再从中选择增益率最高的。\n",
    "\n",
    "下面的代码，是使用增益率准则选择最佳划分属性的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('纹理', 0.2630853587192754)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBestDivideFeature(dataframe):    \n",
    "    featureGainratios = {}    \n",
    "    for colname in dataframe.columns[:-1]:\n",
    "        # 对非标记属性，计算其信息增益，标记属性为dataframe中最后一列\n",
    "        gainratio = calcGainRatio(dataframe,colname)\n",
    "        featureGainratios[colname] = gainratio\n",
    "    # 对已计算增益的结果进行排序\n",
    "    bestFeature = sorted(featureGainratios.items(),key =lambda item:item[1],reverse=True)[0]\n",
    "    #print(featureInfoGains)\n",
    "    return bestFeature\n",
    "dataframe = pd.read_csv(\"data/maloon/maloon2.txt\",header=0,sep=',')        \n",
    "dataframe.columns[1]\n",
    "getBestDivideFeature(dataframe.iloc[:,1:])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼指数（Gini index）*\n",
    "\n",
    "前面两种（信息增益、增益率）方法，分别偏好“分支数量多的”、“分支数量少的”属性，第三种判决依据是“基尼指数”（Gini index）。\n",
    "\n",
    "CART(Classification and Regression Tree)决策树[Breiman et al., 1984]使用基尼指数来选择划分属性。\n",
    "\n",
    "数据集D的纯度可用基尼值来度量：\n",
    "\n",
    "$ Gini(D) = \\sum_{k=1}^{|y|}{\\sum_{k^{'} \\neq k}p_k p_{k^{'}}} = 1 - \\sum_{k=1}^{|y|}p_k^2$  ————式（5）\n",
    "\n",
    "$p_k$ 指数据集 D 中第k类样本所占的比例。$p_{k^{'}}$ 指数据集 D 中第$k^{'}$类样本所占的比例。\n",
    "\n",
    "直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。\n",
    "\n",
    "因此，Gini(D)越小，则数据集D的纯度越高。\n",
    "\n",
    "于是，我们在侯选属性集合A中，选择那个使得划分后属性指数最小的属性作为最优划分属性，即：\n",
    "\n",
    "$a_* = argmin_{a \\in A} Gini_index(D,a) $\n",
    "\n",
    "下列程序，是使用基尼指数进行选择属性的程序:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-536b7a7daff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m#print(featureInfoGains)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbestFeature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/maloon/maloon2.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mgetBestDivideFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def calcGini(dataFrame):\n",
    "    \"\"\"\n",
    "    功能：计算某数据集中某个列属性的gini值。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列;\n",
    "          aFeature，某个列属性.\n",
    "    输出：ginivalue，数据集对某个属性计算出的基尼值（随机抽取两个样本，其类别标记不一致的概率）\n",
    "    \n",
    "    \"\"\"  \n",
    "    numEntries = dataFrame.shape[0] #s数据集示例数\n",
    "    \n",
    "    labelCounts = {} #定义字典，键为分类标记名，值为标记的计数值\n",
    "    labelCounts.update(dataFrame.iloc[:,-1].value_counts())#为字典赋值,认为数据集最后一列为label\n",
    "    \n",
    "    #print(labelCounts)\n",
    "    gini = 1.0\n",
    "    for key in labelCounts:\n",
    "        # 按公式求增益率iv\n",
    "        p = float(labelCounts[key])/numEntries        \n",
    "        gini -= p*p    \n",
    "        \n",
    "    return gini\n",
    "\n",
    "def calcGiniIndex(dataframe,aFeature):\n",
    "    \"\"\"\n",
    "    计算数据集dataframe中，属性aFeature的基尼指数值\n",
    "    \"\"\"\n",
    "    giniindex = 0.0\n",
    "    \n",
    "    sampleNum = dataframe.shape[0]\n",
    "    labelCounts = {}\n",
    "    labelCounts.update(dataframe[aFeature].value_counts())\n",
    "    for key in labelCounts:\n",
    "        prior = float(labelCounts[key])/sampleNum        \n",
    "        giniindex += prior *  calcGini(dataframe[dataframe[aFeature] == labelCounts[key]])   # 求以2为底的对数。    \n",
    "    return giniindex  \n",
    "\n",
    "def getBestDivideFeature(dataframe):    \n",
    "    featureGiniIndexes = {}    \n",
    "    for colname in dataframe.columns[:-1]:\n",
    "        # 对非标记属性，计算其信息增益，标记属性为dataframe中最后一列\n",
    "        gainratio = calcGiniIndex(dataframe,colname)\n",
    "        featureGiniIndexes[colname] = gainratio\n",
    "    # 对已计算增益的结果进行排序\n",
    "    bestFeature = sorted(featureGiniIndexes.items(),key =lambda item:item[1],reverse=True)[0]\n",
    "    #print(featureInfoGains)\n",
    "    return bestFeature\n",
    "dataframe = pd.read_csv(\"data/maloon/maloon2.txt\",header=0,sep=',')        \n",
    "dataframe.columns[1]\n",
    "getBestDivideFeature(dataframe.iloc[:,1:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树剪枝（pruning）*\n",
    "\n",
    "剪枝是决策树学习算法对付“过拟合”的主要手段，例如：限定树的深度。\n",
    "\n",
    "在决策树学习中，为了尽可能正确分类训练样本，结点划分过程讲不断重复，有时会造成决策树分支过多。这样就可能因训练样本学习得过好，使模型过拟合。所以需要主动剪枝掉一部分树的分支，降低过拟合风险。\n",
    "\n",
    "决策树剪枝的基本策略有“预剪枝”（prepruning）和“后剪枝”（post-pruning）。\n",
    "\n",
    "### 预剪枝\n",
    "\n",
    "预剪枝是指决策树生成过程中，对每个节点在划分前，先进性估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶节点。\n",
    "\n",
    "假设我们采用信息增益准则选择属性，我们选择属性“脐部”来对训练集进行划分，并产生了3个分支，如下图所示：\n",
    "\n",
    "![选择属性“脐部”来对训练集进行划分](images/dt/未减枝的决策树.png)\n",
    "\n",
    "但是，是否应该进行这次划分呢？预剪枝要对划分前后的泛化性能进行估计。\n",
    "\n",
    "在划分前，所有样例集中在根节点。若不进行划分，则根据“构造决策树的伪代码”要将节点标记为叶节点，其类别标记为训练样例数最多的类别。\n",
    "\n",
    "假设我们将这个叶节点标记为“好瓜”，用西瓜数据验证集(7个样本)对这个单节点决策树进行评估，有3个样例被分类正确，另外4个分类错误，于是验证集精度为$\\frac{3}{7}*100% = 42.9%$\n",
    "\n",
    "在用属性“脐部”划分后，验证集有5个样例被分类正确，验证精度$\\frac{5}{7}*100% = 71.4% > 42.9%$。所以用“脐部”划分得以确定。\n",
    "\n",
    "之后，依据决策前、决策后的精度计算，我们将否定使用“色泽”、“根蒂”进行划分，因为划分后精度低于或等于71.4%。最终产生的决策树是“决策树桩”（decision stump），如下图所示。\n",
    "\n",
    "![选择属性“脐部”预减枝的决策树](images/dt/预减枝的决策树.png)\n",
    "\n",
    "可以看出，预剪枝使决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的时间开销和预测时间开销。\n",
    "\n",
    "但是，有些划分虽然暂时不能提升泛化性能，甚至降低了泛化性能，但在其基础上的后续划分却有可能使泛化性能提供。\n",
    "\n",
    "预剪枝基于“贪心”本质，禁止这些分支咱开，给预剪枝决策树带来了欠拟合的风险。\n",
    "\n",
    "### 后剪枝\n",
    "\n",
    "先从训练集生成一个完整的决策树，然后自底向上地对非叶节点进行考查，若将该节点对应的子树替换为叶节点能够带来决策树泛化性能提升，则将该子树替换为叶节点。\n",
    "\n",
    "假设我们得到了上面图4.5所示的决策树，容易计算该决策树的验证集精度为42.9%。\n",
    "\n",
    "后剪枝首先考察图4.5种的结点6，或将其领衔的分支剪除，则相当于把6替换为叶节点。此时，决策树的验证精度提升至57.1%。\n",
    "\n",
    "按这一思路，最后形成的决策树如下图所示：\n",
    "\n",
    "![后减枝的决策树](images/dt/后减枝的决策树.png)\n",
    "\n",
    "对比4.7与4.6，可以看出，后剪枝决策树通常比预剪枝决策树保留了更多分支。\n",
    "\n",
    "一般情况下，后剪枝决策树的欠拟合风险很小，泛化性由于预剪枝决策树。\n",
    "\n",
    "但是，后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶节点进行逐一考察，因此其训练时间开销比未减枝决策树和预剪枝决策树都要大。\n",
    "\n",
    "### 泛化性能提升评估\n",
    "\n",
    "如何判断决策树泛化性能提升了呢？可以使用性能评估方法，例如：\n",
    "\n",
    "- 错误率、精度\n",
    "- 查全率、查准率\n",
    "- ROC、AUX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续与缺失值*\n",
    "\n",
    "### 连续值处理\n",
    "\n",
    "上面的讨论是基于离散属性来生成决策树，现实学习任务中常会遇到连续属性，有必要讨论如何在决策树学习中使用连续属性。\n",
    "\n",
    "**由于连续属性的可取值数目不再有限，因此不能直接根据连续属性的取值对结点进行划分。**\n",
    "\n",
    "此时，连续属性离散化技术可派上用场。\n",
    "\n",
    "最简单的策略是采用二分法（bi-partition）对连续属性进行处理，这正是C4.5决策树算法中采用的机制。\n",
    "\n",
    "给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大进行排序，记为${a^1,a^2,...,a^n}$，基于划分点t可将D分为子集$D_t^-$ 和$D_t^+$，其中$D_t^-$包含哪些在属性a上取值不大于t的样本，而$D_t^+$则包含哪些在属性a上取值大于t的样本。\n",
    "\n",
    "显然，对于相邻近的属性取值$a^i$与$a^i+1$来说，t在区间$[a^i,a^{i+1})$ 中取任意值所产生的划分结果相同。因此，对连续属性a，我们可考察包含n-1个元素的候选划分点集合：\n",
    "\n",
    "$T_a = \\{\\frac{a^i + a^{i+1}}{2}| 1 \\leq i \\leq n-1\\}$  ——式（7）\n",
    "\n",
    "> 根据【Quinlan，1993】的建议，可将划分点设为该属性在训练集中出现的不大于中位点的最大值，从而使得最终决策树使用得划分点都在训练集中出现过。\n",
    "\n",
    "即把区间$[a^i,a^{i+1})$中位点$\\frac{a^i + a^{i+1}}{2}$作为候选划分点。然后，我们就像离散属性值一样，来考察这些划分点，选取最优的划分点进行样本集合的划分。\n",
    "\n",
    "例如，可对式（2）稍加改造：\n",
    "\n",
    "$Gain(D,a) = max_{t \\in T_a}Gain(D,a,t) = max_{t \\in T_a} Ent(D) - \\sum_{\\lambda \\in \\{ -,+\\}}^{V}\\frac{|D_t^v|}{|D|}Ent(D_t^v)$  ——式（2）\n",
    "\n",
    "其中$Gain(D,a,t)$是样本集D基于划分点t二分后得信息增益。于是，我们就可选择使Gain(D,a,t)最大化得划分点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值处理\n",
    "\n",
    "现实任务中，常会遇到不完整样本，即样本得某些属性值缺失。例如：隐私问题、成本问题等造成得部分数据缺失。这种数据普遍存在，弃之不用会造成数据得浪费，但也不能简单得使用类似完整数据的应用方法。\n",
    "\n",
    "我们需要解决两个问题：\n",
    "\n",
    "- （1）如何在属性值缺失的情况下进行划分属性选择？\n",
    "- （2）给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？\n",
    "\n",
    "给定训练集D和属性a，令$\\tilde{D}$表示D中在属性a上没有缺失值的样本子集。对于问题（1），显然我们仅可根据$\\tilde{D}$来判断属性a的优劣。假定属性a有V个可取值${a^1,a^2,...,a^V}$，令$\\tilde{D}^v$表示$\\tilde{D}$中在属性a上取值为$a^v$的样本子集，$\\tilde{D}_k$表示$\\tilde{D}$中属于第k类${k=1,2,...|y|}$的样本子集，则显然有$\\tilde{D} = \\bigcup_{k=1}^{|y|}\\tilde{D}_k$，$\\tilde{D} = \\bigcup_{v=1}^{V}\\tilde{D}^v$。假定我们为每个样本x赋予一个权重$w_x$（在决策树学习开始阶段，根节点中个样本的权重初始化为1），并定义：\n",
    "\n",
    "$\\rho = \\frac{\\sum_{x \\in \\tilde{D}}w_x}{\\sum_{x \\in D}w_x}$   ——式（9）\n",
    "\n",
    "$\\tilde{p}_k = \\frac{\\sum_{x \\in \\tilde{D}_k}w_x}{\\sum_{x \\in \\tilde{D}}w_x} ， 其中 (1 \\leq k \\leq |y|)$  ——式（10）\n",
    "\n",
    "直观地看，对属性a，$\\rho$表示无缺失值样本所占地比例，$\\tilde{p}_k$表示无缺失值样本中第k类所占地比例，$\\tilde{r}_v$则表示无缺失值样本中在属性a上取值$a^v$的样本所占的比例。显然，$\\sum_{k=1}^{|y|}\\tilde{p}_k = 1, \\sum_{v=1}^{V}\\tilde{r}_v = 1$。\n",
    "\n",
    "基于上述定义，我们可以将信息增益的计算公式（2）推广为：\n",
    "\n",
    "$Gain(D,a) = \\rho \\times Gain(\\tilde{D},a) = \\rho \\times (Ent(\\tilde{D}) - \\sum_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v)))$ ——式（12）\n",
    "\n",
    "根据式（1），有：\n",
    "\n",
    "$Ent(\\tilde{D}) = -\\sum_{k=1}^{|y|}\\tilde{p}_k \\log_{2}{\\tilde{p}_k} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于问题（2），若样本x在划分属性上的取值已知，则将x划入与其取值对应的子结点，且样本权值在子节点中保持为$w_x$。若样本x在划分属性a上的取值未知，则将x同时划入所有子节点，且样本权值在属性值$a^v$对应的子节点中调整为$\\tilde{r}_v w_x$。直观地看，这就是让同一个样本以不同地概率画入到不同地子节点中去。\n",
    "\n",
    "C4.5算法使用了上述解决方案[Quinlan，1993]。\n",
    "\n",
    "## 多变量决策树 *\n",
    "\n",
    "若我们把每个属性视为坐标空间中地一个坐标轴，则d个属性描述地样本就对应了d维空间中的一个数据点，对样本分类意味着在这个坐标空间中寻找不同类样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：**轴平行（axis-parallel），即它的分类边界由若干个与坐标轴平行的分类组成。**\n",
    "\n",
    "（未完待续）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn库决策树分类器应用\n",
    "\n",
    "上面的程序是构建决策树的示例。在实际应用中，我们往往使用较为成熟的第三方工具sklearn。\n",
    "\n",
    "下面的示例中，使用sklearn.tree 中的DecisionTreeClassifier对投票数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "republican\n",
      "republican\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def decisionTree(list):\n",
    "    dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "    #data.describe()\n",
    "    X = dataframe.iloc[:,:-1] #存放训练样本中无标记的数据\n",
    "    X1 = pd.DataFrame() # sklearn的算法分类器大多只处理数值型矩阵，X1将存放数值化的样本\n",
    "    for column in X.columns:\n",
    "        X1[column] = X[column].apply(lambda x:1 if x=='y' else 2)\n",
    "        \n",
    "    y = dataframe.iloc[:,-1] # 存放标记    \n",
    "    y1 = y.apply(lambda x:1 if x == \"republican\" else 2) # 对标记进行数值化\n",
    "        \n",
    "    # 使用决策树模型对X,y进行拟合，即生成决策树分类器\n",
    "    clf = DecisionTreeClassifier(max_depth=3)\n",
    "    clf.fit(X1, y1)\n",
    "    # 对输入的list按上面生成的决策树分类器进行批量预测\n",
    "    predict = clf.predict([list])     \n",
    "    return predict[0]\n",
    "\n",
    "def predict():\n",
    "    dataframe = pd.read_csv(\"data/vote/Vote.csv\",header=None,sep=',')    \n",
    "    for index,row in dataframe.iterrows():\n",
    "        #print(row.tolist()[:-1])\n",
    "        row = row.apply(lambda x:1 if x=='y' else 2)\n",
    "        result = decisionTree(row.tolist()[:-1])\n",
    "        if result == 1:\n",
    "            print(\"republican\")\n",
    "        else:\n",
    "            print(\"democrat\")\n",
    "        \n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解sklearn的决策树分类器\n",
    "\n",
    "下面我们通过分析程序来理解上述决策树的基本情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes=11\n",
      "children_left=[ 1  2  3 -1 -1 -1  7 -1  9 -1 -1]\n",
      "children_right=[ 6  5  4 -1 -1 -1  8 -1 10 -1 -1]\n",
      "feature=[ 3 10  8 -2 -2 -2  2 -2  5 -2 -2]\n",
      "threshold=[ 1.5  1.5  1.5 -2.  -2.  -2.   1.5 -2.   1.5 -2.  -2. ]\n",
      "The binary tree structure has 11 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 3] <= 1.5 else to node 6.\n",
      "\tnode=1 test node: go to node 2 if X[:, 10] <= 1.5 else to node 5.\n",
      "\t\tnode=2 test node: go to node 3 if X[:, 8] <= 1.5 else to node 4.\n",
      "\t\t\tnode=3 leaf node.\n",
      "\t\t\tnode=4 leaf node.\n",
      "\t\tnode=5 leaf node.\n",
      "\tnode=6 test node: go to node 7 if X[:, 2] <= 1.5 else to node 8.\n",
      "\t\tnode=7 leaf node.\n",
      "\t\tnode=8 test node: go to node 9 if X[:, 5] <= 1.5 else to node 10.\n",
      "\t\t\tnode=9 leaf node.\n",
      "\t\t\tnode=10 leaf node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "#data.describe()\n",
    "X = dataframe.iloc[:,:-1] #存放训练样本中无标记的数据\n",
    "X1 = pd.DataFrame() # sklearn的算法分类器大多只处理数值型矩阵，X1将存放数值化的样本\n",
    "for column in X.columns:\n",
    "    X1[column] = X[column].apply(lambda x:1 if x=='y' else 2)\n",
    "y = dataframe.iloc[:,-1] # 存放标记    \n",
    "y1 = y.apply(lambda x:1 if x == \"republican\" else 2) # 对标记进行数值化\n",
    "\n",
    "# 使用决策树模型对X,y进行拟合，即生成决策树分类器\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X1, y1)\n",
    "\n",
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "    \n",
    "print(\"n_nodes={}\".format(n_nodes))\n",
    "print(\"children_left={}\".format(children_left))\n",
    "print(\"children_right={}\".format(children_right))\n",
    "print(\"feature={}\".format(feature))\n",
    "print(\"threshold={}\".format(threshold))\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的图形化显示\n",
    "\n",
    "上面构建的的决策树，不容易查看，我们能否借助图形化的方法来理解已经构建的决策树呢？\n",
    "\n",
    "### 使用matplotlib显示节点\n",
    "\n",
    "下面我们先尝试使用matplotlib的注解工具（annotations）来可视化自定义的节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# 设置显示中文\n",
    "from matplotlib.font_manager import FontProperties \n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=10)\n",
    "\n",
    "# 定义文本框和箭头格式\n",
    "decisionNode = dict(boxstyle =\"sawtooth\",fc=\"0.8\")\n",
    "leafNode = dict(boxstyle = \"round4\",fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"<-\")\n",
    "\n",
    "def plotNode(nodeTxt,centerPt,parentPt,nodeType):\n",
    "    createPlot.ax1.annotate(nodeTxt,xy = parentPt,xycoords=\"axes fraction\",\n",
    "                           xytext = centerPt,textcoords=\"axes fraction\",\n",
    "                           va = \"center\",ha=\"center\",bbox=nodeType,arrowprops=arrow_args,fontproperties='SimHei',fontsize=12 )\n",
    "def createPlot():\n",
    "    fig = plt.figure(1,facecolor =\"white\")\n",
    "    fig.clf()\n",
    "    createPlot.ax1 = plt.subplot(111,frameon=False)\n",
    "    plotNode('决策节点',(0.5,0.1),(0.1,0.5),decisionNode)\n",
    "    plotNode('叶节点',(0.8,0.1),(0.3,0.8),leafNode)\n",
    "    plt.show()\n",
    "createPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用matplotlib显示决策树\n",
    "\n",
    "上面的图示显然还需要完善，下面我们尝试显示整棵树。\n",
    "\n",
    "为了画出整棵树，需要知道树的层数有多高，有多少叶子节点，下面我们写两个函数来计算树的深度和叶子节点数。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.615px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

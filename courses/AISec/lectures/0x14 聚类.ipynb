{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 聚类\n",
    "\n",
    "\n",
    ">以下内容参考了周志华《机器学习》 清华大学出版社 2016.\n",
    "\n",
    "在无监督学习（unsupervised learning）中，训练样本的的标记信息是未知的。\n",
    "\n",
    "无监督学习的目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。\n",
    "\n",
    "此类学习任务中研究最多、应用最广的是聚类（clustering）。\n",
    "\n",
    "![聚类示例](images/clustering/clusteringdemo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 聚类任务\n",
    "\n",
    "聚类算法试图将数据集中的样本，划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）。\n",
    "\n",
    "划分后，每个簇可能对应某个潜在的概念/类别，以西瓜为例：“本地瓜”、“外地瓜”等。\n",
    "\n",
    "**需要注意：这些概念事先是未知的，聚类过程仅能够自动形成簇结构，簇所对应的概念语义需由使用者（聚类算法之外的程序或人）来把握和命名。**\n",
    "\n",
    "\n",
    "形式化表示：\n",
    "\n",
    "假定样本集 $D={x_1,x_2,...,x_m}$ 包含m个无标记样本，每个样本 $x_i = (x_{i1},x_{i2},...,x_{im}) $ 是一个n维特征向量，则聚类算法将样本集D划分为k个不相交地簇 $ \\{C_l | l = 1,2,...,k \\}$ ，其中 $ C_{l'} \\cap _{l'\\neq _l }C_l = \\emptyset $ 且 $D= \\cup ^k _{l=1} C_l $ 。\n",
    "\n",
    "相应地，我们用 $\\lambda_j \\in {1,2,...,k} $ 表示样本$x_j$的“簇标记”（cluster label），即$x_j \\in C_{\\lambda_j}$。\n",
    "\n",
    "于是，聚类的结果可用包含m个元素的簇标记向量$\\lambda = (\\lambda_1;\\lambda_2;...;\\lambda_m)$表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 聚类的应用意义\n",
    "\n",
    "聚类即能作为一个单独过程，用于寻找数据的潜在分布结构，也可以作为分类等其他学习任务的前驱过程。\n",
    "\n",
    "例如，在一些商业应用中需要对新用户的类型进行判别（用户画像），但定义“用户类型”对商家来说却不太容易，这时可以借助聚类工具对用户数据进行分析，根据聚类结果将每个簇定义为一个类，然后再基于这些类训练分类模型，用于判别新用户的类型。\n",
    "\n",
    "![用户画像的要素](images/clustering/用户画像的要素.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 用户画像\n",
    "\n",
    "- Step1：准确识别用户\n",
    "  + 用户识别的目的是为了区分用户、单点定位；\n",
    "  + 用户识别的方式有很多种，如cookie、注册ID、邮箱、微信/微博/QQ等第三方登录、手机号等；\n",
    "  + 手机号是目前移动端最为准确的用户标识；\n",
    "  + **微博/微信/QQ等第三方登录成企业识别用户的折中选择。**\n",
    "  \n",
    "- Step2：动态跟踪用户行为轨迹\n",
    "  + 动态行为数据可以确认用户不同场景下的不同访问轨迹，助力广告主跨端控频营销。\n",
    "  + 用户网络行为动态跟踪主要包括三个维度：**场景+媒体+路径**\n",
    "  + 应用到互联网中，场景主要包括访问设备、访问时段；\n",
    "  + 媒体指某一时段下用户具体访问的媒体，如资讯类、视频类、游戏类、社交类等；\n",
    "  + 路径指用户进入和离开某媒体的路径，可以简单理解为用户的站内与站外行为，如是通过搜索导航进入还是直接打开该APP，离开时是站内跳转到其他网页还是直接关闭。\n",
    "  + 一方面有助于媒体自身优化流量运营，另一方面帮助广告主有效控制不同页面的投放频次，避免产生用户倦怠。\n",
    "  \n",
    "- Step3：结合静态数据评估用户价值\n",
    "  + 静态数据获取后，需要对人群进行因子和聚类分析；\n",
    "  + 不同的目的分类依据不同：\n",
    "    + 如对于产品设计来说，按照使用动机或使用行为划分是最为常见的方式；\n",
    "    + 而对于营销类媒体来说，依据消费形态来区分人群是最为直接的分类方式。\n",
    "  + **静态数据主要包括用户的人口属性、商业属性、消费特征、生活形态、CRM五大维度。**\n",
    "    + 其获取方式存在多种，数据挖掘是最为常见也是较为精准的一种方式；\n",
    "    + 如果数据有限，则需要定性与定量结合补充，定性方法如小组座谈会、用户深访、日志法、Laddering 阶梯法、透射法等；\n",
    "    + 主要是通过开放性的问题潜入用户真实的心理需求，具象用户特征。\n",
    "    + 定量更多是通过定量问卷调研的方式进行，关键在于后期定量数据的建模与分析，目的是通过封闭性问题一方面对定性假设进行验证，另一方面获取市场的用户分布规律。\n",
    "    \n",
    "- Step4：用户标签定义与权重\n",
    "  + 根据特征值对群体进行定义，有助于广告主一目了然掌握该群体的特性。\n",
    "  + 一个群体会有多个标签，不同的群体之间也会有标签的重合，此时标签的权重反映了不同群体的核心特征。\n",
    "  + 通常，一个好的用户画像，不同人群之间的标签重合度较小，只有在那些权重较小的标签上会有些许重合。\n",
    "  + **需要从繁杂的数据中抽取共同的特征值**\n",
    "\n",
    "- Step5：不同人群优先级排列\n",
    "  + **根据企业自身情况排列不同组合。**\n",
    "  + 大部分画像只完成上述4步就结束了，然而最后一步决定了最终效果的落地。\n",
    "  + 对于广告主来说可以理解为媒介的组合策略。组合策略可以按照频率的高低、市场的大小、收益的潜力、竞争优势等，根据企业自身情况排列不同组合。\n",
    "  + 例如：品牌刚刚建立，需要快速提升知名度，可以按照不同媒体目标人群覆盖率的高低进行预算分配；当品牌具备一定知名度，企业核心领域营收处于快速增长期时，可以按照不同媒体目标人群贡献的市场大小进行分配；当企业想开拓新市场时，可以按照不同媒体目标人群的收益潜力进行分配，另外如企业品牌需增强差异化的竞争优势时，可按照不同媒体目标人群的竞争优势进行投放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 用户画像示例\n",
    "![用户画像示例](images/clustering/用户画像示例.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "基于不同的学习策略，有多种不同的聚类算法。下面先考虑算法涉及的两个基本问题：性能度量和距离计算。\n",
    "\n",
    "## 聚类算法的性能度量\n",
    "\n",
    "聚类性能度量亦称为聚类“有效性指标”（validity index）。\n",
    "\n",
    "与监督学习中的性能度量作用相似，对聚类结果，我们需要通过某种性能来度量或评估其好坏。\n",
    "\n",
    "另一方面，若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。\n",
    "\n",
    "聚类是将样本集D划分维若干互不相交的子集，即样本簇。那么，什么样的聚类结果比较好呢？\n",
    "\n",
    "直观上看，我们希望“物以类聚”，即同一簇的样本尽可能彼此相似，不同簇尽可能不同。\n",
    "\n",
    "即“簇内相似度”（intra-cluster similarity）高，而“蔟间相似度”（inter-cluster similarity）低。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚类性能度量类型\n",
    "\n",
    "度量类型常见地有两类：\n",
    "\n",
    "- 第一种：将聚类结果与某个“参考模型”（reference model）进行比较，称为“外部指标”（external index）；\n",
    "- 第二种：直接考察聚类结果，而不利用任何参考模型，称为“内部指标”（internal index）。\n",
    "\n",
    "对数据集 $D=\\{x_1,x_2,...,x_m\\}$，假定通过聚类给出的簇，划分为 $ C=\\{C_1,C_2,...,C_k\\}$，参考模型给出的簇划分为 $ C^*=\\{C_1^*,C_2^*,...,C_s^*\\}$。\n",
    "\n",
    "相应地，令$\\lambda$与$\\lambda^*$分别表示与$ C$和$ C^*$对应的簇标记向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "我们将样本两两配对考虑，定义：\n",
    "\n",
    "$a = |SS|, SS = \\{ (x_i,x_j)  |  \\lambda_i = \\lambda_j ,\\lambda_i^* = \\lambda_j^*,i<j \\},$   —— 式（1）\n",
    "\n",
    "$b = |SD|, SD = \\{ (x_i,x_j) | \\lambda_i = \\lambda_j ,\\lambda_i^* \\neq \\lambda_j^*,i<j \\},$   —— 式（2）\n",
    "\n",
    "$c = |DS|, DS = \\{ (x_i,x_j) | \\lambda_i \\neq \\lambda_j ,\\lambda_i^* = \\lambda_j^*,i<j \\},$   —— 式（3）\n",
    "\n",
    "$d = |DD|, DD = \\{ (x_i,x_j) | \\lambda_i \\neq \\lambda_j ,\\lambda_i^* \\neq \\lambda_j^*,i<j \\},$   —— 式（4）\n",
    "\n",
    "其中：\n",
    "- 集合SS包含了在C中隶属于相同簇,且在$C^*$中也隶属于相同簇的样本对；\n",
    "- 集合SD包含了在C中隶属于相同簇,但在$C^*$中不隶属于相同簇的样本对；\n",
    "- 集合DS包含了在C中不隶属于相同簇,但在$C^*$中隶属于相同簇的样本对；\n",
    "- 集合DD包含了在C中不隶属于相同簇,且在$C^*$中不隶属于相同簇的样本对；\n",
    "\n",
    "由于每个样本对$(x_i,x_j) ,（i<j）$，仅能出现在上面某一个集合中，因此有$a+b+c+d = m(m-1)/2$成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 聚类性能度量的外部指标\n",
    "\n",
    "基于上面的（1）到（4）式，可以推导出下面这些常用的聚类性能度量外部指标：\n",
    "\n",
    "- Jaccard系数（Jaccard Coefficient，简称JC)\n",
    "\n",
    "> $JC = \\frac{a}{a+b+c}$   —— 式(5)\n",
    "\n",
    "- FM指数（Fowlkes and mallows Index，简称FMI)\n",
    "\n",
    "> $FMI = \\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}$   —— 式(6)\n",
    "\n",
    "- Rand指数（Rand Index，简称RI)\n",
    "\n",
    "> $RI = \\frac{2(a+d)}{m(m-1)}$   —— 式（7）\n",
    "\n",
    "显然，上述性能度量的结果值均在[0,1]区间，值越大越好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 聚类性能度量的内部指标\n",
    "\n",
    "考虑聚类结果的簇划分 $C=\\{C_1,C_2,...,C_k\\}$，定义：\n",
    "\n",
    "> 簇C内样本间的平均距离： $avg(C) = \\frac{2}{|C|(|C|-1)} \\sum_{1\\leq i<j\\leq|C|}dist(x_i,x_j)$   —— 式(8)\n",
    "\n",
    "> 簇C内样本间的最远距离：$diam(C) = max_{1\\leq i<j\\leq|C|}dist(x_i,x_j)$  —— 式(9)\n",
    "\n",
    "> 簇$C_i$与簇$C_j$最近样本间的距离：$d_{min}(C_i,C_j) = min_{x_i \\in C_i,x_j \\in C_j} dist(x_i,x_j)$  —— 式(10)\n",
    "\n",
    "> 簇$C_i$与簇$C_j$中心点间的距离：$d_{cen}(C_i,C_j) = dist(\\mu_i ,\\mu_j)$   —— 式(11)\n",
    "\n",
    "其中：\n",
    "- dist(.,.)用于计算两个样本之间的距离(具体计算方法见后文)；\n",
    "- $\\mu$ 代表簇C的中心点$\\mu = \\frac{1}{|C|}\\sum_{1 \\leq i \\leq |C|}x_i$ 。\n",
    "\n",
    "\n",
    "基于式（8）~（11)可导出下面这些常用的聚类性能度量内部指标：\n",
    "\n",
    "- DB指数（Davies-Bouldin Index，简称DBI)\n",
    "\n",
    "> $ DBI = \\frac{1}{k} \\sum_{i=1}^{k}max_{j \\neq i}(\\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)})$  —— 式(12)\n",
    "\n",
    "- Dunn指数（Dunn Index，简称DI)\n",
    "\n",
    "> $DI = min_{1 \\leq i \\leq k}{min_{j \\neq i} (\\frac{d_{min}(C_i,C_j)}{max_{1 \\leq l \\leq k}diam(C_l)})}$   —— 式(13)\n",
    "\n",
    "显然，DBI的值越小越好，而DI则相反，越大越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 距离计算\n",
    "\n",
    "对函数dist(.,.),若它是一个“距离向量”（distance measure），则需要满足一些基本性质：\n",
    "\n",
    "- 非负性：$ dist(x_i,x_j) \\geq 0$ —— 式(14)\n",
    "- 同一性：$ dist(x_i,x_j) = 0 $当前仅当$x_i= x_j$ ——式(15)\n",
    "- 对称性：$ dist(x_i,x_j) = dist(x_j,x_i) $ —— 式(16)\n",
    "- 直递性：$ dist(x_i,x_j) \\leq dist(x_i,x_k) + dist(x_k,x_j)$ —— 式(17)\n",
    "\n",
    "### 闵可夫斯基距离\n",
    "\n",
    "给定样本$x_i = (x_{i1};x_{i2};...;x_{in})$ 与$x_j = (x_{j1};x_{j2};...;x_{jn})$，最常用的式“闵可夫斯基距离”（Minkowski distance）：\n",
    "\n",
    "> $dist_{mk}(x_i,x_j) = (\\sum_{u=1}^n|x_{iu} - x_{ju}|^p)^\\frac{1}{p}$  —— 式(18)\n",
    "\n",
    "对$p \\geq 1$，式（18）显然满足式（14）~式（18）的距离度量基本性质。其实，式（18）即为$x_i-x_j$的$L_p$范数$||x_i-x_j||_p$\n",
    "\n",
    "> L-P范数是一组范数，$L_p = ||X||_p = \\sqrt[p]{\\sum_{i=1}^{n} |x_i|^p}, X = (x_1,x_2,...,x_n)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "根据$dist_{mk}(x_i,x_j) = (\\sum_{u=1}^n|x_{iu} - x_{ju}|^p)^\\frac{1}{p}$ \n",
    "\n",
    "- p=2时，即为欧式距离（Euclidean distance）\n",
    "- p=1时，即为曼哈顿距离（Manhattan distance)，也称为“街区距离”（city block distance）\n",
    "\n",
    "我们常将属性划分为：\n",
    "- “连续属性”（continuous attribute）或“数值属性”（numerical attribute）\n",
    "- “离散数学”（categorical attribute）或“列命属性”（nominal attribute）\n",
    "\n",
    "连续属性在定义域上有无穷多个可能的取值，而离散属性在定义域上是有限个取值。\n",
    "\n",
    "然而在讨论距离计算时，属性上是否定义了“序”关系更为重要。\n",
    "\n",
    "例如定义域为{1，2，3}的离散属性与连续属性的性质更接近一些，能直接在属性值上计算距离：“1”与“2”比较接近、与“3”比较远，这样的属性称为“有序”属性（ordinal attribute）；而定义域为{飞机，火车，轮船}这样的离散属性则不能直接在属性值上计算距离，称为“无序属性”（non-ordinal attribute）。\n",
    "\n",
    "显然，**闵可夫斯基距离可用于有序属性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### VDM距离\n",
    "\n",
    "对于无序属性，可采用VDM（Value Difference Metric）。\n",
    "\n",
    "令$m_{u,a}$表示在属性u上取值为a的样本数，$m_{u,a,i}$表示在第i个样本属性u上取值为a的样本数，k为样本簇数，则属性u上两个离散值a与b之间的VDM距离为：\n",
    "\n",
    "> $VDM_p(a,b) = \\sum_{i=1}^k|\\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_{u,b}}|^p$ —— 式（21）\n",
    "\n",
    "于是，**将闵可夫斯基距离和VDM结合即可处理混合属性。**\n",
    "\n",
    "假定有$n_c$个有序属性、$n-n_c$个无序属性，不失一般性，令有序属性排列在无序属性之前，则有：\n",
    "\n",
    "> $MinkovDM_p(x_i,x_j) = (\\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p + \\sum_{u=n_c+1}^n VDM_p(x_{iu},x_{ju}))^\\frac{1}{p}$  ——式（22）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 加权距离\n",
    "\n",
    "当样本空间中不同属性的重要性不同时，可使用“加权距离”（weighted distance）。以加权闵可夫斯基距离为例：\n",
    "\n",
    "$dist_{wmk}(x_i,x_j) = (\\sum_{u=1}^n \\omega_u|x_{iu} - x_{ju}|^p)^\\frac{1}{p}$  —— 式(23)\n",
    "\n",
    "其中，权重$\\omega_u \\geq 0 (i = 1,2,...n)$表征不同属性的重要性，通常$\\sum_{u=1}^n \\omega_u = 1$。\n",
    "\n",
    "需要注意的是，通常我们是基于某种形式的距离来定义“相似度度量”（similarity measure），距离越大，相似度越小。然而，用于相似度度量的距离未必一定要满足距离度量的所有基本性质，尤其是直递性（式17）。\n",
    "\n",
    "例如：可能要“人”“马”分别与“人马”相似，但“人”与“马”很不相似。要达到这个目的可令“人”、“马”与“人马”之间的距离都比较小，但“人”与“马”之间的距离很大。**此时该距离不再满足直递性，这样的距离称为“非度量距离”（non-metric distance）**。\n",
    "\n",
    "此外，本节介绍的距离计算式都是事先定义好的，但有些任务中，有必要基于数据样本来确定合适的距离计算式，或者可通过“距离度量学习”（distance metric learning）来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### “非度量距离”（non-metric distance）举例\n",
    "![ “非度量距离”（non-metric distance）举例](images\\clustering\\非度量距离例子.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原型聚类\n",
    "\n",
    "原型聚类亦称为“基于原型的聚类”（prototype-based clustering），此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常用。\n",
    "\n",
    "通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解。\n",
    "\n",
    "采用不同的原型表示、不同的求解方式，将产生不同的算法。下面是一些著名的原型聚类算法：\n",
    "\n",
    "- k均值算法\n",
    "- 学习向量量化\n",
    "- 高斯混合聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-MEANS算法\n",
    "\n",
    "K-Means算法是一种典型的聚类算法。\n",
    "\n",
    "#### 一般形式：\n",
    "\n",
    "给定样本集 $ D = \\{x_1,x_2,...，x_m\\}$，“k均值”（k-means）算法针对聚类所得簇划分 $ C = \\{C_1,C_2,...,C_k\\}$ 最小化平方误差：\n",
    "\n",
    "$ E = \\sum_{i=1}^k \\sum_{x \\in C_i}||x- \\mu_i||_2^2$ ——式（24）\n",
    "\n",
    "其中，$\\mu_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i}x$ 是簇$C_i$的均值向量。\n",
    "\n",
    "直观来看，式（24）在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，E值越小则簇内样本相似度越高。\n",
    "\n",
    "#### 最小化\n",
    "\n",
    "求式（24）的最小化并不容易，找到它的最优解需要考查样本集D所有可能的簇划分，这是一个NP难题（Aloise et al. 2009)。\n",
    "\n",
    "因此，k均值算法采用了贪心策略，通过迭代优化来近似求解式（24）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### k均值算法流程（贪心策略）\n",
    "\n",
    "K-means聚类算法以k为参数，把n各对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。它的基本思想有以下几点：\n",
    "\n",
    "  - 随机选择k个点，作为初始的聚类中心；\n",
    "  - 对于剩下的点，根据其与聚类中心的距离，将其归入最近的簇；\n",
    "  - 对每个簇,计算所有点的均值作为新的聚类中心；\n",
    "  - 重复上述两步直到聚类中心不再发生改变。\n",
    "\n",
    "请看下面图示：\n",
    "图1：给定一个数据集；\n",
    "![图1](images\\clustering\\example1.jpg)\n",
    "图2：根据K = 5初始化聚类中心，保证　聚类中心处于数据空间内；\n",
    "![图2](images\\clustering\\example2.jpg)\n",
    "图3：根据计算类内对象和聚类中心之间的相似度指标，将数据进行划分；\n",
    "![图3](images\\clustering\\example3.jpg)\n",
    "图4：将类内之间数据的均值作为聚类中心，更新聚类中心。\n",
    "![图4](images\\clustering\\example4.jpg)\n",
    "最后判断算法结束与否即可，目的是为了保证算法的收敛。\n",
    "\n",
    "![k均值算法流程](images\\clustering\\k均值算法流程.png)\n",
    "\n",
    "下面以下表所示数据为例，进行演示k-mean算法的学习过程。\n",
    "\n",
    "![西瓜数据集4](images\\clustering\\西瓜数据集4.png)\n",
    "\n",
    "假定聚类簇数 k=3，算法开始时随机选取三个样本$x_6,x_{12},x_{27}$作为起始均值向量，即：\n",
    "\n",
    "$\\mu_1 = (0.403;0.237), \\mu_2 = (0.343;0.099), \\mu_3 = (0.532;0.472)$\n",
    "\n",
    "考察样本$x_1 = (0.697;0.460)$,它与当前均值向量 $\\mu_1,\\mu_2,\\mu_3$ 的距离分别为0.369，0.506，0.166.因此$x_1$被划入簇$C_3$中。类似的，对数据集中的所有样本考察一遍后，可得当前簇划分为：\n",
    "\n",
    "$C_1 = \\{x_5,x_6,x_7,x_8,x_9,x_10,x_13,x_14,x_17,x_18,x_19,x_20,x_23\\}$\n",
    "\n",
    "$C_2 = \\{x_11,x_12,x_16\\}$\n",
    "\n",
    "$C_3 = \\{x_1,x_2,x_3,x_4,x_21,x_22,x_24,x_25,x_26,x_27,x_28,x_29,x_30\\}$\n",
    "\n",
    "于是，可从$C_1,C_2,C_3 $分别求出新的均值向量：\n",
    "\n",
    "$\\mu_1^{'} = (0.473;0.214), \\mu_2^{'} = (0.394;0.066), \\mu_3^{'} = (0.623;0.388)$\n",
    "\n",
    "更新当前均值向量后，不断重复上述过程，如下图所示，第五轮迭代产生的结果与第四轮迭代相同，于是算法停止，得到最终的簇划分。\n",
    "\n",
    "![kmean聚类西瓜集4](images\\clustering\\kmean聚类西瓜集4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贪心算法\n",
    "\n",
    "在每一步求解的步骤中，它要求“贪婪”的选择最佳操作，并希望通过一系列的最优选择，能够产生一个问题的（全局的）最优解。\n",
    "\n",
    "贪心算法每一步必须满足一下条件：\n",
    "\n",
    "- 可行的：即它必须满足问题的约束。\n",
    "\n",
    "- 局部最优：他是当前步骤中所有可行选择中最佳的局部选择。\n",
    "\n",
    "- 不可取消：即选择一旦做出，在算法的后面步骤就不可改变了。\n",
    "\n",
    "例如：\n",
    "\n",
    "【活动选择问题】这是《算法导论》上的例子，也是一个非常经典的问题。有n个需要在同一天使用同一个教室的活动a1,a2,…,an，教室同一时刻只能由一个活动使用。每个活动ai都有一个开始时间si和结束时间fi 。一旦被选择后，活动ai就占据半开时间区间[si,fi)。如果[si,fi]和[sj,fj]互不重叠，ai和aj两个活动就可以被安排在这一天。该问题就是要安排这些活动使得尽量多的活动能不冲突的举行。例如下图所示的活动集合S，其中各项活动按照结束时间单调递增排序。\n",
    "\n",
    "用贪心法的话思想很简单：活动越早结束，剩余的时间是不是越多？那我就早最早结束的那个活动，找到后在剩下的活动中再找最早结束的不就得了？\n",
    "\n",
    "虽然贪心算法的思想简单，但是贪心法不保证能得到问题的最优解，如果得不到最优解，那就不是我们想要的东西了，所以我们现在要证明的是在这个问题中，用贪心法能得到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means的应用案例\n",
    "\n",
    "- 数据介绍\n",
    "现有1999年全国31个省份城镇居民家庭平均每人全年消费性支出的八个主要变量数据，这8个变量分别是：**食品、衣着、家庭设备用品及服务、医疗保健、交通和通信、娱乐教育文化服务、居住以及杂项商品和服务**。利用已有数据，对31个省份进行聚类。\n",
    "- 实验目的\n",
    "通过聚类，了解1999年各个省份的消费水平在国内的情况。\n",
    "- 技术路线\n",
    "sklearn.cluster.Kmeans\n",
    "- 实验过程\n",
    "  + step 1 建立程序，导入sklearn.cluster.Kmeans\n",
    "  + step 2 导入数据\n",
    "  + step 3 训练模型，确定k的数目\n",
    "  \n",
    "> sklearn中每个聚类方法有两个变体：一个类用于实现fit学习训练数据;一个函数用于对给定训练数据，返回对应于不同聚类的整数标签数组。对于这个类，可以在labels_属性中找到训练数据上的标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>北京</th>\n",
       "      <th>2959.19</th>\n",
       "      <th>730.79</th>\n",
       "      <th>749.41</th>\n",
       "      <th>513.34</th>\n",
       "      <th>467.87</th>\n",
       "      <th>1141.82</th>\n",
       "      <th>478.42</th>\n",
       "      <th>457.64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天津</td>\n",
       "      <td>2459.77</td>\n",
       "      <td>495.47</td>\n",
       "      <td>697.33</td>\n",
       "      <td>302.87</td>\n",
       "      <td>284.19</td>\n",
       "      <td>735.97</td>\n",
       "      <td>570.84</td>\n",
       "      <td>305.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>河北</td>\n",
       "      <td>1495.63</td>\n",
       "      <td>515.90</td>\n",
       "      <td>362.37</td>\n",
       "      <td>285.32</td>\n",
       "      <td>272.95</td>\n",
       "      <td>540.58</td>\n",
       "      <td>364.91</td>\n",
       "      <td>188.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>山西</td>\n",
       "      <td>1406.33</td>\n",
       "      <td>477.77</td>\n",
       "      <td>290.15</td>\n",
       "      <td>208.57</td>\n",
       "      <td>201.50</td>\n",
       "      <td>414.72</td>\n",
       "      <td>281.84</td>\n",
       "      <td>212.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>内蒙古</td>\n",
       "      <td>1303.97</td>\n",
       "      <td>524.29</td>\n",
       "      <td>254.83</td>\n",
       "      <td>192.17</td>\n",
       "      <td>249.81</td>\n",
       "      <td>463.09</td>\n",
       "      <td>287.87</td>\n",
       "      <td>192.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>辽宁</td>\n",
       "      <td>1730.84</td>\n",
       "      <td>553.90</td>\n",
       "      <td>246.91</td>\n",
       "      <td>279.81</td>\n",
       "      <td>239.18</td>\n",
       "      <td>445.20</td>\n",
       "      <td>330.24</td>\n",
       "      <td>163.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    北京  2959.19  730.79  749.41  513.34  467.87  1141.82  478.42  457.64\n",
       "0   天津  2459.77  495.47  697.33  302.87  284.19   735.97  570.84  305.08\n",
       "1   河北  1495.63  515.90  362.37  285.32  272.95   540.58  364.91  188.63\n",
       "2   山西  1406.33  477.77  290.15  208.57  201.50   414.72  281.84  212.10\n",
       "3  内蒙古  1303.97  524.29  254.83  192.17  249.81   463.09  287.87  192.96\n",
       "4   辽宁  1730.84  553.90  246.91  279.81  239.18   445.20  330.24  163.86"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"数据概览\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "d  = pd.read_csv('data/clustering/31city.txt',encoding='gb2312')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2959.19], [2459.77], [1495.63], [1406.33], [1303.97]]\n",
      "['北京', '天津', '河北', '山西', '内蒙古']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "def loadData(filePath):\n",
    "    fr = open(filePath,'r+',encoding='gb2312')\n",
    "    lines = fr.readlines()\n",
    "    retData = []\n",
    "    retCityName = []\n",
    "    for line in lines:\n",
    "        items = line.strip().split(\",\")\n",
    "        retCityName.append(items[0])\n",
    "        retData.append([float(items[1])])\n",
    "    for i in range(1,len(items)):\n",
    "        return retData,retCityName\n",
    "a = loadData('data/clustering/31city.txt')\n",
    "\n",
    "print(a[0][:5])\n",
    "print(a[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习向量量化\n",
    "\n",
    "与k均值算法类似，“学习向量量化”（Learning Vector Quantization，简称LVQ）也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。\n",
    "\n",
    "给定样本集$D= \\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m) \\}$，每个样本$x_j$是由n个属性描述的特征向量$(x_{j1},x_{j2},...,x_{jn})$, $y_j \\in Y$是样本$x_j$的类别标记，LVQ的目标是学得一组n维原型向量$\\{p_1,p_2,...,p_q\\}$，每个原型向量代表一个聚类簇，簇标记$t_i \\in Y$.\n",
    "\n",
    "LVQ算法步骤如下：\n",
    "\n",
    "![LVQ算法](images\\clustering\\LVQ算法步骤.png)\n",
    "\n",
    "显然，LVQ的关键在于第6～第10行，即如何更新原型向量。\n",
    "\n",
    "直观上看，对样本$x_j$，若最近的原型向量$p_{i^*}$与$x_j$的类标记相同，则令$p_{i^*}$向$x_j$的方向靠拢，如第7行所示，此时新原型向量为：\n",
    "\n",
    "> $p' = p_{i^*}+\\eta \\centerdot (x_j - p_{i^*})$  ——式（25）\n",
    "\n",
    "$p'$与$x_j$之间的距离为：\n",
    "\n",
    "> $||p'-x_j||_2 = ||p_{i^*} + \\eta \\centerdot (x_j - p_{i^*}) - x_j||_2 = (1-\\eta) \\centerdot ||p_{i^*} - x_j||_2$ ——式（26）\n",
    "\n",
    "令学习率$\\eta \\in (0,1)$，则原型向量$p_{i^*}$在更更新为p‘之后将更接近$x_j$。\n",
    "\n",
    "类似的，若$p'$与$x_j$的类别标记不同，则更新后的原型向量与$x_j$之间的距离将增大为$(1+\\eta) \\centerdot ||p_{i^*} - x_j||_2$ ,从而远离$x_j$。\n",
    "\n",
    "在学得一组原型向量$\\{p_1,p_2,...,p_q\\}$后，即可实现对样本空间X的簇划分。对于任意x，他将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量$p_i$定义了与之相关的一个区域$R_i$,该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i'},(i' \\neq i)$的距离，即：\n",
    "\n",
    "$R_i = \\{x \\in X| \\quad ||x-p_i||_2 \\leq ||x-p_{i'}||_2 ,\\quad i' \\leq i \\}$ ——式（27）\n",
    "\n",
    "由此形成了对样本空间X的簇划分$\\{R_1,R_2,...,R_q \\}$,该划分通常称为“Voronaoi剖分”（Voronoi tessellation）。\n",
    "\n",
    "下面以上面西瓜数据集为例，演示LVQ的学习过程。令9-21号样本的类别标记为$C_2$,其他样本的标记为$C_1$，假定$q=5$，即学习目标是找到5个原型向量$\\{p_1,p_2,...,p_5\\}$，并假定其对应的类别标记分别为$\\{c_1,c_2,c_2,c_1,c_1\\}$。\n",
    "\n",
    "算法开始时，根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，假定初始化为样本$\\{ x_5,x_{12},x_{18},x_{23},x_{29}\\}$，在第一轮迭代中，假定随机选取的样本为$x_1$，该样本与当前原型向量$\\{p_1,p_2,...,p_5\\}$的距离分别为：0.283，0.506，0.434，0.260，0.032.\n",
    "\n",
    "由于$p_5$与$x_1$最接近且两者具有相同的类别标记$c_2$,假定学习率$\\eta = 0.1$,则LVQ更新$p_5$得到新的原型向量：\n",
    "\n",
    "$p‘ = p_5 + \\eta \\centerdot (x_1 - p_5) = (0.725;0.445) +0.1(0.697;0.46) -(0.725;0.445) =(0.722;0.442)$\n",
    "\n",
    "将$p_5$更新为p‘后，不断重复上述过程，不同轮数之后的聚类结果如下图所示：\n",
    "\n",
    "![lvq对西瓜数据4的聚类](images\\clustering\\lvq对西瓜数据4的聚类.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高思混合聚类\n",
    "\n",
    "与k-mean、LVQ用原型向量来刻画聚类结构不同，高斯混合（Mixture-of-Gaussian）聚类采用概率模型来表达聚类原型。\n",
    "\n",
    "我们先简单回顾一下高斯分布的定义。对n维样本空间X中的随机向量x，若x服从高斯分布，其概率密度函数为：\n",
    "\n",
    "> $p(x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\sum|^{\\frac{1}{2}}} e^{- \\frac{1}{2}(x-\\mu)^T \\sum^{-1}(x-\\mu)}$ ——式（28）\n",
    "\n",
    "其中，$\\mu$是n维均值向量，$\\sum$是 $n \\times n$的协方差矩阵。\n",
    "\n",
    "> 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。期望值分别为$E(X),E(Y)$的两个实随机变量X与Y之间的协方差Cov(X,Y)定义为: $Cov(X,Y) = E((X-E(X))(Y-E(Y))) = E(XY)-E(X)E(Y)$. 如果X与Y是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足E[XY]=E[X]E[Y]。但是，反过来并不成立。即如果X与Y的协方差为0，二者并不一定是统计独立的。\n",
    "\n",
    "> 协方差矩阵（Covariance Matrix ）：设有$X = (x_1,x_2,...,x_n)$为n维随机向量,称矩阵$C = (c_{ij})_{n*n}, c_{ij}=Cov(x_i,x_j)$为n维随机变量向量X的协方差矩阵。协方差矩阵为对称非负定矩阵。\n",
    "\n",
    "从高斯分布的概率密度函数公式可以看出，高斯分布完全由均值向量$\\mu$ 和协方差矩阵$\\sum$这两个参数确定。\n",
    "\n",
    "为了明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为$p(x|\\mu,\\sum)$。\n",
    "\n",
    "我们可定义高斯混合分布：\n",
    "\n",
    "> $p_M (x) = \\sum _{i=1}^k \\alpha_i \\centerdot p(x|\\mu_i,\\sum_i)$  ——式（29）\n",
    "\n",
    "$p_M (x) $也是概率密度函数，有$\\int p_M(x) = 1$.\n",
    "\n",
    "该分布共由k个混合成分组成，每个混合成分对应一个高斯分布。其中$\\mu_i$与$\\sum_i$是第i个高斯混合成分的参数，而$\\alpha_i > 0 $为相应的“混合系数”（mixture coefficient），$\\sum_{i=1}^k \\alpha_i = 1$.\n",
    "\n",
    "假设样本的生成过程由高斯混合分布给出：\n",
    "\n",
    "- 首先，根据$\\alpha_1,\\alpha_2,...,\\alpha_k$定义的先验分布选择高斯混合成分，其中$\\alpha_i $为第i个混合成分的概率；\n",
    "- 然后，根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。\n",
    "\n",
    "（未完。。。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn中的高斯混合聚类模型\n",
    "\n",
    "sklearn.mixture 是一个应用高斯混合模型进行非监督学习的包，支持 diagonal，spherical，tied，full 四种协方差矩阵 （注：diagonal 指每个分量有各自不同对角协方差矩阵， spherical 指每个分量有各自不同的简单协方差矩阵， tied 指所有分量有相同的标准协方差矩阵， full 指每个分量有各自不同的标准协方差矩阵），它对数据进行抽样，并且根据数据估计模型。同时包也提供了相关支持，来帮助用户决定合适的分量数（分量个数）。 （译注：在高斯混合模型中，我们将每一个高斯分布称为一个分量，即 component ）\n",
    "\n",
    "![二分量高斯混合模型](images/clustering/二分量高斯混合模型.jpg)\n",
    "\n",
    "高斯混合模型是一个假设所有的数据点都是生成于一个混合的有限数量的并且未知参数的高斯分布的概率模型。 我们可以将混合模型看作是 k-means 聚类算法的推广，它利用了关于数据的协方差结构以及潜在高斯中心的信息。\n",
    "\n",
    "对应不同的估算策略，Scikit-learn 实现了不同的类来估算高斯混合模型。 详细描述如下：\n",
    "\n",
    "##### GaussianMixture \n",
    "\n",
    "GaussianMixture 对象实现了用来拟合高斯混合模型的 期望最大化 (EM) 算法。它还可以为多变量模型绘制置信区间，同时计算 BIC（Bayesian Information Criterion，贝叶斯信息准则）来评估数据中聚类的数量。 GaussianMixture.fit 提供了从训练数据中学习高斯混合模型的方法。\n",
    "\n",
    "给定测试数据，通过使用 GaussianMixture.predict 方法，可以为每个样本分配最有可能对应的高斯分布。\n",
    "\n",
    "GaussianMixture 方法中自带了不同的选项来约束不同估类的协方差：spherical，diagonal，tied 或 full 协方差。\n",
    "\n",
    "##### 变分贝叶斯高斯混合\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 密度聚类\n",
    "\n",
    "密度聚类也称为“基于密度的聚类”（density-based clustering）。\n",
    "\n",
    "此类算法假设聚类结构能够通过样本分布的紧密程度确定。通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇，以获得最终的聚类结果。\n",
    "\n",
    "### DBSCAN 方法\n",
    "\n",
    "DBSCAN是一种著名的密度聚类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 18\n",
      "Homogeneity: 0.953\n",
      "Completeness: 0.883\n",
      "V-measure: 0.917\n",
      "Adjusted Rand Index: 0.952\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "adjusted_mutual_info_score() got an unexpected keyword argument 'average_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-95251211d89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"V-measure: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_measure_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adjusted Rand Index: %0.3f\"\u001b[0m  \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjusted_rand_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adjusted Mutual Information: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjusted_mutual_info_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arithmetic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Silhouette Coefficient: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: adjusted_mutual_info_score() got an unexpected keyword argument 'average_method'"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"  % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels,average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

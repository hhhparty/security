{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯\n",
    "\n",
    "## 根据概率进行分类\n",
    "\n",
    "使用机器学习算法进行预测分类，通常我们得到的结果是：“样本属于类型1或类型2”。\n",
    "\n",
    "![2分类问题](images/bayes/两个类型2.jpg)\n",
    "\n",
    "这个结论有点生硬，而且还不总是正确的。\n",
    "\n",
    "人们希望分类器不仅给出预测分类的结果，同时给出预测正确的概率估计值，这样更容易让人接受。\n",
    "\n",
    "或者在预测时，计算出样本属于各个类别的概率，这样我们可以比较概率值，属于哪个类别的概率值大，就判定该样本属于哪个类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如，我们用P1(x)来表示数据样本x属于类别1的概率，用P2(x)来表示数据样本x属于类别2的概率.\n",
    "\n",
    "对于一个新的数据样本$x^{'}$，可以用下面的规则来判断它的类别：\n",
    "\n",
    "- 如果$P1(x^{'}) > P2(x^{'})$ ,那么类别为1；\n",
    "- 如果$P1(x^{'}) < P2(x^{'})$ ,那么类别为2；\n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。\n",
    "\n",
    "**这一思想是利用概率进行分类的思想，也是贝叶斯决策论的核心思想。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率计算基础\n",
    "\n",
    "### 条件概率\n",
    "\n",
    "条件概率（又称后验概率）就是事件B在另外一个事件A已经发生条件下的发生概率。\n",
    "\n",
    "$P(B|A) = \\frac{P(AB)}{P(A)}，其中P(A) \\neq 0$\n",
    "\n",
    "性质：$P(\\bullet |A)$是概率，\n",
    "\n",
    "- 非负性：$P(B|A) \\geq 0$\n",
    "- 规范性：$P(S|A) = 1$\n",
    "- 可列可加性：$B_1,B_2,...,B_iBj = \\emptyset ,i \\neq j$\n",
    "\n",
    "则 $P(\\bigcup_{i=1}^\\infty B_i|A) = \\sum_{i=1}^\\infty P(B_i|A)$\n",
    "\n",
    "举例：一对夫妇有两个孩子，至少有一个是女孩儿，那么家里有2个女孩儿的概率是多少？（假定生男生女概率相同）。\n",
    "\n",
    "解：根据题意，样本空间为 $S = {(兄，弟),(兄，妹),(姐，弟),(姐，妹)}$\n",
    "\n",
    "已经知道的事件 $A = {(兄，妹),(姐，弟),(姐，妹)}$\n",
    "\n",
    "欲求概率的事件 $B = {(姐，妹)}$\n",
    "\n",
    "$P(B|A) = \\frac{1}{3}$\n",
    "\n",
    "### 全概率公式\n",
    "\n",
    "全概率公式为概率论中的重要公式，它将对一复杂事件A的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题。\n",
    "\n",
    "定义：称$B_1,B_2,...，B_n$ 为S的一个划分，若（1）不漏，即$B_1\\bigcup B_2 \\bigcup ...\\bigcup B_n = S$；（2）不重，$B_1B_2 = \\emptyset，i \\neq j$.\n",
    "\n",
    "定理：设$B_1,B_2,...，B_n$ 为S的一个划分，且$P(B_i) > 0$. 则有全概率公式：$P(A) = \\sum_{j=1}^n P(B_j)P(A|B_j)$。\n",
    "\n",
    "注：运用全概率公式，关键是构造合适的划分。\n",
    "\n",
    "### 贝叶斯公式\n",
    "\n",
    "定理：设$B_1,B_2,...，B_n$ 为S的一个划分，且$P(B_i) > 0$。对$P(A) > 0 $有Bayes公式：\n",
    "\n",
    "$P(B_i|A) = \\frac{P(B_i)P(A|B_i)}{\\sum_{j=1}^n P(B_j)P(A|B_j)} = \\frac{p_iq_i}{\\sum_{j=1}^n p_jq_j}$\n",
    "\n",
    "贝叶斯公式的分子是条件概率；分母是全概率。但它所表达的意义却非常刻！ \n",
    "\n",
    "在全概率公式中，如果将A看成是“结果”，$B_i$看成是导致结果发生的诸多“原因”之一，那么**全概率公式就是一个“原因推结果”的过程**。\n",
    "\n",
    "但贝叶斯公式却恰恰相反，**贝叶斯公式是“由结果归因溯源的过程”**。\n",
    "\n",
    "贝叶斯公式中，我们是知道结果A已经发生了，所要做的是反过来研究造成结果发生的原因，即“结果推原因”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯决策论\n",
    "\n",
    "贝叶斯决策论（Bayesian decision theory）是概率框架下实施决策的基本方法。\n",
    " \n",
    "对分类任务来说，**在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。**\n",
    "\n",
    "下面以多分类任务为例，解释其基本原理。\n",
    "\n",
    "假设有N种可能的类别标记，即$y = \\{c_1,c_2,...,c_N\\}$, \n",
    "\n",
    "> 例如：前面讲过的西瓜分类中，$y = \\{ c_1 = 好瓜,c_2 = 坏瓜\\}$\n",
    "\n",
    "$\\lambda_{ij}$ 是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。损失值可以根据经验或实际问题进行确定。\n",
    "\n",
    "> 例如：$\\lambda_{12}$就是将一个本身是$c_2 = 坏瓜$的样本误分类为$c_1 = 好瓜$时产生的损失。\n",
    "\n",
    "基于后验概率$P(c_i|x)$（即样本x被误判为$c_i$类型标记的概率）可获得将样本x分类为$c_i$所产生的期望损失（expected loss），即在样本x上的“条件风险”（conditional risk）\n",
    "\n",
    "> $R(c_i|x) = \\sum_{j=1}^N \\lambda_{ij}P(c_j|x)$  ——式（1）\n",
    "\n",
    "注：决策论中将“期望损失”称为“风险”（risk）\n",
    "\n",
    "我们的任务是寻找一个判定准则$h：x \\to y$ 以最小化总体风险：\n",
    "\n",
    "> $R(h) = E_x[R(h(x)|x)]$  ——式（2）\n",
    "\n",
    "注：可以把h当作某种函数，自变量是属性向量 x，值为y。找到这个函数 h，使“平均损失最小”。\n",
    "\n",
    "显然，对每个样本x，若h能最小化条件风险R(h(x)|x)，则总体风险R(h)也将被最小化。\n",
    "\n",
    "**这就产生了贝叶斯判定准则（Bayese Decision Rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记**，即：\n",
    "\n",
    "> $h^*(x) = argmin_{c \\in y}R(c|x)$ ——式（3）\n",
    "\n",
    "**此时, $h^*$称为贝叶斯最优分类器（Bayes optimal classifier）**，与之对应的**总体风险$R(h^*)$称为贝叶斯风险（Bayes risk）。**\n",
    "\n",
    "$1 - R(h^*)$反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。\n",
    "\n",
    "具体说来，若目标是最小化分类错误率，则误判损失$\\lambda_{ij}$可写为\n",
    "$$ \\lambda_{ij} = \\left\\{\n",
    "\\begin{aligned}\n",
    "0,  i = j; \\\\\n",
    "1, i \\neq j.\\\\\n",
    "\\end{aligned}\n",
    "\\right.  ——式（4）\n",
    "$$\n",
    "\n",
    "此时条件风险为：$R(c|x) = 1 - P(c|x)$ ——式（5）\n",
    "\n",
    "于是，最小分类错误率的贝叶斯最优分类器为：\n",
    "\n",
    "> $h^*(x) = argmax_{c \\in y}P(c|x)$ ——式（6）\n",
    "\n",
    "即对每个样本x，选择能使后验概率$P(c|x)$最大的类别标签。\n",
    "\n",
    "上面的分析，是通过概率论知识来构建机器学习中的分类器，由于计算$𝑃(𝑐|𝑥)$需要用到贝叶斯公式，所以称为贝叶斯决策论。\n",
    "\n",
    "![贝叶斯分类器](images/bayes/bayesclassifier.png)\n",
    "\n",
    "注意：有的机器学习算法不需要计算概率也能进行分类，那些分类器比较适合那些相关概率不容易得到，以至于不能推算$P(c|x)$的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 描述我们的问题\n",
    "\n",
    "对于分类问题，给定训练样本集$D = {(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$。假设标记y有N种可能的类别标记，即$y = \\{c_1,c_2,...,c_N\\}$。\n",
    "\n",
    "$P(c_i|x_i)$指样本$x_i$属于$y_i$标记类别的概率。\n",
    "\n",
    "如何求出这个概率值$P(c_i|x_i)$呢？\n",
    "\n",
    "**事实上，这个概率有时候很难求出。对于机器学习来说，就是要根据有限的数据集，尽可能准确的估计所有的$P(c_i|x_i)$。**\n",
    "\n",
    "例如对于下面的西瓜数据集，就是要根据这有限的17条数据计算$P(好瓜|x_i)$和$P(坏瓜|x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>色泽</th>\n",
       "      <th>根蒂</th>\n",
       "      <th>敲声</th>\n",
       "      <th>纹理</th>\n",
       "      <th>脐部</th>\n",
       "      <th>触感</th>\n",
       "      <th>密度</th>\n",
       "      <th>含糖率</th>\n",
       "      <th>标记</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>青绿</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.460</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.376</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.264</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>青绿</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.318</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>浅白</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.215</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>青绿</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>软粘</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.237</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>浊响</td>\n",
       "      <td>稍糊</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>软粘</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.149</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.211</td>\n",
       "      <td>好瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>稍糊</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.091</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>青绿</td>\n",
       "      <td>硬挺</td>\n",
       "      <td>清脆</td>\n",
       "      <td>清晰</td>\n",
       "      <td>平坦</td>\n",
       "      <td>软粘</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.267</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>浅白</td>\n",
       "      <td>硬挺</td>\n",
       "      <td>清脆</td>\n",
       "      <td>模糊</td>\n",
       "      <td>平坦</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.057</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>浅白</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>模糊</td>\n",
       "      <td>平坦</td>\n",
       "      <td>软粘</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.099</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>青绿</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>浊响</td>\n",
       "      <td>稍糊</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.161</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>浅白</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>稍糊</td>\n",
       "      <td>凹陷</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.198</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>乌黑</td>\n",
       "      <td>稍蜷</td>\n",
       "      <td>浊响</td>\n",
       "      <td>清晰</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>软粘</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.370</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>浅白</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>浊响</td>\n",
       "      <td>模糊</td>\n",
       "      <td>平坦</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.042</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>青绿</td>\n",
       "      <td>蜷缩</td>\n",
       "      <td>沉闷</td>\n",
       "      <td>稍糊</td>\n",
       "      <td>稍凹</td>\n",
       "      <td>硬滑</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.103</td>\n",
       "      <td>坏瓜</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    编号  色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率  标记\n",
       "0    1  青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460  好瓜\n",
       "1    2  乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376  好瓜\n",
       "2    3  乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264  好瓜\n",
       "3    4  青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318  好瓜\n",
       "4    5  浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215  好瓜\n",
       "5    6  青绿  稍蜷  浊响  清晰  稍凹  软粘  0.403  0.237  好瓜\n",
       "6    7  乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149  好瓜\n",
       "7    8  乌黑  稍蜷  浊响  清晰  稍凹  硬滑  0.437  0.211  好瓜\n",
       "8    9  乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  0.666  0.091  坏瓜\n",
       "9   10  青绿  硬挺  清脆  清晰  平坦  软粘  0.243  0.267  坏瓜\n",
       "10  11  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057  坏瓜\n",
       "11  12  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099  坏瓜\n",
       "12  13  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161  坏瓜\n",
       "13  14  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198  坏瓜\n",
       "14  15  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370  坏瓜\n",
       "15  16  浅白  蜷缩  浊响  模糊  平坦  硬滑  0.593  0.042  坏瓜\n",
       "16  17  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103  坏瓜"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/maloon/maloon3.txt\",header=0) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大体来说，为计算$P(c_i|x_i)$，主要有两种策略：\n",
    "\n",
    "- 判别式策略（discrimination models）\n",
    "\n",
    "  + 给定x，可通过直接建模P(c|x)来预测c。\n",
    "\n",
    "  + 注：决策树、BP神经网络、支持向量机等都是判别式模型。\n",
    "\n",
    "- 生成式（generative models）\n",
    "\n",
    "  + 先对联合概率分布$P(x,c)$建模，然后再由此获得$P(c|x)$。\n",
    "\n",
    "\n",
    "生成式模型，要考虑计算$P(c|x)$，根据条件概率公式：\n",
    "\n",
    "$P(c|x) = \\frac{P(x,c)}{P(x)}$。 ——式（7）\n",
    "\n",
    "根据贝叶斯定理：\n",
    "\n",
    "$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$。  ——式（8）\n",
    "\n",
    "其中：\n",
    "\n",
    "- P(c)是类“先验”概率（即有几种类型、各类型的概率已知，例如老农对瓜田里“好瓜”和“坏瓜”的概率是“心里有数的”）。\n",
    "\n",
    "- $P(x|c)$是样本x相对于类标记c的类条件概率，或称为似然（likelihood）。例如，类型为“好瓜”的拍声为“闷响”的概率为80%。\n",
    "\n",
    "- $P(x)$是用于归一化的“证据”因子（evidence）。对于给定样本x，证据因子$P(x)$与类标记无关。\n",
    "\n",
    "因此，估计$P(x|c)$的问题就转化为“如何基于训练数据D来估计先验概率P(c)和条件概率$P(x|c)$”。\n",
    "\n",
    "类先验概率$P(c)$,表达了样本空间中，各类样本所占的比例。根据大数定理，当训练集包含充足的独立同分布样本时，$P(c)$可通过各类样本出现的频率来进行估计。\n",
    "\n",
    "对类条件概率$P(x|c)$，由于它涉及关于x所有属性的联合概率，直接根据样本出现的频率来估计会遇到严重困难。\n",
    "\n",
    "例如，假设样本的d个属性都是二值的，则样本空间将有$2^d$种可能的取值，在现实应用中，这个值往往远大于训练样本数m，也就是说很多样本取值在训练集种根本没有出现，直接使用频率来估计$P(x|c)$显然不行。**因为“未被观测到”与“出现概率为0”通常是不同的**。\n",
    "\n",
    "如上溯西瓜数据集有17条样本数据，但属性个数为8个，即便它们都是二值属性，样本空间的取值将为$2^8 = 256$种，但事实上远不止如此。那么瓜的类型（好瓜或坏瓜）已知时，各属性的联合概率不能通过统计频率来计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计 *\n",
    "\n",
    "估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，在基于训练样本对概率分布的参数进行估计。\n",
    "\n",
    "具体地说，记关于类别 c 的类条件概率为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\\theta_c$唯一确定，则我们的任务就是利用训练集 D 估计参数 $\\theta_c$。为明确期间，我们将$P(x|c)$记为$P(x|\\theta_c)$。\n",
    "\n",
    "事实上，概率模型的训练过程就是参数估计过程（parameter estimation）。\n",
    "\n",
    "对于参数估计，统计学界的两个学派提供了不同的解决方案：\n",
    "\n",
    "- 频率主义学派（Frequentist）认为参数虽然未知，但却是客观存在的固定值。\n",
    "\n",
    "因此，可通过优化似然函数等准则来确定参数值。例如：极大似然估计（maximum likelihood estimation,简称MLE)\n",
    "\n",
    "- 贝叶斯学派（Bayesian）认为参数是未观测到的随机变量，其本身也可有分布。\n",
    "\n",
    "因此，可假定参数服从一个先验分布，然后基于观察到的数据来计算参数的后验分布。\n",
    "\n",
    "**极大似然估计（MLE）是根据数据采样来估计概率分布参数的经典方法。**\n",
    "\n",
    "令$D_c$表示训练集 D 中第 c 类样本组成的集合，假设这些样本是独立同分布的，则参数$\\theta_c$对于数据集$D_c$的似然是：\n",
    "\n",
    "$P(D_c|\\theta_c) = \\prod_{x \\in D_c}P(x|\\theta_c) $  ——式（9）\n",
    "\n",
    "对$\\theta_c$进行极大似然估计，就是要去寻找最大化似然$P(D_c|\\theta_c)$的参数值$\\theta_c$。\n",
    "\n",
    "直观上看，极大似然估计是视图在$\\theta_c$所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。\n",
    "\n",
    "式（9）中的连乘操作易造成下溢，所以通常会使用对数似然（log-likelihood）\n",
    "\n",
    "$LL(\\theta_c) = \\log P(D_c|\\theta_c) = \\sum_{x \\in D_c} \\log P(x|\\theta_c)$  ——式（10）\n",
    "\n",
    "注：所谓下溢，指利用计算机实现数值计算，当取值过于接近0时，可能超出计算机的表示范围，这种情况是下溢。而上溢指在处理绝对值非常大的数时出现的溢出情况。\n",
    "\n",
    "对于式（10），此时参数$\\theta_c$的极大似然估计：\n",
    "\n",
    "$\\hat{\\theta}_c = argmax_{\\theta_c}LL(\\theta_c)$ —— 式（11）\n",
    "\n",
    "例如，在连续属性情形下，假设概率密度函数 $p(x|c) \\sim N(\\mu_c,\\delta_c^2)$，则参数$\\mu_c$和$\\delta_c^2$的极大似然估计为：\n",
    "\n",
    "$\\hat{\\mu}_c = \\frac{1}{|D_c|} \\sum_{x \\in D_c}x$  ——式（12）\n",
    "\n",
    "$\\hat{\\mu}_c^2 = \\frac{1}{|D_c|} \\sum_{x \\in D_c}(x - \\hat{\\mu}_c)(x - \\hat{\\mu}_c)^T$  ——式（13）\n",
    "\n",
    "也就是说，通过极大似然法得到的正态分布均值，就是样本均值；方差就是$(x - \\hat{\\mu}_c)(x - \\hat{\\mu}_c)^T$的均值，这是一个符合直觉的结果。\n",
    "\n",
    "在离散属性情形下，也可通过类似的方法估计类条件概率。\n",
    "\n",
    "需要注意的是，这种参数化的方法虽然能使类条件概率估计变得相对简单，但**估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布**。\n",
    "\n",
    "现实中，为了较好的接近潜在真实分布的假设，往往需要在一定程度上利用关于应用任务本身的经验知识（获得数据分布知识），否则仅凭借“猜测”来假设概率分布形式，很可能产生误导性的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类器（Naive Bayes classifier）\n",
    "\n",
    "根据上面的分析，要算$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$。，就需要算$P(x|c)$，而样本量不够，使得以频率计算联合概率变得不可能（困难）。\n",
    "\n",
    "为了避开这个困难，朴素贝叶斯分类器采用了**“属性条件独立性假设”：**\n",
    "\n",
    "对已知类别，假设所有属性相互独立（so naive!）。\n",
    "\n",
    "换言之，假设每个属性独立地对分类结果发生影响。\n",
    "\n",
    "$P(c|x) = P(c|x_1,x_2,...,x_d) = \\frac{P(y)P(x_1,...,x_d|c)}{P(x_1,...x_d)}$\n",
    "\n",
    "有了Naive假设，则：\n",
    "\n",
    "$P(x_1,...,x_d|c) = \\prod_{i=1}^{d}P(x_i|c)$\n",
    "\n",
    "\n",
    "$P(c|x) = \\frac{P(x|c)P(c)}{P(x)} = \\frac{P(c)}{P(x)}\\prod_{i=1}^{d}P(x_i|c)$   ——式（14）\n",
    "\n",
    "其中，d为属性数目，$x_i$为x在第i个属性上的取值。\n",
    "\n",
    "由于对于所有类别来说，P(x)相同，因此基于式（6）的贝叶斯判定准则有：\n",
    "\n",
    "> $h^*(x) = argmax_{c \\in y}P(c|x) = argmax_{c \\in y}\\frac{P(c)}{P(x)}\\prod_{i=1}^{d}P(x_i|c)$  ——式（15）\n",
    "\n",
    "这就是朴素贝叶斯分类器的表达式。\n",
    "\n",
    "显然，朴素贝叶斯分类器的训练过程，就是基于训练集D来估计类先验概率P(c),并为每个属性估计条件概率$P(x_i|c)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$D_c$表示训练集D中第c类样本组合的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率：\n",
    "\n",
    "$P(c) = \\frac{|D_c|}{|D|}$ ——式（16）\n",
    "\n",
    "对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第i个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为：\n",
    "\n",
    "$P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}$ ——式（17）\n",
    "\n",
    "对连续属性，可考虑概率密度函数，假定$P(x_i|c) \\sim N(\\mu_{c,i},\\delta_{c,i}^{2})$,其中$\\mu_{c,i}$和$\\delta_{c,i}^{2}$，分别是第c类样本在第i个属性上取值的均值和方差，则有：\n",
    "\n",
    "$p(x_i|c) = \\frac{1}{\\sqrt{2 \\pi \\delta_{c,i}}}exp(- \\frac{(x_i - \\mu_{c,i })^2}{2 \\delta_{c,i }^2})$ ——式（18）\n",
    "\n",
    "下面我们根据上面的西瓜数据，来尝试构造朴素贝叶斯分类器：\n",
    "\n",
    "### 计算P(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算P(C): \n",
      "P(c=好瓜)=0.47\n",
      "P(c=坏瓜)=0.53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/maloon/maloon3.txt\",header=0) \n",
    "print(\"计算P(C): \")\n",
    "p_c_good = df[df['标记'] == '好瓜'].shape[0] /df.shape[0]\n",
    "print(\"P(c=好瓜)={:.2}\".format(p_c_good))\n",
    "p_c_bad = df[df['标记'] == '坏瓜'].shape[0] /df.shape[0]\n",
    "print(\"P(c=坏瓜)={:.2}\".format(p_c_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算每个属性对应的条件概率$P(x_i|c)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算P(xi|c): \n",
      "P(色泽=青绿|标记=好瓜)=0.375\n",
      "P(色泽=青绿|标记=坏瓜)=0.3333333333333333\n",
      "P(色泽=青绿|标记=好瓜)=0.38\n",
      "P(色泽=青绿|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "P(色泽=乌黑|标记=好瓜)=0.5\n",
      "P(色泽=乌黑|标记=坏瓜)=0.22\n",
      "------------------------------\n",
      "P(色泽=浅白|标记=好瓜)=0.12\n",
      "P(色泽=浅白|标记=坏瓜)=0.44\n",
      "------------------------------\n",
      "P(根蒂=蜷缩|标记=好瓜)=0.62\n",
      "P(根蒂=蜷缩|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "P(根蒂=稍蜷|标记=好瓜)=0.38\n",
      "P(根蒂=稍蜷|标记=坏瓜)=0.44\n",
      "------------------------------\n",
      "P(根蒂=硬挺|标记=好瓜)=0.0\n",
      "P(根蒂=硬挺|标记=坏瓜)=0.22\n",
      "------------------------------\n",
      "P(敲声=浊响|标记=好瓜)=0.75\n",
      "P(敲声=浊响|标记=坏瓜)=0.44\n",
      "------------------------------\n",
      "P(敲声=沉闷|标记=好瓜)=0.25\n",
      "P(敲声=沉闷|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "P(敲声=清脆|标记=好瓜)=0.0\n",
      "P(敲声=清脆|标记=坏瓜)=0.22\n",
      "------------------------------\n",
      "P(纹理=清晰|标记=好瓜)=0.88\n",
      "P(纹理=清晰|标记=坏瓜)=0.22\n",
      "------------------------------\n",
      "P(纹理=稍糊|标记=好瓜)=0.12\n",
      "P(纹理=稍糊|标记=坏瓜)=0.44\n",
      "------------------------------\n",
      "P(纹理=模糊|标记=好瓜)=0.0\n",
      "P(纹理=模糊|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "P(脐部=凹陷|标记=好瓜)=0.62\n",
      "P(脐部=凹陷|标记=坏瓜)=0.22\n",
      "------------------------------\n",
      "P(脐部=稍凹|标记=好瓜)=0.38\n",
      "P(脐部=稍凹|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "P(脐部=平坦|标记=好瓜)=0.0\n",
      "P(脐部=平坦|标记=坏瓜)=0.44\n",
      "------------------------------\n",
      "P(触感=硬滑|标记=好瓜)=0.75\n",
      "P(触感=硬滑|标记=坏瓜)=0.67\n",
      "------------------------------\n",
      "P(触感=软粘|标记=好瓜)=0.25\n",
      "P(触感=软粘|标记=坏瓜)=0.33\n",
      "------------------------------\n",
      "好瓜的密度均值为0.574\n",
      "坏瓜的密度均值为0.496\n",
      "好瓜的密度标准差为0.129\n",
      "坏瓜的密度标准差为0.195\n",
      "----------------------------------------\n",
      "P(密度=0.639|标记=好瓜)=2.72\n",
      "P(密度=0.639|标记=坏瓜)=1.57\n",
      "P(密度=0.657|标记=好瓜)=2.51\n",
      "P(密度=0.657|标记=坏瓜)=1.46\n",
      "P(密度=0.6970000000000001|标记=好瓜)=1.96\n",
      "P(密度=0.6970000000000001|标记=坏瓜)=1.2\n",
      "P(密度=0.608|标记=好瓜)=2.98\n",
      "P(密度=0.608|标记=坏瓜)=1.74\n",
      "P(密度=0.245|标记=好瓜)=0.121\n",
      "P(密度=0.245|标记=坏瓜)=0.892\n",
      "P(密度=0.43700000000000006|标记=好瓜)=1.76\n",
      "P(密度=0.43700000000000006|标记=坏瓜)=1.96\n",
      "P(密度=0.593|标记=好瓜)=3.05\n",
      "P(密度=0.593|标记=坏瓜)=1.81\n",
      "P(密度=0.7190000000000001|标记=好瓜)=1.64\n",
      "P(密度=0.7190000000000001|标记=坏瓜)=1.06\n",
      "P(密度=0.6659999999999999|标记=好瓜)=2.39\n",
      "P(密度=0.6659999999999999|标记=坏瓜)=1.4\n",
      "P(密度=0.243|标记=好瓜)=0.117\n",
      "P(密度=0.243|标记=坏瓜)=0.88\n",
      "P(密度=0.36|标记=好瓜)=0.786\n",
      "P(密度=0.36|标记=坏瓜)=1.6\n",
      "P(密度=0.774|标记=好瓜)=0.929\n",
      "P(密度=0.774|标记=坏瓜)=0.74\n",
      "P(密度=0.34299999999999997|标记=好瓜)=0.627\n",
      "P(密度=0.34299999999999997|标记=坏瓜)=1.5\n",
      "P(密度=0.40299999999999997|标记=好瓜)=1.29\n",
      "P(密度=0.40299999999999997|标记=坏瓜)=1.83\n",
      "P(密度=0.556|标记=好瓜)=3.06\n",
      "P(密度=0.556|标记=坏瓜)=1.95\n",
      "P(密度=0.48100000000000004|标记=好瓜)=2.39\n",
      "P(密度=0.48100000000000004|标记=坏瓜)=2.04\n",
      "P(密度=0.634|标记=好瓜)=2.77\n",
      "P(密度=0.634|标记=坏瓜)=1.59\n",
      "好瓜的含糖率均值为0.279\n",
      "坏瓜的含糖率均值为0.154\n",
      "好瓜的含糖率标准差为0.101\n",
      "坏瓜的含糖率标准差为0.108\n",
      "----------------------------------------\n",
      "P(含糖率=0.237|标记=好瓜)=3.63\n",
      "P(含糖率=0.237|标记=坏瓜)=2.76\n",
      "P(含糖率=0.21100000000000002|标记=好瓜)=3.16\n",
      "P(含糖率=0.21100000000000002|标记=坏瓜)=3.22\n",
      "P(含糖率=0.057|标记=好瓜)=0.354\n",
      "P(含糖率=0.057|标记=坏瓜)=2.46\n",
      "P(含糖率=0.149|标记=好瓜)=1.73\n",
      "P(含糖率=0.149|标记=坏瓜)=3.7\n",
      "P(含糖率=0.318|标记=好瓜)=3.66\n",
      "P(含糖率=0.318|标记=坏瓜)=1.17\n",
      "P(含糖率=0.215|标记=好瓜)=3.24\n",
      "P(含糖率=0.215|标记=坏瓜)=3.16\n",
      "P(含糖率=0.042|标记=好瓜)=0.252\n",
      "P(含糖率=0.042|标记=坏瓜)=2.15\n",
      "P(含糖率=0.10300000000000001|标记=好瓜)=0.868\n",
      "P(含糖率=0.10300000000000001|标记=坏瓜)=3.31\n",
      "P(含糖率=0.091|标记=好瓜)=0.701\n",
      "P(含糖率=0.091|标记=坏瓜)=3.12\n",
      "P(含糖率=0.161|标记=好瓜)=2.0\n",
      "P(含糖率=0.161|标记=坏瓜)=3.69\n",
      "P(含糖率=0.267|标记=好瓜)=3.93\n",
      "P(含糖率=0.267|标记=坏瓜)=2.14\n",
      "P(含糖率=0.198|标记=好瓜)=2.87\n",
      "P(含糖率=0.198|标记=坏瓜)=3.41\n",
      "P(含糖率=0.37|标记=好瓜)=2.63\n",
      "P(含糖率=0.37|标记=坏瓜)=0.499\n",
      "P(含糖率=0.099|标记=好瓜)=0.809\n",
      "P(含糖率=0.099|标记=坏瓜)=3.25\n",
      "P(含糖率=0.376|标记=好瓜)=2.48\n",
      "P(含糖率=0.376|标记=坏瓜)=0.446\n",
      "P(含糖率=0.264|标记=好瓜)=3.91\n",
      "P(含糖率=0.264|标记=坏瓜)=2.2\n",
      "P(含糖率=0.46|标记=好瓜)=0.788\n",
      "P(含糖率=0.46|标记=坏瓜)=0.0662\n",
      "{'标记=好瓜': 0.5294117647058824, '密度=0.639|标记=好瓜': 2.7179186724569, '密度=0.639|标记=坏瓜': 1.5651996837233972, '密度=0.657|标记=好瓜': 2.508824201032962, '密度=0.657|标记=坏瓜': 1.4563094069064093, '密度=0.6970000000000001|标记=好瓜': 1.959011549465037, '密度=0.6970000000000001|标记=坏瓜': 1.2033038984540714, '密度=0.608|标记=好瓜': 2.9809509930560973, '密度=0.608|标记=坏瓜': 1.7370146326372236, '密度=0.245|标记=好瓜': 0.12131663775982275, '密度=0.245|标记=坏瓜': 0.8920029801681834, '密度=0.43700000000000006|标记=好瓜': 1.7635361852235452, '密度=0.43700000000000006|标记=坏瓜': 1.9565505480189005, '密度=0.593|标记=好瓜': 3.0534619198546515, '密度=0.593|标记=坏瓜': 1.8102519538148056, '密度=0.7190000000000001|标记=好瓜': 1.6413754258883257, '密度=0.7190000000000001|标记=坏瓜': 1.064094991222338, '密度=0.6659999999999999|标记=好瓜': 2.3929110260284943, '密度=0.6659999999999999|标记=坏瓜': 1.400244602353778, '密度=0.243|标记=好瓜': 0.11661780554686658, '密度=0.243|标记=坏瓜': 0.8802190942040011, '密度=0.36|标记=好瓜': 0.7858867016502383, '密度=0.36|标记=坏瓜': 1.604721998084733, '密度=0.774|标记=好瓜': 0.9290796761464745, '密度=0.774|标记=坏瓜': 0.7400116815197482, '密度=0.34299999999999997|标记=好瓜': 0.6267245971797755, '密度=0.34299999999999997|标记=坏瓜': 1.5039750993085828, '密度=0.40299999999999997|标记=好瓜': 1.2894425429009304, '密度=0.40299999999999997|标记=坏瓜': 1.8274682955906263, '密度=0.556|标记=好瓜': 3.0585414174692085, '密度=0.556|标记=坏瓜': 1.9541639275610996, '密度=0.48100000000000004|标记=好瓜': 2.386291282481823, '密度=0.48100000000000004|标记=坏瓜': 2.0426534687395987, '密度=0.634|标记=好瓜': 2.7694785219256155, '密度=0.634|标记=坏瓜': 1.594446839922648, '含糖率=0.237|标记=好瓜': 3.62873769564797, '含糖率=0.237|标记=坏瓜': 2.75588115661399, '含糖率=0.21100000000000002|标记=好瓜': 3.1554442909435316, '含糖率=0.21100000000000002|标记=坏瓜': 3.2215757885574616, '含糖率=0.057|标记=好瓜': 0.3536695234900731, '含糖率=0.057|标记=坏瓜': 2.464177826380975, '含糖率=0.149|标记=好瓜': 1.7298559437772758, '含糖率=0.149|标记=坏瓜': 3.696605147428518, '含糖率=0.318|标记=好瓜': 3.6649889047117408, '含糖率=0.318|标记=坏瓜': 1.166931067801782, '含糖率=0.215|标记=好瓜': 3.2379809281405443, '含糖率=0.215|标记=坏瓜': 3.1570459641466004, '含糖率=0.042|标记=好瓜': 0.25233444512084935, '含糖率=0.042|标记=坏瓜': 2.1525884893738167, '含糖率=0.10300000000000001|标记=好瓜': 0.8677901018234492, '含糖率=0.10300000000000001|标记=坏瓜': 3.3058342103279315, '含糖率=0.091|标记=好瓜': 0.7005214200256812, '含糖率=0.091|标记=坏瓜': 3.116136019945967, '含糖率=0.161|标记=好瓜': 2.0013692760186825, '含糖率=0.161|标记=坏瓜': 3.693637100766974, '含糖率=0.267|标记=好瓜': 3.9262006962654876, '含糖率=0.267|标记=坏瓜': 2.141041231747107, '含糖率=0.198|标记=好瓜': 2.870147911644192, '含糖率=0.198|标记=坏瓜': 3.407983948676968, '含糖率=0.37|标记=好瓜': 2.6266502559982516, '含糖率=0.37|标记=坏瓜': 0.49911998042723477, '含糖率=0.099|标记=好瓜': 0.809280621273476, '含糖率=0.099|标记=坏瓜': 3.245818217734705, '含糖率=0.376|标记=好瓜': 2.484794708962111, '含糖率=0.376|标记=坏瓜': 0.4458031057760228, '含糖率=0.264|标记=好瓜': 3.910908353223812, '含糖率=0.264|标记=坏瓜': 2.2034453985902833, '含糖率=0.46|标记=好瓜': 0.7880520952044109, '含糖率=0.46|标记=坏瓜': 0.06622115248436898}\n"
     ]
    }
   ],
   "source": [
    "#计算每个属性对应的条件概率 𝑃(𝑥𝑖|𝑐)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"data/maloon/maloon3.txt\",header=0) \n",
    "\"\"\"\n",
    "print(\"计算P(xi|c): \")\n",
    "p = df[(df['色泽'] == '青绿') & (df['标记'] == '好瓜')].shape[0]/df[df['标记'] == '好瓜'].shape[0]\n",
    "print(\"P(色泽=青绿|标记=好瓜)={}\".format(p))\n",
    "\n",
    "p = df[(df['色泽'] == '青绿') & (df['标记'] == '坏瓜')].shape[0]/df[df['标记'] == '坏瓜'].shape[0]\n",
    "print(\"P(色泽=青绿|标记=坏瓜)={}\".format(p))\n",
    "\"\"\"\n",
    "for col in df.columns[1:-3]:\n",
    "    dt  = {}\n",
    "    dt.update(df[col].value_counts())\n",
    "    for key,value in dt.items():\n",
    "        p = df[(df[col] == key) & (df['标记'] == '好瓜')].shape[0]/df[df['标记'] == '好瓜'].shape[0]\n",
    "        print(\"P({}={}|标记=好瓜)={:.2}\".format(col,key,p))\n",
    "        p = df[(df[col] == key) & (df['标记'] == '坏瓜')].shape[0]/df[df['标记'] == '坏瓜'].shape[0]\n",
    "        print(\"P({}={}|标记=坏瓜)={:.2}\".format(col,key,p))\n",
    "        print(\"---\"*10)\n",
    "\n",
    "p_dict = {}\n",
    "p_dict[\"标记=好瓜\"] = p_c_good\n",
    "p_dict[\"标记=好瓜\"] = p_c_bad\n",
    "\n",
    "for col in df.columns[-3:-1]:\n",
    "    mean_g =   df[df['标记'] == '好瓜'][col].mean()\n",
    "    print(\"好瓜的{}均值为{:.3}\".format(col,mean_g))\n",
    "    mean_b = df[df['标记'] == '坏瓜'][col].mean()\n",
    "    print(\"坏瓜的{}均值为{:.3}\".format(col,mean_b))\n",
    "    std_g = df[df['标记'] == '好瓜'][col].std()\n",
    "    print(\"好瓜的{}标准差为{:.3}\".format(col,std_g))\n",
    "    std_b = df[df['标记'] == '坏瓜'][col].std()\n",
    "    print(\"坏瓜的{}标准差为{:.3}\".format(col,std_b))\n",
    "    print(\"----\"*10)\n",
    "    dt  = {}\n",
    "    dt.update(df[col].value_counts())\n",
    "    for key in dt.keys():\n",
    "\n",
    "        p = 1/(math.sqrt(2*math.pi) * std_g) *math.exp(- (key-mean_g)**2 / (2*std_g*std_g))\n",
    "        p_dict[\"{}={}|标记=好瓜\".format(col,key)] = p\n",
    "        print(\"P({}={}|标记=好瓜)={:.3}\".format(col,key,p))\n",
    "\n",
    "        p = 1/(math.sqrt(2*math.pi) * std_b) *math.exp(- (key-mean_b)**2 / (2*std_b*std_b))\n",
    "        p_dict[\"{}={}|标记=坏瓜\".format(col,key)] = p\n",
    "        print(\"P({}={}|标记=坏瓜)={:.3}\".format(col,key,p))\n",
    "\n",
    "print(p_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测\n",
    "\n",
    "有新数据样本：\n",
    "\n",
    "|编号|色泽|根蒂|敲声|纹理|脐部|触感|密度|含糖率|标记|\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "|1|青绿|蜷缩|浊响|清晰|凹陷|硬滑|0.697|0.460||\n",
    "\n",
    "根据上面的计算，有：\n",
    "\n",
    "$$ P(c=好瓜) * P(青绿|好瓜) * P(蜷缩|好瓜) * P(浊响|好瓜) * P(清晰|好瓜) * P(凹陷|好瓜) * P(硬滑|好瓜) * P(密度=0.697|好瓜) * P(含糖=0.460|好瓜)$$ = 0.038\n",
    "\n",
    "$$ P(c=坏瓜) * P(青绿|坏瓜) * P(蜷缩|坏瓜) * P(浊响|坏瓜) * P(清晰|坏瓜) * P(凹陷|坏瓜) * P(硬滑|坏瓜) * P(密度=0.697|坏瓜) * P(含糖=0.460|坏瓜)$$ = 0.000068\n",
    "\n",
    "由于0.038 > 0.000068，所以，朴素贝叶斯分类器将测试样本判定为“好瓜”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意，若某个属性值在训练集中没有与某个类同时出现过，直接使用式（17）进行估计，再根据式（15）进行判别将出现问题。\n",
    "\n",
    "例如，在使用maloon3.txt中的数据集训练贝叶斯分类器时，对一个“敲声=清脆”（“清脆”没出现过）的测试样例，有：\n",
    "\n",
    "$P_{清脆|是} = P(敲声 = 清脆|好瓜=是)=\\frac{0}{8} = 0$\n",
    "\n",
    "由于式（15）的连乘式计算出的概率值为零，因此无论该样本其它属性是什么，分类的结果都是无法判别的（既不是好瓜也不是坏瓜，以为概率都为0）。这显然不合理。\n",
    "\n",
    "为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时，要进行**“平滑”**（smoothing），常用**“拉普拉斯修正”（Laplacian correction）**。\n",
    "\n",
    "具体的来说，令 N 表示训练集 D 中可能的类别数，$N_i$表示第i个属性可能属于的类别数，则式（16）和（17）分贝修正为：\n",
    "\n",
    "$\\widehat P(c)= \\frac{|D_c|+1}{|D|+N}$ ——式（19）\n",
    "\n",
    "$\\widehat P(x_i|c)= \\frac{|D_{c,x_i}|+1}{|D_c|+N_i}$ ——式（20）\n",
    "\n",
    "这样，在本例中：\n",
    "\n",
    "$\\widehat P(c=好瓜)= \\frac{8+1}{17+2} \\approx 0.474$ \n",
    "\n",
    "$\\widehat P(c=坏瓜)= \\frac{9+1}{17+2} \\approx 0.526$ \n",
    "\n",
    "类似地，$P(色泽=青绿|好瓜) = \\frac{3+1}{8+3} \\approx 0.364 $\n",
    "\n",
    "$P(敲声 = 清脆|好瓜) = \\frac{0+1}{8+3} \\approx 0.091 $\n",
    "\n",
    "显然，拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零地问题。并且在训练集变大时，修正过程所引入地先验（prior）的影响也会逐渐变得可忽略，使得估值渐趋向于实际概率值。\n",
    "\n",
    "在现实任务中，朴素贝叶斯分类器有多种使用方式。例如：\n",
    "\n",
    "- 若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在预测时只需查表即可计算、判别。\n",
    "\n",
    "- 若任务数据更替频繁，则可采用“懒惰学习”方式，先不进行任务训练（计算），待收到预测请求时，再根据当前数据集进行概率估值；\n",
    "\n",
    "- 若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正，即可实现增量学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用sklearn中的朴素贝叶斯分类器\n",
    "\n",
    "朴素贝叶斯分类器虽然简单，但它却在很多实际预测中运行良好，而且它的效率很高，经常被应用在文本分类中，包括互联网新闻的分类，垃圾邮件的筛选。\n",
    "\n",
    "朴素贝叶斯方法不大适于作回归任务，对于它给出的概率值不能过于相信。\n",
    "\n",
    "在sklearn库种，实现了三个朴素贝叶斯分类器，如下表所示：\n",
    "\n",
    "|分类器|描述|\n",
    "|-|-|\n",
    "| naive_bayes.GaussianNB|高斯朴素贝叶斯分类器|\n",
    "|naive_bayes.BernoulliNB|贝努力朴素贝叶斯分类器|\n",
    "|naive_bayes.MultinomialNB|多项式贝叶斯分类器|\n",
    "\n",
    "它们的区别在于假设某一特征的所有属于某个类别的观测值符合特定分布，如：\n",
    "\n",
    "- 高斯模型\n",
    "\n",
    "适用于样本中每个属性的值是连续的，且服从高斯分布时，进行分类。\n",
    "\n",
    "例如：人的身高一般符合高斯分布，涉及这种数据的分类问题就适合使用高斯朴素贝叶斯。\n",
    "\n",
    "- 多项式模型(multinomial model)\n",
    "\n",
    "如果属性值大部分是多元离散值，则采用多项式模型进行分类预测要好些。\n",
    "\n",
    "- 伯努利模(Bernoulli model)\n",
    "\n",
    "如果属性值多为二元离散值或是稀疏的多元离散值，则采用伯努利模型进行贝叶斯分类，准确率较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯朴素贝叶斯\n",
    "\n",
    "#### 正态分布/高斯分布\n",
    "\n",
    "正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution），最早由A.棣莫弗在求二项分布的渐近公式中得到。C.F.高斯在研究测量误差时从另一个角度导出了它。P.S.拉普拉斯和高斯研究了它的性质。是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。\n",
    "\n",
    "自然界和人类社会中，很多现象可以看作正态分布，例如：\n",
    "\n",
    "- 人的身高、体重（或其他生理尺寸）\n",
    "- 医学检查指标（血小板数、红细胞数）\n",
    "- 测量误差\n",
    "\n",
    "\n",
    "![正态分布](images/bayes/正态分布.jpg)\n",
    "\n",
    "正态分布的定义：\n",
    "\n",
    "若X的概率密度函数为：\n",
    "\n",
    "$f(x) = \\frac{1}{\\sqrt{2 \\pi} \\delta}e^{-\\frac{(x-\\mu)^2}{2\\delta^2}},-\\infty < x < +\\infty$\n",
    "\n",
    "其中，$-\\infty < \\mu < \\infty, \\mu > 0 $，就称X服从参数为$\\mu,\\delta$的正态分布（或高斯分布）。\n",
    "\n",
    "记为$X \\sim N(\\mu,\\delta^2)$\n",
    "\n",
    "特征：\n",
    "- f(x)关于$ x = \\mu$对称\n",
    "- 当$x \\leq \\mu$时，f(x)是严格单调递增函数\n",
    "- $f_{max} = f(\\mu) = \\frac{1}{\\sqrt{2 \\pi}\\delta}$\n",
    "- $lim_{|x-\\mu| \\to \\infty}f(x) = 0$\n",
    "\n",
    "一般正态分布可以转化为标准正态分布：\n",
    "\n",
    "$X \\sim N(\\mu,\\delta^2)$, 可变为标准型$Z=\\frac{X-\\mu}{\\delta} \\sim N(0,1)$. 此时，$f(x) = \\frac{1}{\\sqrt{2 \\pi} }e^{-\\frac{x^2}{2}},-\\infty < x < +\\infty$\n",
    "\n",
    "**两个参数的含义**\n",
    "\n",
    "- 当固定$\\delta$，改变$\\mu$的大小时，$f(x)$图形的形状不变，只是沿着x 轴作平移变换；\n",
    "\n",
    "$\\mu$称为位置参数（决定对称轴位置）。\n",
    "\n",
    "- 当固定$\\mu$，改变$\\delta$的大小时，$f(x)$图形的对称轴不变，而形状在改变，$\\delta$越小，图形越高越瘦，$\\delta$越大，图形越矮越胖。\n",
    "\n",
    "$\\delta$称为尺度参数（决定曲线分散程度）\n",
    "\n",
    "\n",
    "#### 正态分布的概率计算\n",
    "\n",
    "若$X \\sim N(\\mu,\\delta^2)$，对实数x,\n",
    "\n",
    "$P(X \\leq x) = F(x) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x}e^{-\\frac{(t-\\mu)^2}{2\\delta^2}}dt$\n",
    "\n",
    "这个积分可以通过软件来算；或用数值积分法；或转化为标准正态分布，然后查表计算。\n",
    "\n",
    "#### 高斯朴素贝叶斯\n",
    "\n",
    "$P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\delta_y^2}}exp(- \\frac{(x_i - \\mu _y)^2}{2 \\delta _y^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据\n",
    "\n",
    "Iris数据集是机器学习任务中常用的分类实验数据集，由Fisher在1936收集整理。Iris中文名是安德森鸢尾花卉数据集，英文全称是Anderson’s Iris data set，是一类多重变量分析的数据集。Iris一共包含150个样本，分为3类，每类50个数据，每个数据包含4个属性。可通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。\n",
    "\n",
    "![鸢尾花图](images/svm/鸢尾花图.jpg)\n",
    "\n",
    "通俗地说，iris数据集是用来给莺尾花做分类的数据集，每个样本包含了花萼长度、花萼宽度、花瓣长度、花瓣宽度四个特征（下表中的前4列），我们需要建立一个分类器，该分类器可通过样本的四个特征来来判断样本属于山鸢尾（Setosa）、变色鸢尾（Versicolour）还是维吉尼亚鸢尾（Virginica）中的哪一个，即机器学习中的分类问题。\n",
    "\n",
    "iris的每个样本都包含了品种信息，即目标属性（第5列，也叫target或label）。\n",
    "\n",
    "![鸢尾花数据集样例](images/svm/鸢尾花数据集.png)\n",
    "\n",
    "这个数据可以通过下列代码直接调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (iris.data.shape[0],(iris.target != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式贝叶斯（ Multinomial Naive Bayes）\n",
    "\n",
    "如果属性值大部分是多元离散值，可以采用多项式模型进行分类预测，这一模型常用于文本分类。\n",
    "\n",
    "给定训练样本集$D = {(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$。假设标记y有N种可能的类别标记，即$y = \\{c_1,c_2,...,c_N\\}$。\n",
    "\n",
    "对于每个分类标记$c$ ，有概率向量P(c)：\n",
    "\n",
    "$ P(c) = (P(x_1|c),P(x_2|c),...,P(x_m|c))$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $x_i$ 表示属性的数量，在文本分类中，就是指文本中不重复单词的数量。\n",
    "\n",
    "- $P(x_i|y)$是在数据集 D 中属于类型 c 的属性为x_i 的样本的概率 \n",
    "\n",
    "要求$ P(c)$，可以使用平滑版本的极大似然估计，例如相对频率计数：\n",
    "\n",
    "$\\hat{P}(x_i|y) = \\frac{|D_{c,x_i}| + \\alpha}{|D_c|+\\alpha n}$\n",
    "\n",
    "平滑因子$\\alpha \\geq 0$ 的引入是为了考虑在预测时出现训练集中不存在的属性时，预测概率不至于为0.\n",
    "\n",
    "平滑因子$\\alpha = 1$ 就是我们之前提到的拉普拉斯平滑，而$\\alpha < 1$ 被称为lidstone平滑。\n",
    "\n",
    "下面的例子，是使用多项式贝叶斯分类器进行文本分类预测：\n",
    "\n",
    "20newsgroups数据集是用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。数据集收集了大约20,000左右的新闻组文档，均匀分为20个不同主题的新闻组集合。一些新闻组的主题特别相似(e.g. comp.sys.ibm.pc.hardware/ comp.sys.mac.hardware)，还有一些却完全不相关 (e.g misc.forsale /soc.religion.christian)。\n",
    "\n",
    "20news-bydate.tar.gz –是按时间顺序分为训练(60%)和测试(40%)两部分数据集，不包含重复文档和新闻组名（新闻组，路径，隶属于，日期）\n",
    "\n",
    "sklearn.datasets.fetch_20newsgroups，返回一个可以被文本特征提取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "The Accuracy of Naive Bayes Classifier is: 0.8397707979626485\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.86      0.86       201\n",
      "           comp.graphics       0.59      0.86      0.70       250\n",
      " comp.os.ms-windows.misc       0.89      0.10      0.17       248\n",
      "comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240\n",
      "   comp.sys.mac.hardware       0.93      0.78      0.85       242\n",
      "          comp.windows.x       0.82      0.84      0.83       263\n",
      "            misc.forsale       0.91      0.70      0.79       257\n",
      "               rec.autos       0.89      0.89      0.89       238\n",
      "         rec.motorcycles       0.98      0.92      0.95       276\n",
      "      rec.sport.baseball       0.98      0.91      0.95       251\n",
      "        rec.sport.hockey       0.93      0.99      0.96       233\n",
      "               sci.crypt       0.86      0.98      0.91       238\n",
      "         sci.electronics       0.85      0.88      0.86       249\n",
      "                 sci.med       0.92      0.94      0.93       245\n",
      "               sci.space       0.89      0.96      0.92       221\n",
      "  soc.religion.christian       0.78      0.96      0.86       232\n",
      "      talk.politics.guns       0.88      0.96      0.92       251\n",
      "   talk.politics.mideast       0.90      0.98      0.94       231\n",
      "      talk.politics.misc       0.79      0.89      0.84       188\n",
      "      talk.religion.misc       0.93      0.44      0.60       158\n",
      "\n",
      "               micro avg       0.84      0.84      0.84      4712\n",
      "               macro avg       0.86      0.84      0.82      4712\n",
      "            weighted avg       0.86      0.84      0.82      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups  # 从sklearn.datasets里导入新闻数据抓取器 fetch_20newsgroups\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # 从sklearn.feature_extraction.text里导入文本特征向量化模块\n",
    "from sklearn.naive_bayes import MultinomialNB     # 从sklean.naive_bayes里导入朴素贝叶斯模型\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1.数据获取\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "# 或者为了加速程序执行，我们可以选择几个分类\n",
    "# categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "# twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "print( len(news.data))  # 输出数据的条数：18846\n",
    "\n",
    "#2.数据预处理：训练集和测试集分割，文本特征向量化\n",
    "X_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=33) # 随机采样25%的数据样本作为测试集\n",
    "#print X_train[0]  #查看训练样本\n",
    "#print y_train[0:100]  #查看标签\n",
    "\n",
    "#文本特征向量化\n",
    "vec = CountVectorizer()\n",
    "X_train = vec.fit_transform(X_train)\n",
    "X_test = vec.transform(X_test)\n",
    "\n",
    "#3.使用朴素贝叶斯进行训练\n",
    "mnb = MultinomialNB()   # 使用默认配置初始化朴素贝叶斯\n",
    "mnb.fit(X_train,y_train)    # 利用训练数据对模型参数进行估计\n",
    "y_predict = mnb.predict(X_test)     # 对参数进行预测\n",
    "\n",
    "#4.获取结果报告\n",
    "print('The Accuracy of Naive Bayes Classifier is:', mnb.score(X_test,y_test))\n",
    "print(classification_report(y_test, y_predict, target_names = news.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伯努利朴素贝叶斯（ BernoulliNB Naive Bayes）\n",
    "\n",
    "BernoulliNB 用于数据分布为伯努利分布的问题进行分类预测。\n",
    "\n",
    "#### 贝努力分布（bernoulli）或0-1分布  \n",
    "\n",
    "若X的概率分布律为\n",
    "\n",
    "|X|0|1|\n",
    "|-|-|-|\n",
    "|P|1-p|p|\n",
    "\n",
    "其中$0 < p < 1$，就称X服从参数为p的 0-1分布（或两点分布），记为 $X \\sim 0-1(p) $或$X \\sim B(1,p)$\n",
    "\n",
    "其分布律还可以写为：$P(X=K) = p^k(1-p)^{1-k}, k = 0, 1$\n",
    "\n",
    "若P(X=c) = 1，则X服从退化分布。\n",
    "\n",
    "**0-1分布的应用**\n",
    "\n",
    "若一个随机试验，它的样本空间只包含两个元素，即$S = {e_1,e_2}$，我们总能在S上定义一个服从（0-1）分布的随机变量：\n",
    "\n",
    "X = 0，当 e = e_1\n",
    "X = 1, 当 e = e_2\n",
    "来描述这个随机试验的结果。\n",
    "\n",
    "如果输入数据不是0-1数值，而是其它类型的数据，BernoulliNB实例可以利用binarize参数，将其输入二值化。\n",
    "\n",
    "伯努利朴素贝叶斯的决策规则是基于\n",
    "\n",
    "$P(x_i|c) = P(x_i|c)x_i + (1-P(x_i|c))(1-x_i)$\n",
    "\n",
    "这与多项式贝叶斯分类器不同，在预测时，遇到没见过的属性值，多项式贝叶斯简单地忽略了不存在的属性值；而伯努利贝叶斯则它明确地处罚了不存在于训练集中的属性值。\n",
    "\n",
    "在文本分类的情况下，可以使用单词出现向量（而不是单词计数向量）来训练和使用该分类器。BernoulliNB 可能在某些数据集上表现更好，特别是那些文档较短的数据集。如果时间允许，建议评估两种模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
